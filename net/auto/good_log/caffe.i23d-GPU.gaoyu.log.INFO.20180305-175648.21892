Log file created at: 2018/03/05 17:56:48
Running on machine: i23d-GPU
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0305 17:56:48.475936 21892 caffe.cpp:218] Using GPUs 0
I0305 17:56:48.636899 21892 caffe.cpp:223] GPU 0: TITAN Xp
I0305 17:56:49.587046 21892 solver.cpp:44] Initializing solver from parameters: 
test_iter: 10
test_interval: 50000
base_lr: 1e-06
display: 25
max_iter: 5000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0.0005
snapshot: 500
snapshot_prefix: "./snapshots/auto/v3/v3"
solver_mode: GPU
device_id: 0
net: "./train_resnet_auto.prototxt"
train_state {
  level: 0
  stage: ""
}
I0305 17:56:49.587323 21892 solver.cpp:87] Creating training net from net file: ./train_resnet_auto.prototxt
I0305 17:56:49.589408 21892 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: ./train_resnet_auto.prototxt
I0305 17:56:49.589434 21892 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I0305 17:56:49.590173 21892 net.cpp:51] Initializing net from parameters: 
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "Data1"
  type: "Data"
  top: "Data1"
  top: "Data2"
  transform_param {
    scale: 0.01
  }
  data_param {
    source: "/data3/lzh/1000x224x224/raw_data_photon_dis"
    batch_size: 24
    backend: LMDB
  }
}
layer {
  name: "Data3"
  type: "Data"
  top: "Data3"
  top: "Data4"
  transform_param {
    scale: 1
  }
  data_param {
    source: "/data3/lzh/1000x224x224/raw_data_photon_flux"
    batch_size: 24
    backend: LMDB
  }
}
layer {
  name: "Data5"
  type: "Data"
  top: "Data5"
  top: "Data6"
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "/data3/lzh/1000x224x224/raw_data_conv"
    batch_size: 24
    backend: LMDB
  }
}
layer {
  name: "Silence"
  type: "Silence"
  bottom: "Data2"
  bottom: "Data4"
  bottom: "Data6"
}
layer {
  name: "Concat1"
  type: "Concat"
  bottom: "Data1"
  bottom: "Data3"
  top: "Concat1"
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "Concat1"
  top: "Convolution1"
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "Convolution1"
  top: "Convolution1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Convolution1"
  top: "Convolution2"
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "Convolution2"
  top: "Convolution2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "Convolution2"
  top: "Convolution2"
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Convolution2"
  top: "Convolution3"
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "Convolution3"
  top: "Convolution3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "Convolution3"
  top: "Convolution3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise1"
  type: "Eltwise"
  bottom: "Convolution3"
  bottom: "Convolution1"
  top: "Eltwise1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU2"
  type: "ReLU"
  bottom: "Eltwise1"
  top: "Eltwise1"
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "Eltwise1"
  top: "Convolution4"
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm4"
  type: "BatchNorm"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale4"
  type: "Scale"
  bottom: "Convolution4"
  top: "Convolution4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU3"
  type: "ReLU"
  bottom: "Convolution4"
  top: "Convolution4"
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "Convolution4"
  top: "Convolution5"
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm5"
  type: "BatchNorm"
  bottom: "Convolution5"
  top: "Convolution5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale5"
  type: "Scale"
  bottom: "Convolution5"
  top: "Convolution5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise2"
  type: "Eltwise"
  bottom: "Convolution5"
  bottom: "Eltwise1"
  top: "Eltwise2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU4"
  type: "ReLU"
  bottom: "Eltwise2"
  top: "Eltwise2"
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "Eltwise2"
  top: "Convolution6"
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm6"
  type: "BatchNorm"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale6"
  type: "Scale"
  bottom: "Convolution6"
  top: "Convolution6"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU5"
  type: "ReLU"
  bottom: "Convolution6"
  top: "Convolution6"
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "Convolution6"
  top: "Convolution7"
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm7"
  type: "BatchNorm"
  bottom: "Convolution7"
  top: "Convolution7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale7"
  type: "Scale"
  bottom: "Convolution7"
  top: "Convolution7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise3"
  type: "Eltwise"
  bottom: "Convolution7"
  bottom: "Eltwise2"
  top: "Eltwise3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU6"
  type: "ReLU"
  bottom: "Eltwise3"
  top: "Eltwise3"
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution8"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm8"
  type: "BatchNorm"
  bottom: "Convolution8"
  top: "Convolution8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale8"
  type: "Scale"
  bottom: "Convolution8"
  top: "Convolution8"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU7"
  type: "ReLU"
  bottom: "Convolution8"
  top: "Convolution8"
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "Convolution8"
  top: "Convolution9"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm9"
  type: "BatchNorm"
  bottom: "Convolution9"
  top: "Convolution9"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale9"
  type: "Scale"
  bottom: "Convolution9"
  top: "Convolution9"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution10"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm10"
  type: "BatchNorm"
  bottom: "Convolution10"
  top: "Convolution10"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale10"
  type: "Scale"
  bottom: "Convolution10"
  top: "Convolution10"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise4"
  type: "Eltwise"
  bottom: "Convolution9"
  bottom: "Convolution10"
  top: "Eltwise4"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU8"
  type: "ReLU"
  bottom: "Eltwise4"
  top: "Eltwise4"
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "Eltwise4"
  top: "Convolution11"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm11"
  type: "BatchNorm"
  bottom: "Convolution11"
  top: "Convolution11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale11"
  type: "Scale"
  bottom: "Convolution11"
  top: "Convolution11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU9"
  type: "ReLU"
  bottom: "Convolution11"
  top: "Convolution11"
}
layer {
  name: "Convolution12"
  type: "Convolution"
  bottom: "Convolution11"
  top: "Convolution12"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm12"
  type: "BatchNorm"
  bottom: "Convolution12"
  top: "Convolution12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale12"
  type: "Scale"
  bottom: "Convolution12"
  top: "Convolution12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise5"
  type: "Eltwise"
  bottom: "Convolution12"
  bottom: "Eltwise4"
  top: "Eltwise5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU10"
  type: "ReLU"
  bottom: "Eltwise5"
  top: "Eltwise5"
}
layer {
  name: "Convolution13"
  type: "Convolution"
  bottom: "Eltwise5"
  top: "Convolution13"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm13"
  type: "BatchNorm"
  bottom: "Convolution13"
  top: "Convolution13"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale13"
  type: "Scale"
  bottom: "Convolution13"
  top: "Convolution13"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU11"
  type: "ReLU"
  bottom: "Convolution13"
  top: "Convolution13"
}
layer {
  name: "Convolution14"
  type: "Convolution"
  bottom: "Convolution13"
  top: "Convolution14"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm14"
  type: "BatchNorm"
  bottom: "Convolution14"
  top: "Convolution14"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale14"
  type: "Scale"
  bottom: "Convolution14"
  top: "Convolution14"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise6"
  type: "Eltwise"
  bottom: "Convolution14"
  bottom: "Eltwise5"
  top: "Eltwise6"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU12"
  type: "ReLU"
  bottom: "Eltwise6"
  top: "Eltwise6"
}
layer {
  name: "Convolution15"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution15"
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm15"
  type: "BatchNorm"
  bottom: "Convolution15"
  top: "Convolution15"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale15"
  type: "Scale"
  bottom: "Convolution15"
  top: "Convolution15"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU13"
  type: "ReLU"
  bottom: "Convolution15"
  top: "Convolution15"
}
layer {
  name: "Convolution16"
  type: "Convolution"
  bottom: "Convolution15"
  top: "Convolution16"
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm16"
  type: "BatchNorm"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale16"
  type: "Scale"
  bottom: "Convolution16"
  top: "Convolution16"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution17"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution17"
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm17"
  type: "BatchNorm"
  bottom: "Convolution17"
  top: "Convolution17"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale17"
  type: "Scale"
  bottom: "Convolution17"
  top: "Convolution17"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise7"
  type: "Eltwise"
  bottom: "Convolution16"
  bottom: "Convolution17"
  top: "Eltwise7"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU14"
  type: "ReLU"
  bottom: "Eltwise7"
  top: "Eltwise7"
}
layer {
  name: "Convolution18"
  type: "Convolution"
  bottom: "Eltwise7"
  top: "Convolution18"
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm18"
  type: "BatchNorm"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale18"
  type: "Scale"
  bottom: "Convolution18"
  top: "Convolution18"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU15"
  type: "ReLU"
  bottom: "Convolution18"
  top: "Convolution18"
}
layer {
  name: "Convolution19"
  type: "Convolution"
  bottom: "Convolution18"
  top: "Convolution19"
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm19"
  type: "BatchNorm"
  bottom: "Convolution19"
  top: "Convolution19"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale19"
  type: "Scale"
  bottom: "Convolution19"
  top: "Convolution19"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise8"
  type: "Eltwise"
  bottom: "Convolution19"
  bottom: "Eltwise7"
  top: "Eltwise8"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU16"
  type: "ReLU"
  bottom: "Eltwise8"
  top: "Eltwise8"
}
layer {
  name: "Convolution20"
  type: "Convolution"
  bottom: "Eltwise8"
  top: "Convolution20"
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm20"
  type: "BatchNorm"
  bottom: "Convolution20"
  top: "Convolution20"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale20"
  type: "Scale"
  bottom: "Convolution20"
  top: "Convolution20"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU17"
  type: "ReLU"
  bottom: "Convolution20"
  top: "Convolution20"
}
layer {
  name: "Convolution21"
  type: "Convolution"
  bottom: "Convolution20"
  top: "Convolution21"
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm21"
  type: "BatchNorm"
  bottom: "Convolution21"
  top: "Convolution21"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale21"
  type: "Scale"
  bottom: "Convolution21"
  top: "Convolution21"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise9"
  type: "Eltwise"
  bottom: "Convolution21"
  bottom: "Eltwise8"
  top: "Eltwise9"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU18"
  type: "ReLU"
  bottom: "Eltwise9"
  top: "Eltwise9"
}
layer {
  name: "Deconvolution1"
  type: "Deconvolution"
  bottom: "Eltwise9"
  top: "Deconvolution1"
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 0
    kernel_size: 2
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm22"
  type: "BatchNorm"
  bottom: "Deconvolution1"
  top: "Deconvolution1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale22"
  type: "Scale"
  bottom: "Deconvolution1"
  top: "Deconvolution1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU19"
  type: "ReLU"
  bottom: "Deconvolution1"
  top: "Deconvolution1"
}
layer {
  name: "Deconvolution2"
  type: "Deconvolution"
  bottom: "Deconvolution1"
  top: "Deconvolution2"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 0
    kernel_size: 2
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm23"
  type: "BatchNorm"
  bottom: "Deconvolution2"
  top: "Deconvolution2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale23"
  type: "Scale"
  bottom: "Deconvolution2"
  top: "Deconvolution2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU20"
  type: "ReLU"
  bottom: "Deconvolution2"
  top: "Deconvolution2"
}
layer {
  name: "Deconvolution3"
  type: "Deconvolution"
  bottom: "Deconvolution2"
  top: "Deconvolution3"
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 0
    kernel_size: 2
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm24"
  type: "BatchNorm"
  bottom: "Deconvolution3"
  top: "Deconvolution3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale24"
  type: "Scale"
  bottom: "Deconvolution3"
  top: "Deconvolution3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU21"
  type: "ReLU"
  bottom: "Deconvolution3"
  top: "Deconvolution3"
}
layer {
  name: "Convolution22"
  type: "Convolution"
  bottom: "Deconvolution3"
  top: "Convolution22"
  convolution_param {
    num_output: 1
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "ReLU22"
  type: "ReLU"
  bottom: "Convolution22"
  top: "Convolution22"
}
layer {
  name: "Loss"
  type: "EuclideanLoss"
  bottom: "Convolution22"
  bottom: "Data5"
  top: "Loss"
  propagate_down: true
  propagate_down: false
}
I0305 17:56:49.594418 21892 layer_factory.hpp:77] Creating layer Data1
I0305 17:56:49.594614 21892 db_lmdb.cpp:35] Opened lmdb /data3/lzh/1000x224x224/raw_data_photon_dis
I0305 17:56:49.594648 21892 net.cpp:84] Creating Layer Data1
I0305 17:56:49.594662 21892 net.cpp:380] Data1 -> Data1
I0305 17:56:49.594709 21892 net.cpp:380] Data1 -> Data2
I0305 17:56:49.613857 21892 data_layer.cpp:45] output data size: 24,20,224,224
I0305 17:56:49.945933 21892 net.cpp:122] Setting up Data1
I0305 17:56:49.946048 21892 net.cpp:129] Top shape: 24 20 224 224 (24084480)
I0305 17:56:49.946071 21892 net.cpp:129] Top shape: 24 (24)
I0305 17:56:49.946087 21892 net.cpp:137] Memory required for data: 96338016
I0305 17:56:49.946120 21892 layer_factory.hpp:77] Creating layer Data3
I0305 17:56:49.946280 21892 db_lmdb.cpp:35] Opened lmdb /data3/lzh/1000x224x224/raw_data_photon_flux
I0305 17:56:49.946326 21892 net.cpp:84] Creating Layer Data3
I0305 17:56:49.946347 21892 net.cpp:380] Data3 -> Data3
I0305 17:56:49.946377 21892 net.cpp:380] Data3 -> Data4
I0305 17:56:49.954502 21892 data_layer.cpp:45] output data size: 24,20,224,224
I0305 17:56:50.253620 21892 net.cpp:122] Setting up Data3
I0305 17:56:50.253727 21892 net.cpp:129] Top shape: 24 20 224 224 (24084480)
I0305 17:56:50.253748 21892 net.cpp:129] Top shape: 24 (24)
I0305 17:56:50.253767 21892 net.cpp:137] Memory required for data: 192676032
I0305 17:56:50.253782 21892 layer_factory.hpp:77] Creating layer Data5
I0305 17:56:50.253968 21892 db_lmdb.cpp:35] Opened lmdb /data3/lzh/1000x224x224/raw_data_conv
I0305 17:56:50.254007 21892 net.cpp:84] Creating Layer Data5
I0305 17:56:50.254026 21892 net.cpp:380] Data5 -> Data5
I0305 17:56:50.254055 21892 net.cpp:380] Data5 -> Data6
I0305 17:56:50.254251 21892 data_layer.cpp:45] output data size: 24,1,224,224
I0305 17:56:50.293728 21892 net.cpp:122] Setting up Data5
I0305 17:56:50.293784 21892 net.cpp:129] Top shape: 24 1 224 224 (1204224)
I0305 17:56:50.293794 21892 net.cpp:129] Top shape: 24 (24)
I0305 17:56:50.293799 21892 net.cpp:137] Memory required for data: 197493024
I0305 17:56:50.293808 21892 layer_factory.hpp:77] Creating layer Silence
I0305 17:56:50.293859 21892 net.cpp:84] Creating Layer Silence
I0305 17:56:50.293875 21892 net.cpp:406] Silence <- Data2
I0305 17:56:50.293932 21892 net.cpp:406] Silence <- Data4
I0305 17:56:50.293951 21892 net.cpp:406] Silence <- Data6
I0305 17:56:50.293965 21892 net.cpp:122] Setting up Silence
I0305 17:56:50.293999 21892 net.cpp:137] Memory required for data: 197493024
I0305 17:56:50.294006 21892 layer_factory.hpp:77] Creating layer Concat1
I0305 17:56:50.294034 21892 net.cpp:84] Creating Layer Concat1
I0305 17:56:50.294042 21892 net.cpp:406] Concat1 <- Data1
I0305 17:56:50.294067 21892 net.cpp:406] Concat1 <- Data3
I0305 17:56:50.294078 21892 net.cpp:380] Concat1 -> Concat1
I0305 17:56:50.294145 21892 net.cpp:122] Setting up Concat1
I0305 17:56:50.294153 21892 net.cpp:129] Top shape: 24 40 224 224 (48168960)
I0305 17:56:50.294158 21892 net.cpp:137] Memory required for data: 390168864
I0305 17:56:50.294162 21892 layer_factory.hpp:77] Creating layer Convolution1
I0305 17:56:50.294184 21892 net.cpp:84] Creating Layer Convolution1
I0305 17:56:50.294189 21892 net.cpp:406] Convolution1 <- Concat1
I0305 17:56:50.294242 21892 net.cpp:380] Convolution1 -> Convolution1
I0305 17:56:50.299185 21892 net.cpp:122] Setting up Convolution1
I0305 17:56:50.299230 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:50.299240 21892 net.cpp:137] Memory required for data: 428704032
I0305 17:56:50.299274 21892 layer_factory.hpp:77] Creating layer BatchNorm1
I0305 17:56:50.299315 21892 net.cpp:84] Creating Layer BatchNorm1
I0305 17:56:50.299331 21892 net.cpp:406] BatchNorm1 <- Convolution1
I0305 17:56:50.299360 21892 net.cpp:367] BatchNorm1 -> Convolution1 (in-place)
I0305 17:56:50.299669 21892 net.cpp:122] Setting up BatchNorm1
I0305 17:56:50.299695 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:50.299708 21892 net.cpp:137] Memory required for data: 467239200
I0305 17:56:50.299734 21892 layer_factory.hpp:77] Creating layer Scale1
I0305 17:56:50.299772 21892 net.cpp:84] Creating Layer Scale1
I0305 17:56:50.299782 21892 net.cpp:406] Scale1 <- Convolution1
I0305 17:56:50.299793 21892 net.cpp:367] Scale1 -> Convolution1 (in-place)
I0305 17:56:50.299897 21892 layer_factory.hpp:77] Creating layer Scale1
I0305 17:56:50.300109 21892 net.cpp:122] Setting up Scale1
I0305 17:56:50.300127 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:50.300137 21892 net.cpp:137] Memory required for data: 505774368
I0305 17:56:50.300148 21892 layer_factory.hpp:77] Creating layer Convolution1_Scale1_0_split
I0305 17:56:50.300174 21892 net.cpp:84] Creating Layer Convolution1_Scale1_0_split
I0305 17:56:50.300194 21892 net.cpp:406] Convolution1_Scale1_0_split <- Convolution1
I0305 17:56:50.300210 21892 net.cpp:380] Convolution1_Scale1_0_split -> Convolution1_Scale1_0_split_0
I0305 17:56:50.300231 21892 net.cpp:380] Convolution1_Scale1_0_split -> Convolution1_Scale1_0_split_1
I0305 17:56:50.300302 21892 net.cpp:122] Setting up Convolution1_Scale1_0_split
I0305 17:56:50.300343 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:50.300357 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:50.300371 21892 net.cpp:137] Memory required for data: 582844704
I0305 17:56:50.300385 21892 layer_factory.hpp:77] Creating layer Convolution2
I0305 17:56:50.300420 21892 net.cpp:84] Creating Layer Convolution2
I0305 17:56:50.300433 21892 net.cpp:406] Convolution2 <- Convolution1_Scale1_0_split_0
I0305 17:56:50.300475 21892 net.cpp:380] Convolution2 -> Convolution2
I0305 17:56:50.300945 21892 net.cpp:122] Setting up Convolution2
I0305 17:56:50.300968 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:50.300977 21892 net.cpp:137] Memory required for data: 621379872
I0305 17:56:50.300992 21892 layer_factory.hpp:77] Creating layer BatchNorm2
I0305 17:56:50.301034 21892 net.cpp:84] Creating Layer BatchNorm2
I0305 17:56:50.301051 21892 net.cpp:406] BatchNorm2 <- Convolution2
I0305 17:56:50.301077 21892 net.cpp:367] BatchNorm2 -> Convolution2 (in-place)
I0305 17:56:50.301401 21892 net.cpp:122] Setting up BatchNorm2
I0305 17:56:50.301424 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:50.301439 21892 net.cpp:137] Memory required for data: 659915040
I0305 17:56:50.301457 21892 layer_factory.hpp:77] Creating layer Scale2
I0305 17:56:50.301488 21892 net.cpp:84] Creating Layer Scale2
I0305 17:56:50.301497 21892 net.cpp:406] Scale2 <- Convolution2
I0305 17:56:50.301507 21892 net.cpp:367] Scale2 -> Convolution2 (in-place)
I0305 17:56:50.301575 21892 layer_factory.hpp:77] Creating layer Scale2
I0305 17:56:50.301764 21892 net.cpp:122] Setting up Scale2
I0305 17:56:50.301802 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:50.301815 21892 net.cpp:137] Memory required for data: 698450208
I0305 17:56:50.301828 21892 layer_factory.hpp:77] Creating layer ReLU1
I0305 17:56:50.301839 21892 net.cpp:84] Creating Layer ReLU1
I0305 17:56:50.301847 21892 net.cpp:406] ReLU1 <- Convolution2
I0305 17:56:50.301857 21892 net.cpp:367] ReLU1 -> Convolution2 (in-place)
I0305 17:56:50.301868 21892 net.cpp:122] Setting up ReLU1
I0305 17:56:50.301877 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:50.301887 21892 net.cpp:137] Memory required for data: 736985376
I0305 17:56:50.301900 21892 layer_factory.hpp:77] Creating layer Convolution3
I0305 17:56:50.301944 21892 net.cpp:84] Creating Layer Convolution3
I0305 17:56:50.301960 21892 net.cpp:406] Convolution3 <- Convolution2
I0305 17:56:50.301978 21892 net.cpp:380] Convolution3 -> Convolution3
I0305 17:56:50.302778 21892 net.cpp:122] Setting up Convolution3
I0305 17:56:50.302811 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:50.302825 21892 net.cpp:137] Memory required for data: 775520544
I0305 17:56:50.302845 21892 layer_factory.hpp:77] Creating layer BatchNorm3
I0305 17:56:50.302883 21892 net.cpp:84] Creating Layer BatchNorm3
I0305 17:56:50.302899 21892 net.cpp:406] BatchNorm3 <- Convolution3
I0305 17:56:50.302917 21892 net.cpp:367] BatchNorm3 -> Convolution3 (in-place)
I0305 17:56:50.303217 21892 net.cpp:122] Setting up BatchNorm3
I0305 17:56:50.303256 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:50.303270 21892 net.cpp:137] Memory required for data: 814055712
I0305 17:56:50.303298 21892 layer_factory.hpp:77] Creating layer Scale3
I0305 17:56:50.303311 21892 net.cpp:84] Creating Layer Scale3
I0305 17:56:50.303319 21892 net.cpp:406] Scale3 <- Convolution3
I0305 17:56:50.303329 21892 net.cpp:367] Scale3 -> Convolution3 (in-place)
I0305 17:56:50.303393 21892 layer_factory.hpp:77] Creating layer Scale3
I0305 17:56:50.303578 21892 net.cpp:122] Setting up Scale3
I0305 17:56:50.303599 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:50.303612 21892 net.cpp:137] Memory required for data: 852590880
I0305 17:56:50.303629 21892 layer_factory.hpp:77] Creating layer Eltwise1
I0305 17:56:50.303658 21892 net.cpp:84] Creating Layer Eltwise1
I0305 17:56:50.303668 21892 net.cpp:406] Eltwise1 <- Convolution3
I0305 17:56:50.303676 21892 net.cpp:406] Eltwise1 <- Convolution1_Scale1_0_split_1
I0305 17:56:50.303689 21892 net.cpp:380] Eltwise1 -> Eltwise1
I0305 17:56:50.303742 21892 net.cpp:122] Setting up Eltwise1
I0305 17:56:50.303761 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:50.303772 21892 net.cpp:137] Memory required for data: 891126048
I0305 17:56:50.303784 21892 layer_factory.hpp:77] Creating layer ReLU2
I0305 17:56:50.303818 21892 net.cpp:84] Creating Layer ReLU2
I0305 17:56:50.303828 21892 net.cpp:406] ReLU2 <- Eltwise1
I0305 17:56:50.303838 21892 net.cpp:367] ReLU2 -> Eltwise1 (in-place)
I0305 17:56:50.303848 21892 net.cpp:122] Setting up ReLU2
I0305 17:56:50.303856 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:50.303864 21892 net.cpp:137] Memory required for data: 929661216
I0305 17:56:50.303886 21892 layer_factory.hpp:77] Creating layer Eltwise1_ReLU2_0_split
I0305 17:56:50.303905 21892 net.cpp:84] Creating Layer Eltwise1_ReLU2_0_split
I0305 17:56:50.303918 21892 net.cpp:406] Eltwise1_ReLU2_0_split <- Eltwise1
I0305 17:56:50.303933 21892 net.cpp:380] Eltwise1_ReLU2_0_split -> Eltwise1_ReLU2_0_split_0
I0305 17:56:50.303951 21892 net.cpp:380] Eltwise1_ReLU2_0_split -> Eltwise1_ReLU2_0_split_1
I0305 17:56:50.304003 21892 net.cpp:122] Setting up Eltwise1_ReLU2_0_split
I0305 17:56:50.304013 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:50.304023 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:50.304029 21892 net.cpp:137] Memory required for data: 1006731552
I0305 17:56:50.304038 21892 layer_factory.hpp:77] Creating layer Convolution4
I0305 17:56:50.304057 21892 net.cpp:84] Creating Layer Convolution4
I0305 17:56:50.304070 21892 net.cpp:406] Convolution4 <- Eltwise1_ReLU2_0_split_0
I0305 17:56:50.304088 21892 net.cpp:380] Convolution4 -> Convolution4
I0305 17:56:50.304502 21892 net.cpp:122] Setting up Convolution4
I0305 17:56:50.304517 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:50.304524 21892 net.cpp:137] Memory required for data: 1045266720
I0305 17:56:50.304535 21892 layer_factory.hpp:77] Creating layer BatchNorm4
I0305 17:56:50.304546 21892 net.cpp:84] Creating Layer BatchNorm4
I0305 17:56:50.304558 21892 net.cpp:406] BatchNorm4 <- Convolution4
I0305 17:56:50.304574 21892 net.cpp:367] BatchNorm4 -> Convolution4 (in-place)
I0305 17:56:50.304891 21892 net.cpp:122] Setting up BatchNorm4
I0305 17:56:50.304905 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:50.304913 21892 net.cpp:137] Memory required for data: 1083801888
I0305 17:56:50.304924 21892 layer_factory.hpp:77] Creating layer Scale4
I0305 17:56:50.304950 21892 net.cpp:84] Creating Layer Scale4
I0305 17:56:50.304965 21892 net.cpp:406] Scale4 <- Convolution4
I0305 17:56:50.304980 21892 net.cpp:367] Scale4 -> Convolution4 (in-place)
I0305 17:56:50.305047 21892 layer_factory.hpp:77] Creating layer Scale4
I0305 17:56:50.305245 21892 net.cpp:122] Setting up Scale4
I0305 17:56:50.305258 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:50.305265 21892 net.cpp:137] Memory required for data: 1122337056
I0305 17:56:50.305284 21892 layer_factory.hpp:77] Creating layer ReLU3
I0305 17:56:50.305295 21892 net.cpp:84] Creating Layer ReLU3
I0305 17:56:50.305310 21892 net.cpp:406] ReLU3 <- Convolution4
I0305 17:56:50.305323 21892 net.cpp:367] ReLU3 -> Convolution4 (in-place)
I0305 17:56:50.305338 21892 net.cpp:122] Setting up ReLU3
I0305 17:56:50.305352 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:50.305363 21892 net.cpp:137] Memory required for data: 1160872224
I0305 17:56:50.305375 21892 layer_factory.hpp:77] Creating layer Convolution5
I0305 17:56:50.305393 21892 net.cpp:84] Creating Layer Convolution5
I0305 17:56:50.305405 21892 net.cpp:406] Convolution5 <- Convolution4
I0305 17:56:50.305428 21892 net.cpp:380] Convolution5 -> Convolution5
I0305 17:56:50.305860 21892 net.cpp:122] Setting up Convolution5
I0305 17:56:50.305883 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:50.305896 21892 net.cpp:137] Memory required for data: 1199407392
I0305 17:56:50.305913 21892 layer_factory.hpp:77] Creating layer BatchNorm5
I0305 17:56:50.305943 21892 net.cpp:84] Creating Layer BatchNorm5
I0305 17:56:50.305953 21892 net.cpp:406] BatchNorm5 <- Convolution5
I0305 17:56:50.305963 21892 net.cpp:367] BatchNorm5 -> Convolution5 (in-place)
I0305 17:56:50.306246 21892 net.cpp:122] Setting up BatchNorm5
I0305 17:56:50.306267 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:50.306290 21892 net.cpp:137] Memory required for data: 1237942560
I0305 17:56:50.306310 21892 layer_factory.hpp:77] Creating layer Scale5
I0305 17:56:50.306321 21892 net.cpp:84] Creating Layer Scale5
I0305 17:56:50.306329 21892 net.cpp:406] Scale5 <- Convolution5
I0305 17:56:50.306344 21892 net.cpp:367] Scale5 -> Convolution5 (in-place)
I0305 17:56:50.306414 21892 layer_factory.hpp:77] Creating layer Scale5
I0305 17:56:50.306599 21892 net.cpp:122] Setting up Scale5
I0305 17:56:50.306619 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:50.306632 21892 net.cpp:137] Memory required for data: 1276477728
I0305 17:56:50.306658 21892 layer_factory.hpp:77] Creating layer Eltwise2
I0305 17:56:50.306670 21892 net.cpp:84] Creating Layer Eltwise2
I0305 17:56:50.306679 21892 net.cpp:406] Eltwise2 <- Convolution5
I0305 17:56:50.306687 21892 net.cpp:406] Eltwise2 <- Eltwise1_ReLU2_0_split_1
I0305 17:56:50.306699 21892 net.cpp:380] Eltwise2 -> Eltwise2
I0305 17:56:50.306746 21892 net.cpp:122] Setting up Eltwise2
I0305 17:56:50.306764 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:50.306777 21892 net.cpp:137] Memory required for data: 1315012896
I0305 17:56:50.306789 21892 layer_factory.hpp:77] Creating layer ReLU4
I0305 17:56:50.306805 21892 net.cpp:84] Creating Layer ReLU4
I0305 17:56:50.306814 21892 net.cpp:406] ReLU4 <- Eltwise2
I0305 17:56:50.306825 21892 net.cpp:367] ReLU4 -> Eltwise2 (in-place)
I0305 17:56:50.306834 21892 net.cpp:122] Setting up ReLU4
I0305 17:56:50.306843 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:50.306850 21892 net.cpp:137] Memory required for data: 1353548064
I0305 17:56:50.306859 21892 layer_factory.hpp:77] Creating layer Eltwise2_ReLU4_0_split
I0305 17:56:50.306879 21892 net.cpp:84] Creating Layer Eltwise2_ReLU4_0_split
I0305 17:56:50.306896 21892 net.cpp:406] Eltwise2_ReLU4_0_split <- Eltwise2
I0305 17:56:50.306912 21892 net.cpp:380] Eltwise2_ReLU4_0_split -> Eltwise2_ReLU4_0_split_0
I0305 17:56:50.306929 21892 net.cpp:380] Eltwise2_ReLU4_0_split -> Eltwise2_ReLU4_0_split_1
I0305 17:56:50.306990 21892 net.cpp:122] Setting up Eltwise2_ReLU4_0_split
I0305 17:56:50.307001 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:50.307011 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:50.307018 21892 net.cpp:137] Memory required for data: 1430618400
I0305 17:56:50.307025 21892 layer_factory.hpp:77] Creating layer Convolution6
I0305 17:56:50.307059 21892 net.cpp:84] Creating Layer Convolution6
I0305 17:56:50.307073 21892 net.cpp:406] Convolution6 <- Eltwise2_ReLU4_0_split_0
I0305 17:56:50.307090 21892 net.cpp:380] Convolution6 -> Convolution6
I0305 17:56:50.307559 21892 net.cpp:122] Setting up Convolution6
I0305 17:56:50.307585 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:50.307597 21892 net.cpp:137] Memory required for data: 1469153568
I0305 17:56:50.307616 21892 layer_factory.hpp:77] Creating layer BatchNorm6
I0305 17:56:50.307633 21892 net.cpp:84] Creating Layer BatchNorm6
I0305 17:56:50.307647 21892 net.cpp:406] BatchNorm6 <- Convolution6
I0305 17:56:50.307663 21892 net.cpp:367] BatchNorm6 -> Convolution6 (in-place)
I0305 17:56:50.308365 21892 net.cpp:122] Setting up BatchNorm6
I0305 17:56:50.308387 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:50.308398 21892 net.cpp:137] Memory required for data: 1507688736
I0305 17:56:50.308411 21892 layer_factory.hpp:77] Creating layer Scale6
I0305 17:56:50.308434 21892 net.cpp:84] Creating Layer Scale6
I0305 17:56:50.308445 21892 net.cpp:406] Scale6 <- Convolution6
I0305 17:56:50.308462 21892 net.cpp:367] Scale6 -> Convolution6 (in-place)
I0305 17:56:50.308545 21892 layer_factory.hpp:77] Creating layer Scale6
I0305 17:56:50.308742 21892 net.cpp:122] Setting up Scale6
I0305 17:56:50.308756 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:50.308780 21892 net.cpp:137] Memory required for data: 1546223904
I0305 17:56:50.308827 21892 layer_factory.hpp:77] Creating layer ReLU5
I0305 17:56:50.308869 21892 net.cpp:84] Creating Layer ReLU5
I0305 17:56:50.308903 21892 net.cpp:406] ReLU5 <- Convolution6
I0305 17:56:50.308919 21892 net.cpp:367] ReLU5 -> Convolution6 (in-place)
I0305 17:56:50.308933 21892 net.cpp:122] Setting up ReLU5
I0305 17:56:50.308944 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:50.308951 21892 net.cpp:137] Memory required for data: 1584759072
I0305 17:56:50.308959 21892 layer_factory.hpp:77] Creating layer Convolution7
I0305 17:56:50.308981 21892 net.cpp:84] Creating Layer Convolution7
I0305 17:56:50.308995 21892 net.cpp:406] Convolution7 <- Convolution6
I0305 17:56:50.309012 21892 net.cpp:380] Convolution7 -> Convolution7
I0305 17:56:50.309461 21892 net.cpp:122] Setting up Convolution7
I0305 17:56:50.309481 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:50.309494 21892 net.cpp:137] Memory required for data: 1623294240
I0305 17:56:50.309514 21892 layer_factory.hpp:77] Creating layer BatchNorm7
I0305 17:56:50.309530 21892 net.cpp:84] Creating Layer BatchNorm7
I0305 17:56:50.309543 21892 net.cpp:406] BatchNorm7 <- Convolution7
I0305 17:56:50.309561 21892 net.cpp:367] BatchNorm7 -> Convolution7 (in-place)
I0305 17:56:50.309862 21892 net.cpp:122] Setting up BatchNorm7
I0305 17:56:50.309888 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:50.309901 21892 net.cpp:137] Memory required for data: 1661829408
I0305 17:56:50.309921 21892 layer_factory.hpp:77] Creating layer Scale7
I0305 17:56:50.309954 21892 net.cpp:84] Creating Layer Scale7
I0305 17:56:50.309964 21892 net.cpp:406] Scale7 <- Convolution7
I0305 17:56:50.309993 21892 net.cpp:367] Scale7 -> Convolution7 (in-place)
I0305 17:56:50.310055 21892 layer_factory.hpp:77] Creating layer Scale7
I0305 17:56:50.310241 21892 net.cpp:122] Setting up Scale7
I0305 17:56:50.310261 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:50.310272 21892 net.cpp:137] Memory required for data: 1700364576
I0305 17:56:50.310287 21892 layer_factory.hpp:77] Creating layer Eltwise3
I0305 17:56:50.310300 21892 net.cpp:84] Creating Layer Eltwise3
I0305 17:56:50.310308 21892 net.cpp:406] Eltwise3 <- Convolution7
I0305 17:56:50.310317 21892 net.cpp:406] Eltwise3 <- Eltwise2_ReLU4_0_split_1
I0305 17:56:50.310326 21892 net.cpp:380] Eltwise3 -> Eltwise3
I0305 17:56:50.310362 21892 net.cpp:122] Setting up Eltwise3
I0305 17:56:50.310379 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:50.310390 21892 net.cpp:137] Memory required for data: 1738899744
I0305 17:56:50.310418 21892 layer_factory.hpp:77] Creating layer ReLU6
I0305 17:56:50.310446 21892 net.cpp:84] Creating Layer ReLU6
I0305 17:56:50.310459 21892 net.cpp:406] ReLU6 <- Eltwise3
I0305 17:56:50.310472 21892 net.cpp:367] ReLU6 -> Eltwise3 (in-place)
I0305 17:56:50.310482 21892 net.cpp:122] Setting up ReLU6
I0305 17:56:50.310492 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:50.310498 21892 net.cpp:137] Memory required for data: 1777434912
I0305 17:56:50.310506 21892 layer_factory.hpp:77] Creating layer Eltwise3_ReLU6_0_split
I0305 17:56:50.310515 21892 net.cpp:84] Creating Layer Eltwise3_ReLU6_0_split
I0305 17:56:50.310525 21892 net.cpp:406] Eltwise3_ReLU6_0_split <- Eltwise3
I0305 17:56:50.310541 21892 net.cpp:380] Eltwise3_ReLU6_0_split -> Eltwise3_ReLU6_0_split_0
I0305 17:56:50.310559 21892 net.cpp:380] Eltwise3_ReLU6_0_split -> Eltwise3_ReLU6_0_split_1
I0305 17:56:50.310622 21892 net.cpp:122] Setting up Eltwise3_ReLU6_0_split
I0305 17:56:50.310637 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:50.310647 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:50.310653 21892 net.cpp:137] Memory required for data: 1854505248
I0305 17:56:50.310660 21892 layer_factory.hpp:77] Creating layer Convolution8
I0305 17:56:50.310683 21892 net.cpp:84] Creating Layer Convolution8
I0305 17:56:50.310693 21892 net.cpp:406] Convolution8 <- Eltwise3_ReLU6_0_split_0
I0305 17:56:50.310710 21892 net.cpp:380] Convolution8 -> Convolution8
I0305 17:56:50.319759 21892 net.cpp:122] Setting up Convolution8
I0305 17:56:50.319864 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:50.319875 21892 net.cpp:137] Memory required for data: 1873772832
I0305 17:56:50.319892 21892 layer_factory.hpp:77] Creating layer BatchNorm8
I0305 17:56:50.319936 21892 net.cpp:84] Creating Layer BatchNorm8
I0305 17:56:50.319952 21892 net.cpp:406] BatchNorm8 <- Convolution8
I0305 17:56:50.319970 21892 net.cpp:367] BatchNorm8 -> Convolution8 (in-place)
I0305 17:56:50.320340 21892 net.cpp:122] Setting up BatchNorm8
I0305 17:56:50.320359 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:50.320369 21892 net.cpp:137] Memory required for data: 1893040416
I0305 17:56:50.320421 21892 layer_factory.hpp:77] Creating layer Scale8
I0305 17:56:50.320447 21892 net.cpp:84] Creating Layer Scale8
I0305 17:56:50.320461 21892 net.cpp:406] Scale8 <- Convolution8
I0305 17:56:50.320479 21892 net.cpp:367] Scale8 -> Convolution8 (in-place)
I0305 17:56:50.320930 21892 layer_factory.hpp:77] Creating layer Scale8
I0305 17:56:50.321193 21892 net.cpp:122] Setting up Scale8
I0305 17:56:50.321223 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:50.321239 21892 net.cpp:137] Memory required for data: 1912308000
I0305 17:56:50.321259 21892 layer_factory.hpp:77] Creating layer ReLU7
I0305 17:56:50.321296 21892 net.cpp:84] Creating Layer ReLU7
I0305 17:56:50.321312 21892 net.cpp:406] ReLU7 <- Convolution8
I0305 17:56:50.321323 21892 net.cpp:367] ReLU7 -> Convolution8 (in-place)
I0305 17:56:50.321336 21892 net.cpp:122] Setting up ReLU7
I0305 17:56:50.321353 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:50.321367 21892 net.cpp:137] Memory required for data: 1931575584
I0305 17:56:50.321379 21892 layer_factory.hpp:77] Creating layer Convolution9
I0305 17:56:50.321403 21892 net.cpp:84] Creating Layer Convolution9
I0305 17:56:50.321418 21892 net.cpp:406] Convolution9 <- Convolution8
I0305 17:56:50.321435 21892 net.cpp:380] Convolution9 -> Convolution9
I0305 17:56:50.322186 21892 net.cpp:122] Setting up Convolution9
I0305 17:56:50.322212 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:50.322227 21892 net.cpp:137] Memory required for data: 1950843168
I0305 17:56:50.322247 21892 layer_factory.hpp:77] Creating layer BatchNorm9
I0305 17:56:50.322268 21892 net.cpp:84] Creating Layer BatchNorm9
I0305 17:56:50.322280 21892 net.cpp:406] BatchNorm9 <- Convolution9
I0305 17:56:50.322299 21892 net.cpp:367] BatchNorm9 -> Convolution9 (in-place)
I0305 17:56:50.322582 21892 net.cpp:122] Setting up BatchNorm9
I0305 17:56:50.322603 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:50.322615 21892 net.cpp:137] Memory required for data: 1970110752
I0305 17:56:50.322654 21892 layer_factory.hpp:77] Creating layer Scale9
I0305 17:56:50.322677 21892 net.cpp:84] Creating Layer Scale9
I0305 17:56:50.322686 21892 net.cpp:406] Scale9 <- Convolution9
I0305 17:56:50.322698 21892 net.cpp:367] Scale9 -> Convolution9 (in-place)
I0305 17:56:50.322779 21892 layer_factory.hpp:77] Creating layer Scale9
I0305 17:56:50.322989 21892 net.cpp:122] Setting up Scale9
I0305 17:56:50.323004 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:50.323012 21892 net.cpp:137] Memory required for data: 1989378336
I0305 17:56:50.323022 21892 layer_factory.hpp:77] Creating layer Convolution10
I0305 17:56:50.323042 21892 net.cpp:84] Creating Layer Convolution10
I0305 17:56:50.323057 21892 net.cpp:406] Convolution10 <- Eltwise3_ReLU6_0_split_1
I0305 17:56:50.323077 21892 net.cpp:380] Convolution10 -> Convolution10
I0305 17:56:50.323449 21892 net.cpp:122] Setting up Convolution10
I0305 17:56:50.323477 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:50.323489 21892 net.cpp:137] Memory required for data: 2008645920
I0305 17:56:50.323518 21892 layer_factory.hpp:77] Creating layer BatchNorm10
I0305 17:56:50.323532 21892 net.cpp:84] Creating Layer BatchNorm10
I0305 17:56:50.323541 21892 net.cpp:406] BatchNorm10 <- Convolution10
I0305 17:56:50.323557 21892 net.cpp:367] BatchNorm10 -> Convolution10 (in-place)
I0305 17:56:50.323848 21892 net.cpp:122] Setting up BatchNorm10
I0305 17:56:50.323895 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:50.323911 21892 net.cpp:137] Memory required for data: 2027913504
I0305 17:56:50.323932 21892 layer_factory.hpp:77] Creating layer Scale10
I0305 17:56:50.323989 21892 net.cpp:84] Creating Layer Scale10
I0305 17:56:50.324003 21892 net.cpp:406] Scale10 <- Convolution10
I0305 17:56:50.324017 21892 net.cpp:367] Scale10 -> Convolution10 (in-place)
I0305 17:56:50.324103 21892 layer_factory.hpp:77] Creating layer Scale10
I0305 17:56:50.324287 21892 net.cpp:122] Setting up Scale10
I0305 17:56:50.324312 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:50.324326 21892 net.cpp:137] Memory required for data: 2047181088
I0305 17:56:50.324354 21892 layer_factory.hpp:77] Creating layer Eltwise4
I0305 17:56:50.324376 21892 net.cpp:84] Creating Layer Eltwise4
I0305 17:56:50.324384 21892 net.cpp:406] Eltwise4 <- Convolution9
I0305 17:56:50.324393 21892 net.cpp:406] Eltwise4 <- Convolution10
I0305 17:56:50.324405 21892 net.cpp:380] Eltwise4 -> Eltwise4
I0305 17:56:50.324453 21892 net.cpp:122] Setting up Eltwise4
I0305 17:56:50.324472 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:50.324484 21892 net.cpp:137] Memory required for data: 2066448672
I0305 17:56:50.324496 21892 layer_factory.hpp:77] Creating layer ReLU8
I0305 17:56:50.324514 21892 net.cpp:84] Creating Layer ReLU8
I0305 17:56:50.324523 21892 net.cpp:406] ReLU8 <- Eltwise4
I0305 17:56:50.324533 21892 net.cpp:367] ReLU8 -> Eltwise4 (in-place)
I0305 17:56:50.324543 21892 net.cpp:122] Setting up ReLU8
I0305 17:56:50.324553 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:50.324559 21892 net.cpp:137] Memory required for data: 2085716256
I0305 17:56:50.324566 21892 layer_factory.hpp:77] Creating layer Eltwise4_ReLU8_0_split
I0305 17:56:50.324580 21892 net.cpp:84] Creating Layer Eltwise4_ReLU8_0_split
I0305 17:56:50.324609 21892 net.cpp:406] Eltwise4_ReLU8_0_split <- Eltwise4
I0305 17:56:50.324625 21892 net.cpp:380] Eltwise4_ReLU8_0_split -> Eltwise4_ReLU8_0_split_0
I0305 17:56:50.324641 21892 net.cpp:380] Eltwise4_ReLU8_0_split -> Eltwise4_ReLU8_0_split_1
I0305 17:56:50.324717 21892 net.cpp:122] Setting up Eltwise4_ReLU8_0_split
I0305 17:56:50.324728 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:50.324736 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:50.324743 21892 net.cpp:137] Memory required for data: 2124251424
I0305 17:56:50.324750 21892 layer_factory.hpp:77] Creating layer Convolution11
I0305 17:56:50.324765 21892 net.cpp:84] Creating Layer Convolution11
I0305 17:56:50.324776 21892 net.cpp:406] Convolution11 <- Eltwise4_ReLU8_0_split_0
I0305 17:56:50.324795 21892 net.cpp:380] Convolution11 -> Convolution11
I0305 17:56:50.325537 21892 net.cpp:122] Setting up Convolution11
I0305 17:56:50.325564 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:50.325573 21892 net.cpp:137] Memory required for data: 2143519008
I0305 17:56:50.325595 21892 layer_factory.hpp:77] Creating layer BatchNorm11
I0305 17:56:50.325615 21892 net.cpp:84] Creating Layer BatchNorm11
I0305 17:56:50.325623 21892 net.cpp:406] BatchNorm11 <- Convolution11
I0305 17:56:50.325639 21892 net.cpp:367] BatchNorm11 -> Convolution11 (in-place)
I0305 17:56:50.325918 21892 net.cpp:122] Setting up BatchNorm11
I0305 17:56:50.325937 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:50.325945 21892 net.cpp:137] Memory required for data: 2162786592
I0305 17:56:50.325960 21892 layer_factory.hpp:77] Creating layer Scale11
I0305 17:56:50.325978 21892 net.cpp:84] Creating Layer Scale11
I0305 17:56:50.325991 21892 net.cpp:406] Scale11 <- Convolution11
I0305 17:56:50.326006 21892 net.cpp:367] Scale11 -> Convolution11 (in-place)
I0305 17:56:50.326074 21892 layer_factory.hpp:77] Creating layer Scale11
I0305 17:56:50.326246 21892 net.cpp:122] Setting up Scale11
I0305 17:56:50.326266 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:50.326273 21892 net.cpp:137] Memory required for data: 2182054176
I0305 17:56:50.326284 21892 layer_factory.hpp:77] Creating layer ReLU9
I0305 17:56:50.326331 21892 net.cpp:84] Creating Layer ReLU9
I0305 17:56:50.326344 21892 net.cpp:406] ReLU9 <- Convolution11
I0305 17:56:50.326359 21892 net.cpp:367] ReLU9 -> Convolution11 (in-place)
I0305 17:56:50.326375 21892 net.cpp:122] Setting up ReLU9
I0305 17:56:50.326388 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:50.326395 21892 net.cpp:137] Memory required for data: 2201321760
I0305 17:56:50.326402 21892 layer_factory.hpp:77] Creating layer Convolution12
I0305 17:56:50.326431 21892 net.cpp:84] Creating Layer Convolution12
I0305 17:56:50.326442 21892 net.cpp:406] Convolution12 <- Convolution11
I0305 17:56:50.326458 21892 net.cpp:380] Convolution12 -> Convolution12
I0305 17:56:50.327183 21892 net.cpp:122] Setting up Convolution12
I0305 17:56:50.327215 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:50.327225 21892 net.cpp:137] Memory required for data: 2220589344
I0305 17:56:50.327246 21892 layer_factory.hpp:77] Creating layer BatchNorm12
I0305 17:56:50.327260 21892 net.cpp:84] Creating Layer BatchNorm12
I0305 17:56:50.327268 21892 net.cpp:406] BatchNorm12 <- Convolution12
I0305 17:56:50.327286 21892 net.cpp:367] BatchNorm12 -> Convolution12 (in-place)
I0305 17:56:50.327589 21892 net.cpp:122] Setting up BatchNorm12
I0305 17:56:50.327605 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:50.327616 21892 net.cpp:137] Memory required for data: 2239856928
I0305 17:56:50.327636 21892 layer_factory.hpp:77] Creating layer Scale12
I0305 17:56:50.327656 21892 net.cpp:84] Creating Layer Scale12
I0305 17:56:50.327668 21892 net.cpp:406] Scale12 <- Convolution12
I0305 17:56:50.327682 21892 net.cpp:367] Scale12 -> Convolution12 (in-place)
I0305 17:56:50.327742 21892 layer_factory.hpp:77] Creating layer Scale12
I0305 17:56:50.327917 21892 net.cpp:122] Setting up Scale12
I0305 17:56:50.327939 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:50.327956 21892 net.cpp:137] Memory required for data: 2259124512
I0305 17:56:50.327972 21892 layer_factory.hpp:77] Creating layer Eltwise5
I0305 17:56:50.328001 21892 net.cpp:84] Creating Layer Eltwise5
I0305 17:56:50.328016 21892 net.cpp:406] Eltwise5 <- Convolution12
I0305 17:56:50.328029 21892 net.cpp:406] Eltwise5 <- Eltwise4_ReLU8_0_split_1
I0305 17:56:50.328042 21892 net.cpp:380] Eltwise5 -> Eltwise5
I0305 17:56:50.328083 21892 net.cpp:122] Setting up Eltwise5
I0305 17:56:50.328097 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:50.328109 21892 net.cpp:137] Memory required for data: 2278392096
I0305 17:56:50.328121 21892 layer_factory.hpp:77] Creating layer ReLU10
I0305 17:56:50.328136 21892 net.cpp:84] Creating Layer ReLU10
I0305 17:56:50.328148 21892 net.cpp:406] ReLU10 <- Eltwise5
I0305 17:56:50.328163 21892 net.cpp:367] ReLU10 -> Eltwise5 (in-place)
I0305 17:56:50.328178 21892 net.cpp:122] Setting up ReLU10
I0305 17:56:50.328191 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:50.328199 21892 net.cpp:137] Memory required for data: 2297659680
I0305 17:56:50.328217 21892 layer_factory.hpp:77] Creating layer Eltwise5_ReLU10_0_split
I0305 17:56:50.328228 21892 net.cpp:84] Creating Layer Eltwise5_ReLU10_0_split
I0305 17:56:50.328235 21892 net.cpp:406] Eltwise5_ReLU10_0_split <- Eltwise5
I0305 17:56:50.328245 21892 net.cpp:380] Eltwise5_ReLU10_0_split -> Eltwise5_ReLU10_0_split_0
I0305 17:56:50.328259 21892 net.cpp:380] Eltwise5_ReLU10_0_split -> Eltwise5_ReLU10_0_split_1
I0305 17:56:50.328321 21892 net.cpp:122] Setting up Eltwise5_ReLU10_0_split
I0305 17:56:50.328337 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:50.328351 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:50.328361 21892 net.cpp:137] Memory required for data: 2336194848
I0305 17:56:50.328368 21892 layer_factory.hpp:77] Creating layer Convolution13
I0305 17:56:50.328382 21892 net.cpp:84] Creating Layer Convolution13
I0305 17:56:50.328390 21892 net.cpp:406] Convolution13 <- Eltwise5_ReLU10_0_split_0
I0305 17:56:50.328402 21892 net.cpp:380] Convolution13 -> Convolution13
I0305 17:56:50.329071 21892 net.cpp:122] Setting up Convolution13
I0305 17:56:50.329092 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:50.329104 21892 net.cpp:137] Memory required for data: 2355462432
I0305 17:56:50.329120 21892 layer_factory.hpp:77] Creating layer BatchNorm13
I0305 17:56:50.329138 21892 net.cpp:84] Creating Layer BatchNorm13
I0305 17:56:50.329149 21892 net.cpp:406] BatchNorm13 <- Convolution13
I0305 17:56:50.329164 21892 net.cpp:367] BatchNorm13 -> Convolution13 (in-place)
I0305 17:56:50.329428 21892 net.cpp:122] Setting up BatchNorm13
I0305 17:56:50.329447 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:50.329460 21892 net.cpp:137] Memory required for data: 2374730016
I0305 17:56:50.329479 21892 layer_factory.hpp:77] Creating layer Scale13
I0305 17:56:50.329512 21892 net.cpp:84] Creating Layer Scale13
I0305 17:56:50.329521 21892 net.cpp:406] Scale13 <- Convolution13
I0305 17:56:50.329531 21892 net.cpp:367] Scale13 -> Convolution13 (in-place)
I0305 17:56:50.329596 21892 layer_factory.hpp:77] Creating layer Scale13
I0305 17:56:50.329772 21892 net.cpp:122] Setting up Scale13
I0305 17:56:50.329793 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:50.329805 21892 net.cpp:137] Memory required for data: 2393997600
I0305 17:56:50.329818 21892 layer_factory.hpp:77] Creating layer ReLU11
I0305 17:56:50.329838 21892 net.cpp:84] Creating Layer ReLU11
I0305 17:56:50.329846 21892 net.cpp:406] ReLU11 <- Convolution13
I0305 17:56:50.329855 21892 net.cpp:367] ReLU11 -> Convolution13 (in-place)
I0305 17:56:50.329864 21892 net.cpp:122] Setting up ReLU11
I0305 17:56:50.329877 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:50.329890 21892 net.cpp:137] Memory required for data: 2413265184
I0305 17:56:50.329900 21892 layer_factory.hpp:77] Creating layer Convolution14
I0305 17:56:50.329919 21892 net.cpp:84] Creating Layer Convolution14
I0305 17:56:50.329931 21892 net.cpp:406] Convolution14 <- Convolution13
I0305 17:56:50.329948 21892 net.cpp:380] Convolution14 -> Convolution14
I0305 17:56:50.330683 21892 net.cpp:122] Setting up Convolution14
I0305 17:56:50.330700 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:50.330724 21892 net.cpp:137] Memory required for data: 2432532768
I0305 17:56:50.330740 21892 layer_factory.hpp:77] Creating layer BatchNorm14
I0305 17:56:50.330770 21892 net.cpp:84] Creating Layer BatchNorm14
I0305 17:56:50.330785 21892 net.cpp:406] BatchNorm14 <- Convolution14
I0305 17:56:50.330806 21892 net.cpp:367] BatchNorm14 -> Convolution14 (in-place)
I0305 17:56:50.331087 21892 net.cpp:122] Setting up BatchNorm14
I0305 17:56:50.331109 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:50.331120 21892 net.cpp:137] Memory required for data: 2451800352
I0305 17:56:50.331140 21892 layer_factory.hpp:77] Creating layer Scale14
I0305 17:56:50.331157 21892 net.cpp:84] Creating Layer Scale14
I0305 17:56:50.331169 21892 net.cpp:406] Scale14 <- Convolution14
I0305 17:56:50.331182 21892 net.cpp:367] Scale14 -> Convolution14 (in-place)
I0305 17:56:50.331234 21892 layer_factory.hpp:77] Creating layer Scale14
I0305 17:56:50.331466 21892 net.cpp:122] Setting up Scale14
I0305 17:56:50.331485 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:50.331498 21892 net.cpp:137] Memory required for data: 2471067936
I0305 17:56:50.331521 21892 layer_factory.hpp:77] Creating layer Eltwise6
I0305 17:56:50.331533 21892 net.cpp:84] Creating Layer Eltwise6
I0305 17:56:50.331542 21892 net.cpp:406] Eltwise6 <- Convolution14
I0305 17:56:50.331552 21892 net.cpp:406] Eltwise6 <- Eltwise5_ReLU10_0_split_1
I0305 17:56:50.331568 21892 net.cpp:380] Eltwise6 -> Eltwise6
I0305 17:56:50.331612 21892 net.cpp:122] Setting up Eltwise6
I0305 17:56:50.331629 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:50.331641 21892 net.cpp:137] Memory required for data: 2490335520
I0305 17:56:50.331653 21892 layer_factory.hpp:77] Creating layer ReLU12
I0305 17:56:50.331668 21892 net.cpp:84] Creating Layer ReLU12
I0305 17:56:50.331707 21892 net.cpp:406] ReLU12 <- Eltwise6
I0305 17:56:50.331717 21892 net.cpp:367] ReLU12 -> Eltwise6 (in-place)
I0305 17:56:50.331732 21892 net.cpp:122] Setting up ReLU12
I0305 17:56:50.331765 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:50.331794 21892 net.cpp:137] Memory required for data: 2509603104
I0305 17:56:50.331805 21892 layer_factory.hpp:77] Creating layer Eltwise6_ReLU12_0_split
I0305 17:56:50.331820 21892 net.cpp:84] Creating Layer Eltwise6_ReLU12_0_split
I0305 17:56:50.331832 21892 net.cpp:406] Eltwise6_ReLU12_0_split <- Eltwise6
I0305 17:56:50.331847 21892 net.cpp:380] Eltwise6_ReLU12_0_split -> Eltwise6_ReLU12_0_split_0
I0305 17:56:50.331879 21892 net.cpp:380] Eltwise6_ReLU12_0_split -> Eltwise6_ReLU12_0_split_1
I0305 17:56:50.331928 21892 net.cpp:122] Setting up Eltwise6_ReLU12_0_split
I0305 17:56:50.331945 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:50.331959 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:50.331971 21892 net.cpp:137] Memory required for data: 2548138272
I0305 17:56:50.331984 21892 layer_factory.hpp:77] Creating layer Convolution15
I0305 17:56:50.332013 21892 net.cpp:84] Creating Layer Convolution15
I0305 17:56:50.332027 21892 net.cpp:406] Convolution15 <- Eltwise6_ReLU12_0_split_0
I0305 17:56:50.332039 21892 net.cpp:380] Convolution15 -> Convolution15
I0305 17:56:50.337893 21892 net.cpp:122] Setting up Convolution15
I0305 17:56:50.337960 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:50.337973 21892 net.cpp:137] Memory required for data: 2557772064
I0305 17:56:50.338006 21892 layer_factory.hpp:77] Creating layer BatchNorm15
I0305 17:56:50.338034 21892 net.cpp:84] Creating Layer BatchNorm15
I0305 17:56:50.338050 21892 net.cpp:406] BatchNorm15 <- Convolution15
I0305 17:56:50.338070 21892 net.cpp:367] BatchNorm15 -> Convolution15 (in-place)
I0305 17:56:50.338390 21892 net.cpp:122] Setting up BatchNorm15
I0305 17:56:50.338416 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:50.338428 21892 net.cpp:137] Memory required for data: 2567405856
I0305 17:56:50.338443 21892 layer_factory.hpp:77] Creating layer Scale15
I0305 17:56:50.338467 21892 net.cpp:84] Creating Layer Scale15
I0305 17:56:50.338490 21892 net.cpp:406] Scale15 <- Convolution15
I0305 17:56:50.338506 21892 net.cpp:367] Scale15 -> Convolution15 (in-place)
I0305 17:56:50.338589 21892 layer_factory.hpp:77] Creating layer Scale15
I0305 17:56:50.338758 21892 net.cpp:122] Setting up Scale15
I0305 17:56:50.338773 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:50.338781 21892 net.cpp:137] Memory required for data: 2577039648
I0305 17:56:50.338793 21892 layer_factory.hpp:77] Creating layer ReLU13
I0305 17:56:50.338804 21892 net.cpp:84] Creating Layer ReLU13
I0305 17:56:50.338812 21892 net.cpp:406] ReLU13 <- Convolution15
I0305 17:56:50.338838 21892 net.cpp:367] ReLU13 -> Convolution15 (in-place)
I0305 17:56:50.338855 21892 net.cpp:122] Setting up ReLU13
I0305 17:56:50.338881 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:50.338906 21892 net.cpp:137] Memory required for data: 2586673440
I0305 17:56:50.338918 21892 layer_factory.hpp:77] Creating layer Convolution16
I0305 17:56:50.338940 21892 net.cpp:84] Creating Layer Convolution16
I0305 17:56:50.338955 21892 net.cpp:406] Convolution16 <- Convolution15
I0305 17:56:50.338968 21892 net.cpp:380] Convolution16 -> Convolution16
I0305 17:56:50.340747 21892 net.cpp:122] Setting up Convolution16
I0305 17:56:50.340772 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:50.340781 21892 net.cpp:137] Memory required for data: 2596307232
I0305 17:56:50.340816 21892 layer_factory.hpp:77] Creating layer BatchNorm16
I0305 17:56:50.340847 21892 net.cpp:84] Creating Layer BatchNorm16
I0305 17:56:50.340860 21892 net.cpp:406] BatchNorm16 <- Convolution16
I0305 17:56:50.340878 21892 net.cpp:367] BatchNorm16 -> Convolution16 (in-place)
I0305 17:56:50.341173 21892 net.cpp:122] Setting up BatchNorm16
I0305 17:56:50.341205 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:50.341269 21892 net.cpp:137] Memory required for data: 2605941024
I0305 17:56:50.341284 21892 layer_factory.hpp:77] Creating layer Scale16
I0305 17:56:50.341305 21892 net.cpp:84] Creating Layer Scale16
I0305 17:56:50.341312 21892 net.cpp:406] Scale16 <- Convolution16
I0305 17:56:50.341323 21892 net.cpp:367] Scale16 -> Convolution16 (in-place)
I0305 17:56:50.341390 21892 layer_factory.hpp:77] Creating layer Scale16
I0305 17:56:50.341539 21892 net.cpp:122] Setting up Scale16
I0305 17:56:50.341559 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:50.341572 21892 net.cpp:137] Memory required for data: 2615574816
I0305 17:56:50.341586 21892 layer_factory.hpp:77] Creating layer Convolution17
I0305 17:56:50.341600 21892 net.cpp:84] Creating Layer Convolution17
I0305 17:56:50.341609 21892 net.cpp:406] Convolution17 <- Eltwise6_ReLU12_0_split_1
I0305 17:56:50.341620 21892 net.cpp:380] Convolution17 -> Convolution17
I0305 17:56:50.342034 21892 net.cpp:122] Setting up Convolution17
I0305 17:56:50.342059 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:50.342070 21892 net.cpp:137] Memory required for data: 2625208608
I0305 17:56:50.342079 21892 layer_factory.hpp:77] Creating layer BatchNorm17
I0305 17:56:50.342092 21892 net.cpp:84] Creating Layer BatchNorm17
I0305 17:56:50.342103 21892 net.cpp:406] BatchNorm17 <- Convolution17
I0305 17:56:50.342113 21892 net.cpp:367] BatchNorm17 -> Convolution17 (in-place)
I0305 17:56:50.342414 21892 net.cpp:122] Setting up BatchNorm17
I0305 17:56:50.342428 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:50.342437 21892 net.cpp:137] Memory required for data: 2634842400
I0305 17:56:50.342458 21892 layer_factory.hpp:77] Creating layer Scale17
I0305 17:56:50.342478 21892 net.cpp:84] Creating Layer Scale17
I0305 17:56:50.342492 21892 net.cpp:406] Scale17 <- Convolution17
I0305 17:56:50.342525 21892 net.cpp:367] Scale17 -> Convolution17 (in-place)
I0305 17:56:50.342609 21892 layer_factory.hpp:77] Creating layer Scale17
I0305 17:56:50.342774 21892 net.cpp:122] Setting up Scale17
I0305 17:56:50.342789 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:50.342798 21892 net.cpp:137] Memory required for data: 2644476192
I0305 17:56:50.342808 21892 layer_factory.hpp:77] Creating layer Eltwise7
I0305 17:56:50.342820 21892 net.cpp:84] Creating Layer Eltwise7
I0305 17:56:50.342828 21892 net.cpp:406] Eltwise7 <- Convolution16
I0305 17:56:50.342844 21892 net.cpp:406] Eltwise7 <- Convolution17
I0305 17:56:50.342864 21892 net.cpp:380] Eltwise7 -> Eltwise7
I0305 17:56:50.342927 21892 net.cpp:122] Setting up Eltwise7
I0305 17:56:50.342945 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:50.342957 21892 net.cpp:137] Memory required for data: 2654109984
I0305 17:56:50.342967 21892 layer_factory.hpp:77] Creating layer ReLU14
I0305 17:56:50.342979 21892 net.cpp:84] Creating Layer ReLU14
I0305 17:56:50.342988 21892 net.cpp:406] ReLU14 <- Eltwise7
I0305 17:56:50.342996 21892 net.cpp:367] ReLU14 -> Eltwise7 (in-place)
I0305 17:56:50.343008 21892 net.cpp:122] Setting up ReLU14
I0305 17:56:50.343019 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:50.343031 21892 net.cpp:137] Memory required for data: 2663743776
I0305 17:56:50.343053 21892 layer_factory.hpp:77] Creating layer Eltwise7_ReLU14_0_split
I0305 17:56:50.343070 21892 net.cpp:84] Creating Layer Eltwise7_ReLU14_0_split
I0305 17:56:50.343082 21892 net.cpp:406] Eltwise7_ReLU14_0_split <- Eltwise7
I0305 17:56:50.343097 21892 net.cpp:380] Eltwise7_ReLU14_0_split -> Eltwise7_ReLU14_0_split_0
I0305 17:56:50.343114 21892 net.cpp:380] Eltwise7_ReLU14_0_split -> Eltwise7_ReLU14_0_split_1
I0305 17:56:50.343189 21892 net.cpp:122] Setting up Eltwise7_ReLU14_0_split
I0305 17:56:50.343206 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:50.343220 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:50.343232 21892 net.cpp:137] Memory required for data: 2683011360
I0305 17:56:50.343255 21892 layer_factory.hpp:77] Creating layer Convolution18
I0305 17:56:50.343302 21892 net.cpp:84] Creating Layer Convolution18
I0305 17:56:50.343312 21892 net.cpp:406] Convolution18 <- Eltwise7_ReLU14_0_split_0
I0305 17:56:50.343323 21892 net.cpp:380] Convolution18 -> Convolution18
I0305 17:56:50.345026 21892 net.cpp:122] Setting up Convolution18
I0305 17:56:50.345052 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:50.345073 21892 net.cpp:137] Memory required for data: 2692645152
I0305 17:56:50.345091 21892 layer_factory.hpp:77] Creating layer BatchNorm18
I0305 17:56:50.345106 21892 net.cpp:84] Creating Layer BatchNorm18
I0305 17:56:50.345114 21892 net.cpp:406] BatchNorm18 <- Convolution18
I0305 17:56:50.345124 21892 net.cpp:367] BatchNorm18 -> Convolution18 (in-place)
I0305 17:56:50.345420 21892 net.cpp:122] Setting up BatchNorm18
I0305 17:56:50.345445 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:50.345453 21892 net.cpp:137] Memory required for data: 2702278944
I0305 17:56:50.345465 21892 layer_factory.hpp:77] Creating layer Scale18
I0305 17:56:50.345494 21892 net.cpp:84] Creating Layer Scale18
I0305 17:56:50.345501 21892 net.cpp:406] Scale18 <- Convolution18
I0305 17:56:50.345516 21892 net.cpp:367] Scale18 -> Convolution18 (in-place)
I0305 17:56:50.345587 21892 layer_factory.hpp:77] Creating layer Scale18
I0305 17:56:50.345764 21892 net.cpp:122] Setting up Scale18
I0305 17:56:50.345784 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:50.345793 21892 net.cpp:137] Memory required for data: 2711912736
I0305 17:56:50.345803 21892 layer_factory.hpp:77] Creating layer ReLU15
I0305 17:56:50.345824 21892 net.cpp:84] Creating Layer ReLU15
I0305 17:56:50.345834 21892 net.cpp:406] ReLU15 <- Convolution18
I0305 17:56:50.345844 21892 net.cpp:367] ReLU15 -> Convolution18 (in-place)
I0305 17:56:50.345860 21892 net.cpp:122] Setting up ReLU15
I0305 17:56:50.345875 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:50.345887 21892 net.cpp:137] Memory required for data: 2721546528
I0305 17:56:50.345912 21892 layer_factory.hpp:77] Creating layer Convolution19
I0305 17:56:50.345933 21892 net.cpp:84] Creating Layer Convolution19
I0305 17:56:50.345947 21892 net.cpp:406] Convolution19 <- Convolution18
I0305 17:56:50.345960 21892 net.cpp:380] Convolution19 -> Convolution19
I0305 17:56:50.347645 21892 net.cpp:122] Setting up Convolution19
I0305 17:56:50.347671 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:50.347685 21892 net.cpp:137] Memory required for data: 2731180320
I0305 17:56:50.347702 21892 layer_factory.hpp:77] Creating layer BatchNorm19
I0305 17:56:50.347720 21892 net.cpp:84] Creating Layer BatchNorm19
I0305 17:56:50.347733 21892 net.cpp:406] BatchNorm19 <- Convolution19
I0305 17:56:50.347748 21892 net.cpp:367] BatchNorm19 -> Convolution19 (in-place)
I0305 17:56:50.348027 21892 net.cpp:122] Setting up BatchNorm19
I0305 17:56:50.348058 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:50.348071 21892 net.cpp:137] Memory required for data: 2740814112
I0305 17:56:50.348116 21892 layer_factory.hpp:77] Creating layer Scale19
I0305 17:56:50.348140 21892 net.cpp:84] Creating Layer Scale19
I0305 17:56:50.348152 21892 net.cpp:406] Scale19 <- Convolution19
I0305 17:56:50.348170 21892 net.cpp:367] Scale19 -> Convolution19 (in-place)
I0305 17:56:50.348255 21892 layer_factory.hpp:77] Creating layer Scale19
I0305 17:56:50.348423 21892 net.cpp:122] Setting up Scale19
I0305 17:56:50.348438 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:50.348444 21892 net.cpp:137] Memory required for data: 2750447904
I0305 17:56:50.348454 21892 layer_factory.hpp:77] Creating layer Eltwise8
I0305 17:56:50.348466 21892 net.cpp:84] Creating Layer Eltwise8
I0305 17:56:50.348475 21892 net.cpp:406] Eltwise8 <- Convolution19
I0305 17:56:50.348484 21892 net.cpp:406] Eltwise8 <- Eltwise7_ReLU14_0_split_1
I0305 17:56:50.348500 21892 net.cpp:380] Eltwise8 -> Eltwise8
I0305 17:56:50.348556 21892 net.cpp:122] Setting up Eltwise8
I0305 17:56:50.348573 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:50.348585 21892 net.cpp:137] Memory required for data: 2760081696
I0305 17:56:50.348624 21892 layer_factory.hpp:77] Creating layer ReLU16
I0305 17:56:50.348637 21892 net.cpp:84] Creating Layer ReLU16
I0305 17:56:50.348645 21892 net.cpp:406] ReLU16 <- Eltwise8
I0305 17:56:50.348659 21892 net.cpp:367] ReLU16 -> Eltwise8 (in-place)
I0305 17:56:50.348676 21892 net.cpp:122] Setting up ReLU16
I0305 17:56:50.348691 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:50.348703 21892 net.cpp:137] Memory required for data: 2769715488
I0305 17:56:50.348716 21892 layer_factory.hpp:77] Creating layer Eltwise8_ReLU16_0_split
I0305 17:56:50.348731 21892 net.cpp:84] Creating Layer Eltwise8_ReLU16_0_split
I0305 17:56:50.348743 21892 net.cpp:406] Eltwise8_ReLU16_0_split <- Eltwise8
I0305 17:56:50.348758 21892 net.cpp:380] Eltwise8_ReLU16_0_split -> Eltwise8_ReLU16_0_split_0
I0305 17:56:50.348769 21892 net.cpp:380] Eltwise8_ReLU16_0_split -> Eltwise8_ReLU16_0_split_1
I0305 17:56:50.348832 21892 net.cpp:122] Setting up Eltwise8_ReLU16_0_split
I0305 17:56:50.348850 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:50.348865 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:50.348877 21892 net.cpp:137] Memory required for data: 2788983072
I0305 17:56:50.348889 21892 layer_factory.hpp:77] Creating layer Convolution20
I0305 17:56:50.348911 21892 net.cpp:84] Creating Layer Convolution20
I0305 17:56:50.348922 21892 net.cpp:406] Convolution20 <- Eltwise8_ReLU16_0_split_0
I0305 17:56:50.348937 21892 net.cpp:380] Convolution20 -> Convolution20
I0305 17:56:50.352651 21892 net.cpp:122] Setting up Convolution20
I0305 17:56:50.352705 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:50.352715 21892 net.cpp:137] Memory required for data: 2798616864
I0305 17:56:50.352726 21892 layer_factory.hpp:77] Creating layer BatchNorm20
I0305 17:56:50.352767 21892 net.cpp:84] Creating Layer BatchNorm20
I0305 17:56:50.352785 21892 net.cpp:406] BatchNorm20 <- Convolution20
I0305 17:56:50.352805 21892 net.cpp:367] BatchNorm20 -> Convolution20 (in-place)
I0305 17:56:50.353081 21892 net.cpp:122] Setting up BatchNorm20
I0305 17:56:50.353103 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:50.353116 21892 net.cpp:137] Memory required for data: 2808250656
I0305 17:56:50.353137 21892 layer_factory.hpp:77] Creating layer Scale20
I0305 17:56:50.353157 21892 net.cpp:84] Creating Layer Scale20
I0305 17:56:50.353170 21892 net.cpp:406] Scale20 <- Convolution20
I0305 17:56:50.353186 21892 net.cpp:367] Scale20 -> Convolution20 (in-place)
I0305 17:56:50.353240 21892 layer_factory.hpp:77] Creating layer Scale20
I0305 17:56:50.353410 21892 net.cpp:122] Setting up Scale20
I0305 17:56:50.353430 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:50.353442 21892 net.cpp:137] Memory required for data: 2817884448
I0305 17:56:50.353459 21892 layer_factory.hpp:77] Creating layer ReLU17
I0305 17:56:50.353476 21892 net.cpp:84] Creating Layer ReLU17
I0305 17:56:50.353487 21892 net.cpp:406] ReLU17 <- Convolution20
I0305 17:56:50.353503 21892 net.cpp:367] ReLU17 -> Convolution20 (in-place)
I0305 17:56:50.353518 21892 net.cpp:122] Setting up ReLU17
I0305 17:56:50.353528 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:50.353535 21892 net.cpp:137] Memory required for data: 2827518240
I0305 17:56:50.353543 21892 layer_factory.hpp:77] Creating layer Convolution21
I0305 17:56:50.353570 21892 net.cpp:84] Creating Layer Convolution21
I0305 17:56:50.353588 21892 net.cpp:406] Convolution21 <- Convolution20
I0305 17:56:50.353608 21892 net.cpp:380] Convolution21 -> Convolution21
I0305 17:56:50.355270 21892 net.cpp:122] Setting up Convolution21
I0305 17:56:50.355288 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:50.355298 21892 net.cpp:137] Memory required for data: 2837152032
I0305 17:56:50.355315 21892 layer_factory.hpp:77] Creating layer BatchNorm21
I0305 17:56:50.355347 21892 net.cpp:84] Creating Layer BatchNorm21
I0305 17:56:50.355360 21892 net.cpp:406] BatchNorm21 <- Convolution21
I0305 17:56:50.355376 21892 net.cpp:367] BatchNorm21 -> Convolution21 (in-place)
I0305 17:56:50.355680 21892 net.cpp:122] Setting up BatchNorm21
I0305 17:56:50.355700 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:50.355708 21892 net.cpp:137] Memory required for data: 2846785824
I0305 17:56:50.355720 21892 layer_factory.hpp:77] Creating layer Scale21
I0305 17:56:50.355732 21892 net.cpp:84] Creating Layer Scale21
I0305 17:56:50.355741 21892 net.cpp:406] Scale21 <- Convolution21
I0305 17:56:50.355749 21892 net.cpp:367] Scale21 -> Convolution21 (in-place)
I0305 17:56:50.355810 21892 layer_factory.hpp:77] Creating layer Scale21
I0305 17:56:50.355979 21892 net.cpp:122] Setting up Scale21
I0305 17:56:50.355998 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:50.356020 21892 net.cpp:137] Memory required for data: 2856419616
I0305 17:56:50.356031 21892 layer_factory.hpp:77] Creating layer Eltwise9
I0305 17:56:50.356051 21892 net.cpp:84] Creating Layer Eltwise9
I0305 17:56:50.356060 21892 net.cpp:406] Eltwise9 <- Convolution21
I0305 17:56:50.356067 21892 net.cpp:406] Eltwise9 <- Eltwise8_ReLU16_0_split_1
I0305 17:56:50.356077 21892 net.cpp:380] Eltwise9 -> Eltwise9
I0305 17:56:50.356137 21892 net.cpp:122] Setting up Eltwise9
I0305 17:56:50.356153 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:50.356164 21892 net.cpp:137] Memory required for data: 2866053408
I0305 17:56:50.356175 21892 layer_factory.hpp:77] Creating layer ReLU18
I0305 17:56:50.356190 21892 net.cpp:84] Creating Layer ReLU18
I0305 17:56:50.356199 21892 net.cpp:406] ReLU18 <- Eltwise9
I0305 17:56:50.356209 21892 net.cpp:367] ReLU18 -> Eltwise9 (in-place)
I0305 17:56:50.356218 21892 net.cpp:122] Setting up ReLU18
I0305 17:56:50.356226 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:50.356233 21892 net.cpp:137] Memory required for data: 2875687200
I0305 17:56:50.356240 21892 layer_factory.hpp:77] Creating layer Deconvolution1
I0305 17:56:50.356266 21892 net.cpp:84] Creating Layer Deconvolution1
I0305 17:56:50.356278 21892 net.cpp:406] Deconvolution1 <- Eltwise9
I0305 17:56:50.356297 21892 net.cpp:380] Deconvolution1 -> Deconvolution1
I0305 17:56:50.357199 21892 net.cpp:122] Setting up Deconvolution1
I0305 17:56:50.357239 21892 net.cpp:129] Top shape: 24 128 56 56 (9633792)
I0305 17:56:50.357251 21892 net.cpp:137] Memory required for data: 2914222368
I0305 17:56:50.357270 21892 layer_factory.hpp:77] Creating layer BatchNorm22
I0305 17:56:50.357290 21892 net.cpp:84] Creating Layer BatchNorm22
I0305 17:56:50.357303 21892 net.cpp:406] BatchNorm22 <- Deconvolution1
I0305 17:56:50.357313 21892 net.cpp:367] BatchNorm22 -> Deconvolution1 (in-place)
I0305 17:56:50.357590 21892 net.cpp:122] Setting up BatchNorm22
I0305 17:56:50.357606 21892 net.cpp:129] Top shape: 24 128 56 56 (9633792)
I0305 17:56:50.357614 21892 net.cpp:137] Memory required for data: 2952757536
I0305 17:56:50.357626 21892 layer_factory.hpp:77] Creating layer Scale22
I0305 17:56:50.357638 21892 net.cpp:84] Creating Layer Scale22
I0305 17:56:50.357652 21892 net.cpp:406] Scale22 <- Deconvolution1
I0305 17:56:50.357667 21892 net.cpp:367] Scale22 -> Deconvolution1 (in-place)
I0305 17:56:50.357735 21892 layer_factory.hpp:77] Creating layer Scale22
I0305 17:56:50.357902 21892 net.cpp:122] Setting up Scale22
I0305 17:56:50.357913 21892 net.cpp:129] Top shape: 24 128 56 56 (9633792)
I0305 17:56:50.357921 21892 net.cpp:137] Memory required for data: 2991292704
I0305 17:56:50.357933 21892 layer_factory.hpp:77] Creating layer ReLU19
I0305 17:56:50.357942 21892 net.cpp:84] Creating Layer ReLU19
I0305 17:56:50.357951 21892 net.cpp:406] ReLU19 <- Deconvolution1
I0305 17:56:50.357966 21892 net.cpp:367] ReLU19 -> Deconvolution1 (in-place)
I0305 17:56:50.357981 21892 net.cpp:122] Setting up ReLU19
I0305 17:56:50.357995 21892 net.cpp:129] Top shape: 24 128 56 56 (9633792)
I0305 17:56:50.358006 21892 net.cpp:137] Memory required for data: 3029827872
I0305 17:56:50.358018 21892 layer_factory.hpp:77] Creating layer Deconvolution2
I0305 17:56:50.358047 21892 net.cpp:84] Creating Layer Deconvolution2
I0305 17:56:50.358070 21892 net.cpp:406] Deconvolution2 <- Deconvolution1
I0305 17:56:50.358083 21892 net.cpp:380] Deconvolution2 -> Deconvolution2
I0305 17:56:50.358727 21892 net.cpp:122] Setting up Deconvolution2
I0305 17:56:50.358753 21892 net.cpp:129] Top shape: 24 64 112 112 (19267584)
I0305 17:56:50.358767 21892 net.cpp:137] Memory required for data: 3106898208
I0305 17:56:50.358783 21892 layer_factory.hpp:77] Creating layer BatchNorm23
I0305 17:56:50.358799 21892 net.cpp:84] Creating Layer BatchNorm23
I0305 17:56:50.358811 21892 net.cpp:406] BatchNorm23 <- Deconvolution2
I0305 17:56:50.358826 21892 net.cpp:367] BatchNorm23 -> Deconvolution2 (in-place)
I0305 17:56:50.359123 21892 net.cpp:122] Setting up BatchNorm23
I0305 17:56:50.359140 21892 net.cpp:129] Top shape: 24 64 112 112 (19267584)
I0305 17:56:50.359148 21892 net.cpp:137] Memory required for data: 3183968544
I0305 17:56:50.359169 21892 layer_factory.hpp:77] Creating layer Scale23
I0305 17:56:50.359179 21892 net.cpp:84] Creating Layer Scale23
I0305 17:56:50.359190 21892 net.cpp:406] Scale23 <- Deconvolution2
I0305 17:56:50.359205 21892 net.cpp:367] Scale23 -> Deconvolution2 (in-place)
I0305 17:56:50.359272 21892 layer_factory.hpp:77] Creating layer Scale23
I0305 17:56:50.359447 21892 net.cpp:122] Setting up Scale23
I0305 17:56:50.359458 21892 net.cpp:129] Top shape: 24 64 112 112 (19267584)
I0305 17:56:50.359465 21892 net.cpp:137] Memory required for data: 3261038880
I0305 17:56:50.359477 21892 layer_factory.hpp:77] Creating layer ReLU20
I0305 17:56:50.359486 21892 net.cpp:84] Creating Layer ReLU20
I0305 17:56:50.359498 21892 net.cpp:406] ReLU20 <- Deconvolution2
I0305 17:56:50.359511 21892 net.cpp:367] ReLU20 -> Deconvolution2 (in-place)
I0305 17:56:50.359526 21892 net.cpp:122] Setting up ReLU20
I0305 17:56:50.359539 21892 net.cpp:129] Top shape: 24 64 112 112 (19267584)
I0305 17:56:50.359550 21892 net.cpp:137] Memory required for data: 3338109216
I0305 17:56:50.359561 21892 layer_factory.hpp:77] Creating layer Deconvolution3
I0305 17:56:50.359591 21892 net.cpp:84] Creating Layer Deconvolution3
I0305 17:56:50.359601 21892 net.cpp:406] Deconvolution3 <- Deconvolution2
I0305 17:56:50.359611 21892 net.cpp:380] Deconvolution3 -> Deconvolution3
I0305 17:56:50.360076 21892 net.cpp:122] Setting up Deconvolution3
I0305 17:56:50.360093 21892 net.cpp:129] Top shape: 24 32 224 224 (38535168)
I0305 17:56:50.360102 21892 net.cpp:137] Memory required for data: 3492249888
I0305 17:56:50.360118 21892 layer_factory.hpp:77] Creating layer BatchNorm24
I0305 17:56:50.360136 21892 net.cpp:84] Creating Layer BatchNorm24
I0305 17:56:50.360147 21892 net.cpp:406] BatchNorm24 <- Deconvolution3
I0305 17:56:50.360162 21892 net.cpp:367] BatchNorm24 -> Deconvolution3 (in-place)
I0305 17:56:50.360494 21892 net.cpp:122] Setting up BatchNorm24
I0305 17:56:50.360510 21892 net.cpp:129] Top shape: 24 32 224 224 (38535168)
I0305 17:56:50.360518 21892 net.cpp:137] Memory required for data: 3646390560
I0305 17:56:50.360529 21892 layer_factory.hpp:77] Creating layer Scale24
I0305 17:56:50.360540 21892 net.cpp:84] Creating Layer Scale24
I0305 17:56:50.360548 21892 net.cpp:406] Scale24 <- Deconvolution3
I0305 17:56:50.360564 21892 net.cpp:367] Scale24 -> Deconvolution3 (in-place)
I0305 17:56:50.360631 21892 layer_factory.hpp:77] Creating layer Scale24
I0305 17:56:50.364593 21892 net.cpp:122] Setting up Scale24
I0305 17:56:50.364619 21892 net.cpp:129] Top shape: 24 32 224 224 (38535168)
I0305 17:56:50.364625 21892 net.cpp:137] Memory required for data: 3800531232
I0305 17:56:50.364642 21892 layer_factory.hpp:77] Creating layer ReLU21
I0305 17:56:50.364670 21892 net.cpp:84] Creating Layer ReLU21
I0305 17:56:50.364676 21892 net.cpp:406] ReLU21 <- Deconvolution3
I0305 17:56:50.364686 21892 net.cpp:367] ReLU21 -> Deconvolution3 (in-place)
I0305 17:56:50.364694 21892 net.cpp:122] Setting up ReLU21
I0305 17:56:50.364701 21892 net.cpp:129] Top shape: 24 32 224 224 (38535168)
I0305 17:56:50.364706 21892 net.cpp:137] Memory required for data: 3954671904
I0305 17:56:50.364709 21892 layer_factory.hpp:77] Creating layer Convolution22
I0305 17:56:50.364748 21892 net.cpp:84] Creating Layer Convolution22
I0305 17:56:50.364753 21892 net.cpp:406] Convolution22 <- Deconvolution3
I0305 17:56:50.364763 21892 net.cpp:380] Convolution22 -> Convolution22
I0305 17:56:50.365093 21892 net.cpp:122] Setting up Convolution22
I0305 17:56:50.365103 21892 net.cpp:129] Top shape: 24 1 224 224 (1204224)
I0305 17:56:50.365108 21892 net.cpp:137] Memory required for data: 3959488800
I0305 17:56:50.365115 21892 layer_factory.hpp:77] Creating layer ReLU22
I0305 17:56:50.365123 21892 net.cpp:84] Creating Layer ReLU22
I0305 17:56:50.365128 21892 net.cpp:406] ReLU22 <- Convolution22
I0305 17:56:50.365135 21892 net.cpp:367] ReLU22 -> Convolution22 (in-place)
I0305 17:56:50.365142 21892 net.cpp:122] Setting up ReLU22
I0305 17:56:50.365149 21892 net.cpp:129] Top shape: 24 1 224 224 (1204224)
I0305 17:56:50.365152 21892 net.cpp:137] Memory required for data: 3964305696
I0305 17:56:50.365157 21892 layer_factory.hpp:77] Creating layer Loss
I0305 17:56:50.365175 21892 net.cpp:84] Creating Layer Loss
I0305 17:56:50.365180 21892 net.cpp:406] Loss <- Convolution22
I0305 17:56:50.365186 21892 net.cpp:406] Loss <- Data5
I0305 17:56:50.365196 21892 net.cpp:380] Loss -> Loss
I0305 17:56:50.365245 21892 net.cpp:122] Setting up Loss
I0305 17:56:50.365252 21892 net.cpp:129] Top shape: (1)
I0305 17:56:50.365257 21892 net.cpp:132]     with loss weight 1
I0305 17:56:50.365298 21892 net.cpp:137] Memory required for data: 3964305700
I0305 17:56:50.365303 21892 net.cpp:198] Loss needs backward computation.
I0305 17:56:50.365308 21892 net.cpp:198] ReLU22 needs backward computation.
I0305 17:56:50.365311 21892 net.cpp:198] Convolution22 needs backward computation.
I0305 17:56:50.365316 21892 net.cpp:198] ReLU21 needs backward computation.
I0305 17:56:50.365320 21892 net.cpp:198] Scale24 needs backward computation.
I0305 17:56:50.365324 21892 net.cpp:198] BatchNorm24 needs backward computation.
I0305 17:56:50.365329 21892 net.cpp:198] Deconvolution3 needs backward computation.
I0305 17:56:50.365334 21892 net.cpp:198] ReLU20 needs backward computation.
I0305 17:56:50.365337 21892 net.cpp:198] Scale23 needs backward computation.
I0305 17:56:50.365341 21892 net.cpp:198] BatchNorm23 needs backward computation.
I0305 17:56:50.365346 21892 net.cpp:198] Deconvolution2 needs backward computation.
I0305 17:56:50.365350 21892 net.cpp:198] ReLU19 needs backward computation.
I0305 17:56:50.365355 21892 net.cpp:198] Scale22 needs backward computation.
I0305 17:56:50.365360 21892 net.cpp:198] BatchNorm22 needs backward computation.
I0305 17:56:50.365363 21892 net.cpp:198] Deconvolution1 needs backward computation.
I0305 17:56:50.365368 21892 net.cpp:198] ReLU18 needs backward computation.
I0305 17:56:50.365373 21892 net.cpp:198] Eltwise9 needs backward computation.
I0305 17:56:50.365380 21892 net.cpp:198] Scale21 needs backward computation.
I0305 17:56:50.365383 21892 net.cpp:198] BatchNorm21 needs backward computation.
I0305 17:56:50.365388 21892 net.cpp:198] Convolution21 needs backward computation.
I0305 17:56:50.365393 21892 net.cpp:198] ReLU17 needs backward computation.
I0305 17:56:50.365397 21892 net.cpp:198] Scale20 needs backward computation.
I0305 17:56:50.365402 21892 net.cpp:198] BatchNorm20 needs backward computation.
I0305 17:56:50.365407 21892 net.cpp:198] Convolution20 needs backward computation.
I0305 17:56:50.365412 21892 net.cpp:198] Eltwise8_ReLU16_0_split needs backward computation.
I0305 17:56:50.365417 21892 net.cpp:198] ReLU16 needs backward computation.
I0305 17:56:50.365422 21892 net.cpp:198] Eltwise8 needs backward computation.
I0305 17:56:50.365427 21892 net.cpp:198] Scale19 needs backward computation.
I0305 17:56:50.365433 21892 net.cpp:198] BatchNorm19 needs backward computation.
I0305 17:56:50.365437 21892 net.cpp:198] Convolution19 needs backward computation.
I0305 17:56:50.365442 21892 net.cpp:198] ReLU15 needs backward computation.
I0305 17:56:50.365447 21892 net.cpp:198] Scale18 needs backward computation.
I0305 17:56:50.365451 21892 net.cpp:198] BatchNorm18 needs backward computation.
I0305 17:56:50.365465 21892 net.cpp:198] Convolution18 needs backward computation.
I0305 17:56:50.365471 21892 net.cpp:198] Eltwise7_ReLU14_0_split needs backward computation.
I0305 17:56:50.365476 21892 net.cpp:198] ReLU14 needs backward computation.
I0305 17:56:50.365481 21892 net.cpp:198] Eltwise7 needs backward computation.
I0305 17:56:50.365487 21892 net.cpp:198] Scale17 needs backward computation.
I0305 17:56:50.365492 21892 net.cpp:198] BatchNorm17 needs backward computation.
I0305 17:56:50.365497 21892 net.cpp:198] Convolution17 needs backward computation.
I0305 17:56:50.365502 21892 net.cpp:198] Scale16 needs backward computation.
I0305 17:56:50.365507 21892 net.cpp:198] BatchNorm16 needs backward computation.
I0305 17:56:50.365512 21892 net.cpp:198] Convolution16 needs backward computation.
I0305 17:56:50.365517 21892 net.cpp:198] ReLU13 needs backward computation.
I0305 17:56:50.365521 21892 net.cpp:198] Scale15 needs backward computation.
I0305 17:56:50.365526 21892 net.cpp:198] BatchNorm15 needs backward computation.
I0305 17:56:50.365531 21892 net.cpp:198] Convolution15 needs backward computation.
I0305 17:56:50.365536 21892 net.cpp:198] Eltwise6_ReLU12_0_split needs backward computation.
I0305 17:56:50.365541 21892 net.cpp:198] ReLU12 needs backward computation.
I0305 17:56:50.365546 21892 net.cpp:198] Eltwise6 needs backward computation.
I0305 17:56:50.365551 21892 net.cpp:198] Scale14 needs backward computation.
I0305 17:56:50.365556 21892 net.cpp:198] BatchNorm14 needs backward computation.
I0305 17:56:50.365561 21892 net.cpp:198] Convolution14 needs backward computation.
I0305 17:56:50.365566 21892 net.cpp:198] ReLU11 needs backward computation.
I0305 17:56:50.365571 21892 net.cpp:198] Scale13 needs backward computation.
I0305 17:56:50.365576 21892 net.cpp:198] BatchNorm13 needs backward computation.
I0305 17:56:50.365581 21892 net.cpp:198] Convolution13 needs backward computation.
I0305 17:56:50.365586 21892 net.cpp:198] Eltwise5_ReLU10_0_split needs backward computation.
I0305 17:56:50.365592 21892 net.cpp:198] ReLU10 needs backward computation.
I0305 17:56:50.365595 21892 net.cpp:198] Eltwise5 needs backward computation.
I0305 17:56:50.365602 21892 net.cpp:198] Scale12 needs backward computation.
I0305 17:56:50.365605 21892 net.cpp:198] BatchNorm12 needs backward computation.
I0305 17:56:50.365610 21892 net.cpp:198] Convolution12 needs backward computation.
I0305 17:56:50.365615 21892 net.cpp:198] ReLU9 needs backward computation.
I0305 17:56:50.365620 21892 net.cpp:198] Scale11 needs backward computation.
I0305 17:56:50.365624 21892 net.cpp:198] BatchNorm11 needs backward computation.
I0305 17:56:50.365629 21892 net.cpp:198] Convolution11 needs backward computation.
I0305 17:56:50.365634 21892 net.cpp:198] Eltwise4_ReLU8_0_split needs backward computation.
I0305 17:56:50.365640 21892 net.cpp:198] ReLU8 needs backward computation.
I0305 17:56:50.365644 21892 net.cpp:198] Eltwise4 needs backward computation.
I0305 17:56:50.365651 21892 net.cpp:198] Scale10 needs backward computation.
I0305 17:56:50.365655 21892 net.cpp:198] BatchNorm10 needs backward computation.
I0305 17:56:50.365660 21892 net.cpp:198] Convolution10 needs backward computation.
I0305 17:56:50.365666 21892 net.cpp:198] Scale9 needs backward computation.
I0305 17:56:50.365671 21892 net.cpp:198] BatchNorm9 needs backward computation.
I0305 17:56:50.365675 21892 net.cpp:198] Convolution9 needs backward computation.
I0305 17:56:50.365681 21892 net.cpp:198] ReLU7 needs backward computation.
I0305 17:56:50.365686 21892 net.cpp:198] Scale8 needs backward computation.
I0305 17:56:50.365691 21892 net.cpp:198] BatchNorm8 needs backward computation.
I0305 17:56:50.365695 21892 net.cpp:198] Convolution8 needs backward computation.
I0305 17:56:50.365701 21892 net.cpp:198] Eltwise3_ReLU6_0_split needs backward computation.
I0305 17:56:50.365706 21892 net.cpp:198] ReLU6 needs backward computation.
I0305 17:56:50.365711 21892 net.cpp:198] Eltwise3 needs backward computation.
I0305 17:56:50.365716 21892 net.cpp:198] Scale7 needs backward computation.
I0305 17:56:50.365728 21892 net.cpp:198] BatchNorm7 needs backward computation.
I0305 17:56:50.365733 21892 net.cpp:198] Convolution7 needs backward computation.
I0305 17:56:50.365738 21892 net.cpp:198] ReLU5 needs backward computation.
I0305 17:56:50.365742 21892 net.cpp:198] Scale6 needs backward computation.
I0305 17:56:50.365747 21892 net.cpp:198] BatchNorm6 needs backward computation.
I0305 17:56:50.365752 21892 net.cpp:198] Convolution6 needs backward computation.
I0305 17:56:50.365757 21892 net.cpp:198] Eltwise2_ReLU4_0_split needs backward computation.
I0305 17:56:50.365762 21892 net.cpp:198] ReLU4 needs backward computation.
I0305 17:56:50.365767 21892 net.cpp:198] Eltwise2 needs backward computation.
I0305 17:56:50.365773 21892 net.cpp:198] Scale5 needs backward computation.
I0305 17:56:50.365778 21892 net.cpp:198] BatchNorm5 needs backward computation.
I0305 17:56:50.365783 21892 net.cpp:198] Convolution5 needs backward computation.
I0305 17:56:50.365787 21892 net.cpp:198] ReLU3 needs backward computation.
I0305 17:56:50.365792 21892 net.cpp:198] Scale4 needs backward computation.
I0305 17:56:50.365797 21892 net.cpp:198] BatchNorm4 needs backward computation.
I0305 17:56:50.365802 21892 net.cpp:198] Convolution4 needs backward computation.
I0305 17:56:50.365806 21892 net.cpp:198] Eltwise1_ReLU2_0_split needs backward computation.
I0305 17:56:50.365811 21892 net.cpp:198] ReLU2 needs backward computation.
I0305 17:56:50.365816 21892 net.cpp:198] Eltwise1 needs backward computation.
I0305 17:56:50.365821 21892 net.cpp:198] Scale3 needs backward computation.
I0305 17:56:50.365826 21892 net.cpp:198] BatchNorm3 needs backward computation.
I0305 17:56:50.365830 21892 net.cpp:198] Convolution3 needs backward computation.
I0305 17:56:50.365836 21892 net.cpp:198] ReLU1 needs backward computation.
I0305 17:56:50.365840 21892 net.cpp:198] Scale2 needs backward computation.
I0305 17:56:50.365845 21892 net.cpp:198] BatchNorm2 needs backward computation.
I0305 17:56:50.365849 21892 net.cpp:198] Convolution2 needs backward computation.
I0305 17:56:50.365855 21892 net.cpp:198] Convolution1_Scale1_0_split needs backward computation.
I0305 17:56:50.365860 21892 net.cpp:198] Scale1 needs backward computation.
I0305 17:56:50.365865 21892 net.cpp:198] BatchNorm1 needs backward computation.
I0305 17:56:50.365869 21892 net.cpp:198] Convolution1 needs backward computation.
I0305 17:56:50.365875 21892 net.cpp:200] Concat1 does not need backward computation.
I0305 17:56:50.365881 21892 net.cpp:200] Silence does not need backward computation.
I0305 17:56:50.365887 21892 net.cpp:200] Data5 does not need backward computation.
I0305 17:56:50.365892 21892 net.cpp:200] Data3 does not need backward computation.
I0305 17:56:50.365897 21892 net.cpp:200] Data1 does not need backward computation.
I0305 17:56:50.365901 21892 net.cpp:242] This network produces output Loss
I0305 17:56:50.365964 21892 net.cpp:255] Network initialization done.
I0305 17:56:50.368926 21892 upgrade_proto.cpp:77] Attempting to upgrade batch norm layers using deprecated params: ./train_resnet_auto.prototxt
I0305 17:56:50.368978 21892 upgrade_proto.cpp:80] Successfully upgraded batch norm layers using deprecated params.
I0305 17:56:50.369004 21892 solver.cpp:172] Creating test net (#0) specified by net file: ./train_resnet_auto.prototxt
I0305 17:56:50.369774 21892 net.cpp:51] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "Data1"
  type: "Data"
  top: "Data1"
  top: "Data2"
  transform_param {
    scale: 0.01
  }
  data_param {
    source: "/data3/lzh/1000x224x224/raw_data_photon_dis"
    batch_size: 24
    backend: LMDB
  }
}
layer {
  name: "Data3"
  type: "Data"
  top: "Data3"
  top: "Data4"
  transform_param {
    scale: 1
  }
  data_param {
    source: "/data3/lzh/1000x224x224/raw_data_photon_flux"
    batch_size: 24
    backend: LMDB
  }
}
layer {
  name: "Data5"
  type: "Data"
  top: "Data5"
  top: "Data6"
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "/data3/lzh/1000x224x224/raw_data_conv"
    batch_size: 24
    backend: LMDB
  }
}
layer {
  name: "Silence"
  type: "Silence"
  bottom: "Data2"
  bottom: "Data4"
  bottom: "Data6"
}
layer {
  name: "Concat1"
  type: "Concat"
  bottom: "Data1"
  bottom: "Data3"
  top: "Concat1"
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "Concat1"
  top: "Convolution1"
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 3
    kernel_size: 7
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "Convolution1"
  top: "Convolution1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "Convolution1"
  top: "Convolution1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Convolution1"
  top: "Convolution2"
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "Convolution2"
  top: "Convolution2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "Convolution2"
  top: "Convolution2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "Convolution2"
  top: "Convolution2"
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "Convolution2"
  top: "Convolution3"
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "Convolution3"
  top: "Convolution3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "Convolution3"
  top: "Convolution3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise1"
  type: "Eltwise"
  bottom: "Convolution3"
  bottom: "Convolution1"
  top: "Eltwise1"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU2"
  type: "ReLU"
  bottom: "Eltwise1"
  top: "Eltwise1"
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "Eltwise1"
  top: "Convolution4"
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm4"
  type: "BatchNorm"
  bottom: "Convolution4"
  top: "Convolution4"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale4"
  type: "Scale"
  bottom: "Convolution4"
  top: "Convolution4"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU3"
  type: "ReLU"
  bottom: "Convolution4"
  top: "Convolution4"
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "Convolution4"
  top: "Convolution5"
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm5"
  type: "BatchNorm"
  bottom: "Convolution5"
  top: "Convolution5"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale5"
  type: "Scale"
  bottom: "Convolution5"
  top: "Convolution5"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise2"
  type: "Eltwise"
  bottom: "Convolution5"
  bottom: "Eltwise1"
  top: "Eltwise2"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU4"
  type: "ReLU"
  bottom: "Eltwise2"
  top: "Eltwise2"
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "Eltwise2"
  top: "Convolution6"
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm6"
  type: "BatchNorm"
  bottom: "Convolution6"
  top: "Convolution6"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale6"
  type: "Scale"
  bottom: "Convolution6"
  top: "Convolution6"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU5"
  type: "ReLU"
  bottom: "Convolution6"
  top: "Convolution6"
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "Convolution6"
  top: "Convolution7"
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm7"
  type: "BatchNorm"
  bottom: "Convolution7"
  top: "Convolution7"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale7"
  type: "Scale"
  bottom: "Convolution7"
  top: "Convolution7"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise3"
  type: "Eltwise"
  bottom: "Convolution7"
  bottom: "Eltwise2"
  top: "Eltwise3"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU6"
  type: "ReLU"
  bottom: "Eltwise3"
  top: "Eltwise3"
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution8"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm8"
  type: "BatchNorm"
  bottom: "Convolution8"
  top: "Convolution8"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale8"
  type: "Scale"
  bottom: "Convolution8"
  top: "Convolution8"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU7"
  type: "ReLU"
  bottom: "Convolution8"
  top: "Convolution8"
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "Convolution8"
  top: "Convolution9"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm9"
  type: "BatchNorm"
  bottom: "Convolution9"
  top: "Convolution9"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale9"
  type: "Scale"
  bottom: "Convolution9"
  top: "Convolution9"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "Eltwise3"
  top: "Convolution10"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm10"
  type: "BatchNorm"
  bottom: "Convolution10"
  top: "Convolution10"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale10"
  type: "Scale"
  bottom: "Convolution10"
  top: "Convolution10"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise4"
  type: "Eltwise"
  bottom: "Convolution9"
  bottom: "Convolution10"
  top: "Eltwise4"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU8"
  type: "ReLU"
  bottom: "Eltwise4"
  top: "Eltwise4"
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "Eltwise4"
  top: "Convolution11"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm11"
  type: "BatchNorm"
  bottom: "Convolution11"
  top: "Convolution11"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale11"
  type: "Scale"
  bottom: "Convolution11"
  top: "Convolution11"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU9"
  type: "ReLU"
  bottom: "Convolution11"
  top: "Convolution11"
}
layer {
  name: "Convolution12"
  type: "Convolution"
  bottom: "Convolution11"
  top: "Convolution12"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm12"
  type: "BatchNorm"
  bottom: "Convolution12"
  top: "Convolution12"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale12"
  type: "Scale"
  bottom: "Convolution12"
  top: "Convolution12"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise5"
  type: "Eltwise"
  bottom: "Convolution12"
  bottom: "Eltwise4"
  top: "Eltwise5"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU10"
  type: "ReLU"
  bottom: "Eltwise5"
  top: "Eltwise5"
}
layer {
  name: "Convolution13"
  type: "Convolution"
  bottom: "Eltwise5"
  top: "Convolution13"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm13"
  type: "BatchNorm"
  bottom: "Convolution13"
  top: "Convolution13"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale13"
  type: "Scale"
  bottom: "Convolution13"
  top: "Convolution13"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU11"
  type: "ReLU"
  bottom: "Convolution13"
  top: "Convolution13"
}
layer {
  name: "Convolution14"
  type: "Convolution"
  bottom: "Convolution13"
  top: "Convolution14"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm14"
  type: "BatchNorm"
  bottom: "Convolution14"
  top: "Convolution14"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale14"
  type: "Scale"
  bottom: "Convolution14"
  top: "Convolution14"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise6"
  type: "Eltwise"
  bottom: "Convolution14"
  bottom: "Eltwise5"
  top: "Eltwise6"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU12"
  type: "ReLU"
  bottom: "Eltwise6"
  top: "Eltwise6"
}
layer {
  name: "Convolution15"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution15"
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm15"
  type: "BatchNorm"
  bottom: "Convolution15"
  top: "Convolution15"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale15"
  type: "Scale"
  bottom: "Convolution15"
  top: "Convolution15"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU13"
  type: "ReLU"
  bottom: "Convolution15"
  top: "Convolution15"
}
layer {
  name: "Convolution16"
  type: "Convolution"
  bottom: "Convolution15"
  top: "Convolution16"
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm16"
  type: "BatchNorm"
  bottom: "Convolution16"
  top: "Convolution16"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale16"
  type: "Scale"
  bottom: "Convolution16"
  top: "Convolution16"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Convolution17"
  type: "Convolution"
  bottom: "Eltwise6"
  top: "Convolution17"
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm17"
  type: "BatchNorm"
  bottom: "Convolution17"
  top: "Convolution17"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale17"
  type: "Scale"
  bottom: "Convolution17"
  top: "Convolution17"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise7"
  type: "Eltwise"
  bottom: "Convolution16"
  bottom: "Convolution17"
  top: "Eltwise7"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU14"
  type: "ReLU"
  bottom: "Eltwise7"
  top: "Eltwise7"
}
layer {
  name: "Convolution18"
  type: "Convolution"
  bottom: "Eltwise7"
  top: "Convolution18"
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm18"
  type: "BatchNorm"
  bottom: "Convolution18"
  top: "Convolution18"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale18"
  type: "Scale"
  bottom: "Convolution18"
  top: "Convolution18"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU15"
  type: "ReLU"
  bottom: "Convolution18"
  top: "Convolution18"
}
layer {
  name: "Convolution19"
  type: "Convolution"
  bottom: "Convolution18"
  top: "Convolution19"
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm19"
  type: "BatchNorm"
  bottom: "Convolution19"
  top: "Convolution19"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale19"
  type: "Scale"
  bottom: "Convolution19"
  top: "Convolution19"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise8"
  type: "Eltwise"
  bottom: "Convolution19"
  bottom: "Eltwise7"
  top: "Eltwise8"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU16"
  type: "ReLU"
  bottom: "Eltwise8"
  top: "Eltwise8"
}
layer {
  name: "Convolution20"
  type: "Convolution"
  bottom: "Eltwise8"
  top: "Convolution20"
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm20"
  type: "BatchNorm"
  bottom: "Convolution20"
  top: "Convolution20"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale20"
  type: "Scale"
  bottom: "Convolution20"
  top: "Convolution20"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU17"
  type: "ReLU"
  bottom: "Convolution20"
  top: "Convolution20"
}
layer {
  name: "Convolution21"
  type: "Convolution"
  bottom: "Convolution20"
  top: "Convolution21"
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm21"
  type: "BatchNorm"
  bottom: "Convolution21"
  top: "Convolution21"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale21"
  type: "Scale"
  bottom: "Convolution21"
  top: "Convolution21"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Eltwise9"
  type: "Eltwise"
  bottom: "Convolution21"
  bottom: "Eltwise8"
  top: "Eltwise9"
  eltwise_param {
    operation: SUM
  }
}
layer {
  name: "ReLU18"
  type: "ReLU"
  bottom: "Eltwise9"
  top: "Eltwise9"
}
layer {
  name: "Deconvolution1"
  type: "Deconvolution"
  bottom: "Eltwise9"
  top: "Deconvolution1"
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 0
    kernel_size: 2
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm22"
  type: "BatchNorm"
  bottom: "Deconvolution1"
  top: "Deconvolution1"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale22"
  type: "Scale"
  bottom: "Deconvolution1"
  top: "Deconvolution1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU19"
  type: "ReLU"
  bottom: "Deconvolution1"
  top: "Deconvolution1"
}
layer {
  name: "Deconvolution2"
  type: "Deconvolution"
  bottom: "Deconvolution1"
  top: "Deconvolution2"
  convolution_param {
    num_output: 64
    bias_term: true
    pad: 0
    kernel_size: 2
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm23"
  type: "BatchNorm"
  bottom: "Deconvolution2"
  top: "Deconvolution2"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale23"
  type: "Scale"
  bottom: "Deconvolution2"
  top: "Deconvolution2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU20"
  type: "ReLU"
  bottom: "Deconvolution2"
  top: "Deconvolution2"
}
layer {
  name: "Deconvolution3"
  type: "Deconvolution"
  bottom: "Deconvolution2"
  top: "Deconvolution3"
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 0
    kernel_size: 2
    stride: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm24"
  type: "BatchNorm"
  bottom: "Deconvolution3"
  top: "Deconvolution3"
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale24"
  type: "Scale"
  bottom: "Deconvolution3"
  top: "Deconvolution3"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU21"
  type: "ReLU"
  bottom: "Deconvolution3"
  top: "Deconvolution3"
}
layer {
  name: "Convolution22"
  type: "Convolution"
  bottom: "Deconvolution3"
  top: "Convolution22"
  convolution_param {
    num_output: 1
    bias_term: true
    pad: 0
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "ReLU22"
  type: "ReLU"
  bottom: "Convolution22"
  top: "Convolution22"
}
layer {
  name: "Loss"
  type: "EuclideanLoss"
  bottom: "Convolution22"
  bottom: "Data5"
  top: "Loss"
  propagate_down: true
  propagate_down: false
}
I0305 17:56:50.398296 21892 layer_factory.hpp:77] Creating layer Data1
I0305 17:56:50.398411 21892 db_lmdb.cpp:35] Opened lmdb /data3/lzh/1000x224x224/raw_data_photon_dis
I0305 17:56:50.398434 21892 net.cpp:84] Creating Layer Data1
I0305 17:56:50.398442 21892 net.cpp:380] Data1 -> Data1
I0305 17:56:50.398457 21892 net.cpp:380] Data1 -> Data2
I0305 17:56:50.403717 21892 data_layer.cpp:45] output data size: 24,20,224,224
I0305 17:56:50.706256 21892 net.cpp:122] Setting up Data1
I0305 17:56:50.706300 21892 net.cpp:129] Top shape: 24 20 224 224 (24084480)
I0305 17:56:50.706308 21892 net.cpp:129] Top shape: 24 (24)
I0305 17:56:50.706313 21892 net.cpp:137] Memory required for data: 96338016
I0305 17:56:50.706322 21892 layer_factory.hpp:77] Creating layer Data3
I0305 17:56:50.706418 21892 db_lmdb.cpp:35] Opened lmdb /data3/lzh/1000x224x224/raw_data_photon_flux
I0305 17:56:50.706441 21892 net.cpp:84] Creating Layer Data3
I0305 17:56:50.706460 21892 net.cpp:380] Data3 -> Data3
I0305 17:56:50.706477 21892 net.cpp:380] Data3 -> Data4
I0305 17:56:50.715683 21892 data_layer.cpp:45] output data size: 24,20,224,224
I0305 17:56:51.030798 21892 net.cpp:122] Setting up Data3
I0305 17:56:51.030858 21892 net.cpp:129] Top shape: 24 20 224 224 (24084480)
I0305 17:56:51.030875 21892 net.cpp:129] Top shape: 24 (24)
I0305 17:56:51.030884 21892 net.cpp:137] Memory required for data: 192676032
I0305 17:56:51.030894 21892 layer_factory.hpp:77] Creating layer Data5
I0305 17:56:51.031023 21892 db_lmdb.cpp:35] Opened lmdb /data3/lzh/1000x224x224/raw_data_conv
I0305 17:56:51.031054 21892 net.cpp:84] Creating Layer Data5
I0305 17:56:51.031067 21892 net.cpp:380] Data5 -> Data5
I0305 17:56:51.031088 21892 net.cpp:380] Data5 -> Data6
I0305 17:56:51.031253 21892 data_layer.cpp:45] output data size: 24,1,224,224
I0305 17:56:51.090023 21892 net.cpp:122] Setting up Data5
I0305 17:56:51.090098 21892 net.cpp:129] Top shape: 24 1 224 224 (1204224)
I0305 17:56:51.090116 21892 net.cpp:129] Top shape: 24 (24)
I0305 17:56:51.090129 21892 net.cpp:137] Memory required for data: 197493024
I0305 17:56:51.090147 21892 layer_factory.hpp:77] Creating layer Silence
I0305 17:56:51.090170 21892 net.cpp:84] Creating Layer Silence
I0305 17:56:51.090186 21892 net.cpp:406] Silence <- Data2
I0305 17:56:51.090206 21892 net.cpp:406] Silence <- Data4
I0305 17:56:51.090220 21892 net.cpp:406] Silence <- Data6
I0305 17:56:51.090234 21892 net.cpp:122] Setting up Silence
I0305 17:56:51.090246 21892 net.cpp:137] Memory required for data: 197493024
I0305 17:56:51.090260 21892 layer_factory.hpp:77] Creating layer Concat1
I0305 17:56:51.090279 21892 net.cpp:84] Creating Layer Concat1
I0305 17:56:51.090292 21892 net.cpp:406] Concat1 <- Data1
I0305 17:56:51.090306 21892 net.cpp:406] Concat1 <- Data3
I0305 17:56:51.090322 21892 net.cpp:380] Concat1 -> Concat1
I0305 17:56:51.090407 21892 net.cpp:122] Setting up Concat1
I0305 17:56:51.090427 21892 net.cpp:129] Top shape: 24 40 224 224 (48168960)
I0305 17:56:51.090440 21892 net.cpp:137] Memory required for data: 390168864
I0305 17:56:51.090503 21892 layer_factory.hpp:77] Creating layer Convolution1
I0305 17:56:51.090529 21892 net.cpp:84] Creating Layer Convolution1
I0305 17:56:51.090543 21892 net.cpp:406] Convolution1 <- Concat1
I0305 17:56:51.090560 21892 net.cpp:380] Convolution1 -> Convolution1
I0305 17:56:51.091768 21892 net.cpp:122] Setting up Convolution1
I0305 17:56:51.091795 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:51.091804 21892 net.cpp:137] Memory required for data: 428704032
I0305 17:56:51.091823 21892 layer_factory.hpp:77] Creating layer BatchNorm1
I0305 17:56:51.091838 21892 net.cpp:84] Creating Layer BatchNorm1
I0305 17:56:51.091846 21892 net.cpp:406] BatchNorm1 <- Convolution1
I0305 17:56:51.091858 21892 net.cpp:367] BatchNorm1 -> Convolution1 (in-place)
I0305 17:56:51.092097 21892 net.cpp:122] Setting up BatchNorm1
I0305 17:56:51.092113 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:51.092120 21892 net.cpp:137] Memory required for data: 467239200
I0305 17:56:51.092137 21892 layer_factory.hpp:77] Creating layer Scale1
I0305 17:56:51.092151 21892 net.cpp:84] Creating Layer Scale1
I0305 17:56:51.092159 21892 net.cpp:406] Scale1 <- Convolution1
I0305 17:56:51.092169 21892 net.cpp:367] Scale1 -> Convolution1 (in-place)
I0305 17:56:51.092234 21892 layer_factory.hpp:77] Creating layer Scale1
I0305 17:56:51.092399 21892 net.cpp:122] Setting up Scale1
I0305 17:56:51.092412 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:51.092419 21892 net.cpp:137] Memory required for data: 505774368
I0305 17:56:51.092429 21892 layer_factory.hpp:77] Creating layer Convolution1_Scale1_0_split
I0305 17:56:51.092443 21892 net.cpp:84] Creating Layer Convolution1_Scale1_0_split
I0305 17:56:51.092452 21892 net.cpp:406] Convolution1_Scale1_0_split <- Convolution1
I0305 17:56:51.092461 21892 net.cpp:380] Convolution1_Scale1_0_split -> Convolution1_Scale1_0_split_0
I0305 17:56:51.092474 21892 net.cpp:380] Convolution1_Scale1_0_split -> Convolution1_Scale1_0_split_1
I0305 17:56:51.092520 21892 net.cpp:122] Setting up Convolution1_Scale1_0_split
I0305 17:56:51.092532 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:51.092541 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:51.092548 21892 net.cpp:137] Memory required for data: 582844704
I0305 17:56:51.092555 21892 layer_factory.hpp:77] Creating layer Convolution2
I0305 17:56:51.092571 21892 net.cpp:84] Creating Layer Convolution2
I0305 17:56:51.092579 21892 net.cpp:406] Convolution2 <- Convolution1_Scale1_0_split_0
I0305 17:56:51.092589 21892 net.cpp:380] Convolution2 -> Convolution2
I0305 17:56:51.094830 21892 net.cpp:122] Setting up Convolution2
I0305 17:56:51.094916 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:51.094928 21892 net.cpp:137] Memory required for data: 621379872
I0305 17:56:51.094949 21892 layer_factory.hpp:77] Creating layer BatchNorm2
I0305 17:56:51.094969 21892 net.cpp:84] Creating Layer BatchNorm2
I0305 17:56:51.094980 21892 net.cpp:406] BatchNorm2 <- Convolution2
I0305 17:56:51.094992 21892 net.cpp:367] BatchNorm2 -> Convolution2 (in-place)
I0305 17:56:51.095480 21892 net.cpp:122] Setting up BatchNorm2
I0305 17:56:51.095495 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:51.095502 21892 net.cpp:137] Memory required for data: 659915040
I0305 17:56:51.095515 21892 layer_factory.hpp:77] Creating layer Scale2
I0305 17:56:51.095530 21892 net.cpp:84] Creating Layer Scale2
I0305 17:56:51.095538 21892 net.cpp:406] Scale2 <- Convolution2
I0305 17:56:51.095548 21892 net.cpp:367] Scale2 -> Convolution2 (in-place)
I0305 17:56:51.095607 21892 layer_factory.hpp:77] Creating layer Scale2
I0305 17:56:51.095762 21892 net.cpp:122] Setting up Scale2
I0305 17:56:51.095774 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:51.095782 21892 net.cpp:137] Memory required for data: 698450208
I0305 17:56:51.095803 21892 layer_factory.hpp:77] Creating layer ReLU1
I0305 17:56:51.095815 21892 net.cpp:84] Creating Layer ReLU1
I0305 17:56:51.095854 21892 net.cpp:406] ReLU1 <- Convolution2
I0305 17:56:51.095865 21892 net.cpp:367] ReLU1 -> Convolution2 (in-place)
I0305 17:56:51.095877 21892 net.cpp:122] Setting up ReLU1
I0305 17:56:51.095887 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:51.095896 21892 net.cpp:137] Memory required for data: 736985376
I0305 17:56:51.095902 21892 layer_factory.hpp:77] Creating layer Convolution3
I0305 17:56:51.095917 21892 net.cpp:84] Creating Layer Convolution3
I0305 17:56:51.095927 21892 net.cpp:406] Convolution3 <- Convolution2
I0305 17:56:51.095938 21892 net.cpp:380] Convolution3 -> Convolution3
I0305 17:56:51.096302 21892 net.cpp:122] Setting up Convolution3
I0305 17:56:51.096316 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:51.096324 21892 net.cpp:137] Memory required for data: 775520544
I0305 17:56:51.096334 21892 layer_factory.hpp:77] Creating layer BatchNorm3
I0305 17:56:51.096346 21892 net.cpp:84] Creating Layer BatchNorm3
I0305 17:56:51.096354 21892 net.cpp:406] BatchNorm3 <- Convolution3
I0305 17:56:51.096364 21892 net.cpp:367] BatchNorm3 -> Convolution3 (in-place)
I0305 17:56:51.096598 21892 net.cpp:122] Setting up BatchNorm3
I0305 17:56:51.096609 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:51.096616 21892 net.cpp:137] Memory required for data: 814055712
I0305 17:56:51.096632 21892 layer_factory.hpp:77] Creating layer Scale3
I0305 17:56:51.096644 21892 net.cpp:84] Creating Layer Scale3
I0305 17:56:51.096652 21892 net.cpp:406] Scale3 <- Convolution3
I0305 17:56:51.096662 21892 net.cpp:367] Scale3 -> Convolution3 (in-place)
I0305 17:56:51.096709 21892 layer_factory.hpp:77] Creating layer Scale3
I0305 17:56:51.096859 21892 net.cpp:122] Setting up Scale3
I0305 17:56:51.096871 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:51.096879 21892 net.cpp:137] Memory required for data: 852590880
I0305 17:56:51.096889 21892 layer_factory.hpp:77] Creating layer Eltwise1
I0305 17:56:51.096904 21892 net.cpp:84] Creating Layer Eltwise1
I0305 17:56:51.096910 21892 net.cpp:406] Eltwise1 <- Convolution3
I0305 17:56:51.096920 21892 net.cpp:406] Eltwise1 <- Convolution1_Scale1_0_split_1
I0305 17:56:51.096930 21892 net.cpp:380] Eltwise1 -> Eltwise1
I0305 17:56:51.096962 21892 net.cpp:122] Setting up Eltwise1
I0305 17:56:51.096973 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:51.096981 21892 net.cpp:137] Memory required for data: 891126048
I0305 17:56:51.096987 21892 layer_factory.hpp:77] Creating layer ReLU2
I0305 17:56:51.096998 21892 net.cpp:84] Creating Layer ReLU2
I0305 17:56:51.097005 21892 net.cpp:406] ReLU2 <- Eltwise1
I0305 17:56:51.097015 21892 net.cpp:367] ReLU2 -> Eltwise1 (in-place)
I0305 17:56:51.097023 21892 net.cpp:122] Setting up ReLU2
I0305 17:56:51.097033 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:51.097038 21892 net.cpp:137] Memory required for data: 929661216
I0305 17:56:51.097046 21892 layer_factory.hpp:77] Creating layer Eltwise1_ReLU2_0_split
I0305 17:56:51.097056 21892 net.cpp:84] Creating Layer Eltwise1_ReLU2_0_split
I0305 17:56:51.097064 21892 net.cpp:406] Eltwise1_ReLU2_0_split <- Eltwise1
I0305 17:56:51.097072 21892 net.cpp:380] Eltwise1_ReLU2_0_split -> Eltwise1_ReLU2_0_split_0
I0305 17:56:51.097081 21892 net.cpp:380] Eltwise1_ReLU2_0_split -> Eltwise1_ReLU2_0_split_1
I0305 17:56:51.097123 21892 net.cpp:122] Setting up Eltwise1_ReLU2_0_split
I0305 17:56:51.097133 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:51.097141 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:51.097148 21892 net.cpp:137] Memory required for data: 1006731552
I0305 17:56:51.097156 21892 layer_factory.hpp:77] Creating layer Convolution4
I0305 17:56:51.097169 21892 net.cpp:84] Creating Layer Convolution4
I0305 17:56:51.097177 21892 net.cpp:406] Convolution4 <- Eltwise1_ReLU2_0_split_0
I0305 17:56:51.097187 21892 net.cpp:380] Convolution4 -> Convolution4
I0305 17:56:51.097524 21892 net.cpp:122] Setting up Convolution4
I0305 17:56:51.097538 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:51.097558 21892 net.cpp:137] Memory required for data: 1045266720
I0305 17:56:51.097568 21892 layer_factory.hpp:77] Creating layer BatchNorm4
I0305 17:56:51.097580 21892 net.cpp:84] Creating Layer BatchNorm4
I0305 17:56:51.097589 21892 net.cpp:406] BatchNorm4 <- Convolution4
I0305 17:56:51.097597 21892 net.cpp:367] BatchNorm4 -> Convolution4 (in-place)
I0305 17:56:51.097846 21892 net.cpp:122] Setting up BatchNorm4
I0305 17:56:51.097858 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:51.097865 21892 net.cpp:137] Memory required for data: 1083801888
I0305 17:56:51.097877 21892 layer_factory.hpp:77] Creating layer Scale4
I0305 17:56:51.097888 21892 net.cpp:84] Creating Layer Scale4
I0305 17:56:51.097895 21892 net.cpp:406] Scale4 <- Convolution4
I0305 17:56:51.097905 21892 net.cpp:367] Scale4 -> Convolution4 (in-place)
I0305 17:56:51.097952 21892 layer_factory.hpp:77] Creating layer Scale4
I0305 17:56:51.098104 21892 net.cpp:122] Setting up Scale4
I0305 17:56:51.098115 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:51.098122 21892 net.cpp:137] Memory required for data: 1122337056
I0305 17:56:51.098132 21892 layer_factory.hpp:77] Creating layer ReLU3
I0305 17:56:51.098143 21892 net.cpp:84] Creating Layer ReLU3
I0305 17:56:51.098151 21892 net.cpp:406] ReLU3 <- Convolution4
I0305 17:56:51.098160 21892 net.cpp:367] ReLU3 -> Convolution4 (in-place)
I0305 17:56:51.098170 21892 net.cpp:122] Setting up ReLU3
I0305 17:56:51.098177 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:51.098184 21892 net.cpp:137] Memory required for data: 1160872224
I0305 17:56:51.098191 21892 layer_factory.hpp:77] Creating layer Convolution5
I0305 17:56:51.098204 21892 net.cpp:84] Creating Layer Convolution5
I0305 17:56:51.098212 21892 net.cpp:406] Convolution5 <- Convolution4
I0305 17:56:51.098222 21892 net.cpp:380] Convolution5 -> Convolution5
I0305 17:56:51.098569 21892 net.cpp:122] Setting up Convolution5
I0305 17:56:51.098584 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:51.098592 21892 net.cpp:137] Memory required for data: 1199407392
I0305 17:56:51.098603 21892 layer_factory.hpp:77] Creating layer BatchNorm5
I0305 17:56:51.098613 21892 net.cpp:84] Creating Layer BatchNorm5
I0305 17:56:51.098621 21892 net.cpp:406] BatchNorm5 <- Convolution5
I0305 17:56:51.098630 21892 net.cpp:367] BatchNorm5 -> Convolution5 (in-place)
I0305 17:56:51.098860 21892 net.cpp:122] Setting up BatchNorm5
I0305 17:56:51.098899 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:51.098907 21892 net.cpp:137] Memory required for data: 1237942560
I0305 17:56:51.098927 21892 layer_factory.hpp:77] Creating layer Scale5
I0305 17:56:51.098938 21892 net.cpp:84] Creating Layer Scale5
I0305 17:56:51.098947 21892 net.cpp:406] Scale5 <- Convolution5
I0305 17:56:51.098956 21892 net.cpp:367] Scale5 -> Convolution5 (in-place)
I0305 17:56:51.099006 21892 layer_factory.hpp:77] Creating layer Scale5
I0305 17:56:51.099167 21892 net.cpp:122] Setting up Scale5
I0305 17:56:51.099179 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:51.099187 21892 net.cpp:137] Memory required for data: 1276477728
I0305 17:56:51.099196 21892 layer_factory.hpp:77] Creating layer Eltwise2
I0305 17:56:51.099208 21892 net.cpp:84] Creating Layer Eltwise2
I0305 17:56:51.099215 21892 net.cpp:406] Eltwise2 <- Convolution5
I0305 17:56:51.099225 21892 net.cpp:406] Eltwise2 <- Eltwise1_ReLU2_0_split_1
I0305 17:56:51.099233 21892 net.cpp:380] Eltwise2 -> Eltwise2
I0305 17:56:51.099265 21892 net.cpp:122] Setting up Eltwise2
I0305 17:56:51.099275 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:51.099282 21892 net.cpp:137] Memory required for data: 1315012896
I0305 17:56:51.099289 21892 layer_factory.hpp:77] Creating layer ReLU4
I0305 17:56:51.099314 21892 net.cpp:84] Creating Layer ReLU4
I0305 17:56:51.099321 21892 net.cpp:406] ReLU4 <- Eltwise2
I0305 17:56:51.099330 21892 net.cpp:367] ReLU4 -> Eltwise2 (in-place)
I0305 17:56:51.099339 21892 net.cpp:122] Setting up ReLU4
I0305 17:56:51.099365 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:51.099373 21892 net.cpp:137] Memory required for data: 1353548064
I0305 17:56:51.099380 21892 layer_factory.hpp:77] Creating layer Eltwise2_ReLU4_0_split
I0305 17:56:51.099391 21892 net.cpp:84] Creating Layer Eltwise2_ReLU4_0_split
I0305 17:56:51.099398 21892 net.cpp:406] Eltwise2_ReLU4_0_split <- Eltwise2
I0305 17:56:51.099408 21892 net.cpp:380] Eltwise2_ReLU4_0_split -> Eltwise2_ReLU4_0_split_0
I0305 17:56:51.099417 21892 net.cpp:380] Eltwise2_ReLU4_0_split -> Eltwise2_ReLU4_0_split_1
I0305 17:56:51.099462 21892 net.cpp:122] Setting up Eltwise2_ReLU4_0_split
I0305 17:56:51.099472 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:51.099479 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:51.099486 21892 net.cpp:137] Memory required for data: 1430618400
I0305 17:56:51.099493 21892 layer_factory.hpp:77] Creating layer Convolution6
I0305 17:56:51.099506 21892 net.cpp:84] Creating Layer Convolution6
I0305 17:56:51.099514 21892 net.cpp:406] Convolution6 <- Eltwise2_ReLU4_0_split_0
I0305 17:56:51.099524 21892 net.cpp:380] Convolution6 -> Convolution6
I0305 17:56:51.099876 21892 net.cpp:122] Setting up Convolution6
I0305 17:56:51.099890 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:51.099897 21892 net.cpp:137] Memory required for data: 1469153568
I0305 17:56:51.099907 21892 layer_factory.hpp:77] Creating layer BatchNorm6
I0305 17:56:51.099920 21892 net.cpp:84] Creating Layer BatchNorm6
I0305 17:56:51.099928 21892 net.cpp:406] BatchNorm6 <- Convolution6
I0305 17:56:51.099937 21892 net.cpp:367] BatchNorm6 -> Convolution6 (in-place)
I0305 17:56:51.100183 21892 net.cpp:122] Setting up BatchNorm6
I0305 17:56:51.100205 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:51.100211 21892 net.cpp:137] Memory required for data: 1507688736
I0305 17:56:51.100224 21892 layer_factory.hpp:77] Creating layer Scale6
I0305 17:56:51.100234 21892 net.cpp:84] Creating Layer Scale6
I0305 17:56:51.100242 21892 net.cpp:406] Scale6 <- Convolution6
I0305 17:56:51.100251 21892 net.cpp:367] Scale6 -> Convolution6 (in-place)
I0305 17:56:51.100302 21892 layer_factory.hpp:77] Creating layer Scale6
I0305 17:56:51.100457 21892 net.cpp:122] Setting up Scale6
I0305 17:56:51.100471 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:51.100476 21892 net.cpp:137] Memory required for data: 1546223904
I0305 17:56:51.100486 21892 layer_factory.hpp:77] Creating layer ReLU5
I0305 17:56:51.100502 21892 net.cpp:84] Creating Layer ReLU5
I0305 17:56:51.100509 21892 net.cpp:406] ReLU5 <- Convolution6
I0305 17:56:51.100518 21892 net.cpp:367] ReLU5 -> Convolution6 (in-place)
I0305 17:56:51.100528 21892 net.cpp:122] Setting up ReLU5
I0305 17:56:51.100535 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:51.100541 21892 net.cpp:137] Memory required for data: 1584759072
I0305 17:56:51.100548 21892 layer_factory.hpp:77] Creating layer Convolution7
I0305 17:56:51.100561 21892 net.cpp:84] Creating Layer Convolution7
I0305 17:56:51.100569 21892 net.cpp:406] Convolution7 <- Convolution6
I0305 17:56:51.100579 21892 net.cpp:380] Convolution7 -> Convolution7
I0305 17:56:51.100919 21892 net.cpp:122] Setting up Convolution7
I0305 17:56:51.100941 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:51.100949 21892 net.cpp:137] Memory required for data: 1623294240
I0305 17:56:51.100958 21892 layer_factory.hpp:77] Creating layer BatchNorm7
I0305 17:56:51.100972 21892 net.cpp:84] Creating Layer BatchNorm7
I0305 17:56:51.100980 21892 net.cpp:406] BatchNorm7 <- Convolution7
I0305 17:56:51.100991 21892 net.cpp:367] BatchNorm7 -> Convolution7 (in-place)
I0305 17:56:51.101222 21892 net.cpp:122] Setting up BatchNorm7
I0305 17:56:51.101234 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:51.101249 21892 net.cpp:137] Memory required for data: 1661829408
I0305 17:56:51.101260 21892 layer_factory.hpp:77] Creating layer Scale7
I0305 17:56:51.101272 21892 net.cpp:84] Creating Layer Scale7
I0305 17:56:51.101303 21892 net.cpp:406] Scale7 <- Convolution7
I0305 17:56:51.101313 21892 net.cpp:367] Scale7 -> Convolution7 (in-place)
I0305 17:56:51.101366 21892 layer_factory.hpp:77] Creating layer Scale7
I0305 17:56:51.101531 21892 net.cpp:122] Setting up Scale7
I0305 17:56:51.101542 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:51.101550 21892 net.cpp:137] Memory required for data: 1700364576
I0305 17:56:51.101567 21892 layer_factory.hpp:77] Creating layer Eltwise3
I0305 17:56:51.101579 21892 net.cpp:84] Creating Layer Eltwise3
I0305 17:56:51.101586 21892 net.cpp:406] Eltwise3 <- Convolution7
I0305 17:56:51.101594 21892 net.cpp:406] Eltwise3 <- Eltwise2_ReLU4_0_split_1
I0305 17:56:51.101604 21892 net.cpp:380] Eltwise3 -> Eltwise3
I0305 17:56:51.101636 21892 net.cpp:122] Setting up Eltwise3
I0305 17:56:51.101646 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:51.101653 21892 net.cpp:137] Memory required for data: 1738899744
I0305 17:56:51.101660 21892 layer_factory.hpp:77] Creating layer ReLU6
I0305 17:56:51.101670 21892 net.cpp:84] Creating Layer ReLU6
I0305 17:56:51.101677 21892 net.cpp:406] ReLU6 <- Eltwise3
I0305 17:56:51.101687 21892 net.cpp:367] ReLU6 -> Eltwise3 (in-place)
I0305 17:56:51.101696 21892 net.cpp:122] Setting up ReLU6
I0305 17:56:51.101704 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:51.101711 21892 net.cpp:137] Memory required for data: 1777434912
I0305 17:56:51.101717 21892 layer_factory.hpp:77] Creating layer Eltwise3_ReLU6_0_split
I0305 17:56:51.101728 21892 net.cpp:84] Creating Layer Eltwise3_ReLU6_0_split
I0305 17:56:51.101745 21892 net.cpp:406] Eltwise3_ReLU6_0_split <- Eltwise3
I0305 17:56:51.101758 21892 net.cpp:380] Eltwise3_ReLU6_0_split -> Eltwise3_ReLU6_0_split_0
I0305 17:56:51.101768 21892 net.cpp:380] Eltwise3_ReLU6_0_split -> Eltwise3_ReLU6_0_split_1
I0305 17:56:51.101810 21892 net.cpp:122] Setting up Eltwise3_ReLU6_0_split
I0305 17:56:51.101821 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:51.101830 21892 net.cpp:129] Top shape: 24 32 112 112 (9633792)
I0305 17:56:51.101835 21892 net.cpp:137] Memory required for data: 1854505248
I0305 17:56:51.101843 21892 layer_factory.hpp:77] Creating layer Convolution8
I0305 17:56:51.101856 21892 net.cpp:84] Creating Layer Convolution8
I0305 17:56:51.101864 21892 net.cpp:406] Convolution8 <- Eltwise3_ReLU6_0_split_0
I0305 17:56:51.101874 21892 net.cpp:380] Convolution8 -> Convolution8
I0305 17:56:51.102321 21892 net.cpp:122] Setting up Convolution8
I0305 17:56:51.102349 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:51.102356 21892 net.cpp:137] Memory required for data: 1873772832
I0305 17:56:51.102366 21892 layer_factory.hpp:77] Creating layer BatchNorm8
I0305 17:56:51.102378 21892 net.cpp:84] Creating Layer BatchNorm8
I0305 17:56:51.102385 21892 net.cpp:406] BatchNorm8 <- Convolution8
I0305 17:56:51.102396 21892 net.cpp:367] BatchNorm8 -> Convolution8 (in-place)
I0305 17:56:51.102668 21892 net.cpp:122] Setting up BatchNorm8
I0305 17:56:51.102680 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:51.102686 21892 net.cpp:137] Memory required for data: 1893040416
I0305 17:56:51.102699 21892 layer_factory.hpp:77] Creating layer Scale8
I0305 17:56:51.102710 21892 net.cpp:84] Creating Layer Scale8
I0305 17:56:51.102717 21892 net.cpp:406] Scale8 <- Convolution8
I0305 17:56:51.102726 21892 net.cpp:367] Scale8 -> Convolution8 (in-place)
I0305 17:56:51.102779 21892 layer_factory.hpp:77] Creating layer Scale8
I0305 17:56:51.102967 21892 net.cpp:122] Setting up Scale8
I0305 17:56:51.102980 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:51.102986 21892 net.cpp:137] Memory required for data: 1912308000
I0305 17:56:51.102996 21892 layer_factory.hpp:77] Creating layer ReLU7
I0305 17:56:51.103008 21892 net.cpp:84] Creating Layer ReLU7
I0305 17:56:51.103014 21892 net.cpp:406] ReLU7 <- Convolution8
I0305 17:56:51.103024 21892 net.cpp:367] ReLU7 -> Convolution8 (in-place)
I0305 17:56:51.103032 21892 net.cpp:122] Setting up ReLU7
I0305 17:56:51.103040 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:51.103067 21892 net.cpp:137] Memory required for data: 1931575584
I0305 17:56:51.103075 21892 layer_factory.hpp:77] Creating layer Convolution9
I0305 17:56:51.103090 21892 net.cpp:84] Creating Layer Convolution9
I0305 17:56:51.103097 21892 net.cpp:406] Convolution9 <- Convolution8
I0305 17:56:51.103107 21892 net.cpp:380] Convolution9 -> Convolution9
I0305 17:56:51.103981 21892 net.cpp:122] Setting up Convolution9
I0305 17:56:51.104001 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:51.104009 21892 net.cpp:137] Memory required for data: 1950843168
I0305 17:56:51.104019 21892 layer_factory.hpp:77] Creating layer BatchNorm9
I0305 17:56:51.104032 21892 net.cpp:84] Creating Layer BatchNorm9
I0305 17:56:51.104043 21892 net.cpp:406] BatchNorm9 <- Convolution9
I0305 17:56:51.104060 21892 net.cpp:367] BatchNorm9 -> Convolution9 (in-place)
I0305 17:56:51.104404 21892 net.cpp:122] Setting up BatchNorm9
I0305 17:56:51.104423 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:51.104434 21892 net.cpp:137] Memory required for data: 1970110752
I0305 17:56:51.104454 21892 layer_factory.hpp:77] Creating layer Scale9
I0305 17:56:51.104482 21892 net.cpp:84] Creating Layer Scale9
I0305 17:56:51.104496 21892 net.cpp:406] Scale9 <- Convolution9
I0305 17:56:51.104508 21892 net.cpp:367] Scale9 -> Convolution9 (in-place)
I0305 17:56:51.104574 21892 layer_factory.hpp:77] Creating layer Scale9
I0305 17:56:51.104784 21892 net.cpp:122] Setting up Scale9
I0305 17:56:51.104797 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:51.104804 21892 net.cpp:137] Memory required for data: 1989378336
I0305 17:56:51.104820 21892 layer_factory.hpp:77] Creating layer Convolution10
I0305 17:56:51.104852 21892 net.cpp:84] Creating Layer Convolution10
I0305 17:56:51.104866 21892 net.cpp:406] Convolution10 <- Eltwise3_ReLU6_0_split_1
I0305 17:56:51.104884 21892 net.cpp:380] Convolution10 -> Convolution10
I0305 17:56:51.105263 21892 net.cpp:122] Setting up Convolution10
I0305 17:56:51.105281 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:51.105289 21892 net.cpp:137] Memory required for data: 2008645920
I0305 17:56:51.105314 21892 layer_factory.hpp:77] Creating layer BatchNorm10
I0305 17:56:51.105327 21892 net.cpp:84] Creating Layer BatchNorm10
I0305 17:56:51.105340 21892 net.cpp:406] BatchNorm10 <- Convolution10
I0305 17:56:51.105356 21892 net.cpp:367] BatchNorm10 -> Convolution10 (in-place)
I0305 17:56:51.105669 21892 net.cpp:122] Setting up BatchNorm10
I0305 17:56:51.105682 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:51.105690 21892 net.cpp:137] Memory required for data: 2027913504
I0305 17:56:51.105703 21892 layer_factory.hpp:77] Creating layer Scale10
I0305 17:56:51.105721 21892 net.cpp:84] Creating Layer Scale10
I0305 17:56:51.105733 21892 net.cpp:406] Scale10 <- Convolution10
I0305 17:56:51.105749 21892 net.cpp:367] Scale10 -> Convolution10 (in-place)
I0305 17:56:51.105810 21892 layer_factory.hpp:77] Creating layer Scale10
I0305 17:56:51.106006 21892 net.cpp:122] Setting up Scale10
I0305 17:56:51.106026 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:51.106036 21892 net.cpp:137] Memory required for data: 2047181088
I0305 17:56:51.106046 21892 layer_factory.hpp:77] Creating layer Eltwise4
I0305 17:56:51.106060 21892 net.cpp:84] Creating Layer Eltwise4
I0305 17:56:51.106067 21892 net.cpp:406] Eltwise4 <- Convolution9
I0305 17:56:51.106076 21892 net.cpp:406] Eltwise4 <- Convolution10
I0305 17:56:51.106086 21892 net.cpp:380] Eltwise4 -> Eltwise4
I0305 17:56:51.106142 21892 net.cpp:122] Setting up Eltwise4
I0305 17:56:51.106158 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:51.106169 21892 net.cpp:137] Memory required for data: 2066448672
I0305 17:56:51.106178 21892 layer_factory.hpp:77] Creating layer ReLU8
I0305 17:56:51.106189 21892 net.cpp:84] Creating Layer ReLU8
I0305 17:56:51.106196 21892 net.cpp:406] ReLU8 <- Eltwise4
I0305 17:56:51.106205 21892 net.cpp:367] ReLU8 -> Eltwise4 (in-place)
I0305 17:56:51.106252 21892 net.cpp:122] Setting up ReLU8
I0305 17:56:51.106268 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:51.106281 21892 net.cpp:137] Memory required for data: 2085716256
I0305 17:56:51.106292 21892 layer_factory.hpp:77] Creating layer Eltwise4_ReLU8_0_split
I0305 17:56:51.106307 21892 net.cpp:84] Creating Layer Eltwise4_ReLU8_0_split
I0305 17:56:51.106313 21892 net.cpp:406] Eltwise4_ReLU8_0_split <- Eltwise4
I0305 17:56:51.106323 21892 net.cpp:380] Eltwise4_ReLU8_0_split -> Eltwise4_ReLU8_0_split_0
I0305 17:56:51.106333 21892 net.cpp:380] Eltwise4_ReLU8_0_split -> Eltwise4_ReLU8_0_split_1
I0305 17:56:51.106396 21892 net.cpp:122] Setting up Eltwise4_ReLU8_0_split
I0305 17:56:51.106415 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:51.106428 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:51.106437 21892 net.cpp:137] Memory required for data: 2124251424
I0305 17:56:51.106444 21892 layer_factory.hpp:77] Creating layer Convolution11
I0305 17:56:51.106458 21892 net.cpp:84] Creating Layer Convolution11
I0305 17:56:51.106467 21892 net.cpp:406] Convolution11 <- Eltwise4_ReLU8_0_split_0
I0305 17:56:51.106477 21892 net.cpp:380] Convolution11 -> Convolution11
I0305 17:56:51.107237 21892 net.cpp:122] Setting up Convolution11
I0305 17:56:51.107254 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:51.107262 21892 net.cpp:137] Memory required for data: 2143519008
I0305 17:56:51.107272 21892 layer_factory.hpp:77] Creating layer BatchNorm11
I0305 17:56:51.107283 21892 net.cpp:84] Creating Layer BatchNorm11
I0305 17:56:51.107291 21892 net.cpp:406] BatchNorm11 <- Convolution11
I0305 17:56:51.107305 21892 net.cpp:367] BatchNorm11 -> Convolution11 (in-place)
I0305 17:56:51.107640 21892 net.cpp:122] Setting up BatchNorm11
I0305 17:56:51.107658 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:51.107666 21892 net.cpp:137] Memory required for data: 2162786592
I0305 17:56:51.107678 21892 layer_factory.hpp:77] Creating layer Scale11
I0305 17:56:51.107695 21892 net.cpp:84] Creating Layer Scale11
I0305 17:56:51.107702 21892 net.cpp:406] Scale11 <- Convolution11
I0305 17:56:51.107712 21892 net.cpp:367] Scale11 -> Convolution11 (in-place)
I0305 17:56:51.107795 21892 layer_factory.hpp:77] Creating layer Scale11
I0305 17:56:51.107976 21892 net.cpp:122] Setting up Scale11
I0305 17:56:51.107990 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:51.107997 21892 net.cpp:137] Memory required for data: 2182054176
I0305 17:56:51.108013 21892 layer_factory.hpp:77] Creating layer ReLU9
I0305 17:56:51.108029 21892 net.cpp:84] Creating Layer ReLU9
I0305 17:56:51.108042 21892 net.cpp:406] ReLU9 <- Convolution11
I0305 17:56:51.108057 21892 net.cpp:367] ReLU9 -> Convolution11 (in-place)
I0305 17:56:51.108072 21892 net.cpp:122] Setting up ReLU9
I0305 17:56:51.108086 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:51.108094 21892 net.cpp:137] Memory required for data: 2201321760
I0305 17:56:51.108101 21892 layer_factory.hpp:77] Creating layer Convolution12
I0305 17:56:51.108115 21892 net.cpp:84] Creating Layer Convolution12
I0305 17:56:51.108122 21892 net.cpp:406] Convolution12 <- Convolution11
I0305 17:56:51.108134 21892 net.cpp:380] Convolution12 -> Convolution12
I0305 17:56:51.108870 21892 net.cpp:122] Setting up Convolution12
I0305 17:56:51.108891 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:51.108898 21892 net.cpp:137] Memory required for data: 2220589344
I0305 17:56:51.108907 21892 layer_factory.hpp:77] Creating layer BatchNorm12
I0305 17:56:51.108919 21892 net.cpp:84] Creating Layer BatchNorm12
I0305 17:56:51.108927 21892 net.cpp:406] BatchNorm12 <- Convolution12
I0305 17:56:51.108937 21892 net.cpp:367] BatchNorm12 -> Convolution12 (in-place)
I0305 17:56:51.109184 21892 net.cpp:122] Setting up BatchNorm12
I0305 17:56:51.109192 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:51.109258 21892 net.cpp:137] Memory required for data: 2239856928
I0305 17:56:51.109289 21892 layer_factory.hpp:77] Creating layer Scale12
I0305 17:56:51.109357 21892 net.cpp:84] Creating Layer Scale12
I0305 17:56:51.109369 21892 net.cpp:406] Scale12 <- Convolution12
I0305 17:56:51.109383 21892 net.cpp:367] Scale12 -> Convolution12 (in-place)
I0305 17:56:51.109570 21892 layer_factory.hpp:77] Creating layer Scale12
I0305 17:56:51.109843 21892 net.cpp:122] Setting up Scale12
I0305 17:56:51.109856 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:51.109863 21892 net.cpp:137] Memory required for data: 2259124512
I0305 17:56:51.109876 21892 layer_factory.hpp:77] Creating layer Eltwise5
I0305 17:56:51.109890 21892 net.cpp:84] Creating Layer Eltwise5
I0305 17:56:51.109899 21892 net.cpp:406] Eltwise5 <- Convolution12
I0305 17:56:51.109910 21892 net.cpp:406] Eltwise5 <- Eltwise4_ReLU8_0_split_1
I0305 17:56:51.109920 21892 net.cpp:380] Eltwise5 -> Eltwise5
I0305 17:56:51.109964 21892 net.cpp:122] Setting up Eltwise5
I0305 17:56:51.109975 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:51.109982 21892 net.cpp:137] Memory required for data: 2278392096
I0305 17:56:51.109988 21892 layer_factory.hpp:77] Creating layer ReLU10
I0305 17:56:51.110002 21892 net.cpp:84] Creating Layer ReLU10
I0305 17:56:51.110010 21892 net.cpp:406] ReLU10 <- Eltwise5
I0305 17:56:51.110023 21892 net.cpp:367] ReLU10 -> Eltwise5 (in-place)
I0305 17:56:51.110034 21892 net.cpp:122] Setting up ReLU10
I0305 17:56:51.110044 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:51.110049 21892 net.cpp:137] Memory required for data: 2297659680
I0305 17:56:51.110056 21892 layer_factory.hpp:77] Creating layer Eltwise5_ReLU10_0_split
I0305 17:56:51.110069 21892 net.cpp:84] Creating Layer Eltwise5_ReLU10_0_split
I0305 17:56:51.110075 21892 net.cpp:406] Eltwise5_ReLU10_0_split <- Eltwise5
I0305 17:56:51.110086 21892 net.cpp:380] Eltwise5_ReLU10_0_split -> Eltwise5_ReLU10_0_split_0
I0305 17:56:51.110097 21892 net.cpp:380] Eltwise5_ReLU10_0_split -> Eltwise5_ReLU10_0_split_1
I0305 17:56:51.110163 21892 net.cpp:122] Setting up Eltwise5_ReLU10_0_split
I0305 17:56:51.110174 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:51.110183 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:51.110189 21892 net.cpp:137] Memory required for data: 2336194848
I0305 17:56:51.110196 21892 layer_factory.hpp:77] Creating layer Convolution13
I0305 17:56:51.110213 21892 net.cpp:84] Creating Layer Convolution13
I0305 17:56:51.110221 21892 net.cpp:406] Convolution13 <- Eltwise5_ReLU10_0_split_0
I0305 17:56:51.110232 21892 net.cpp:380] Convolution13 -> Convolution13
I0305 17:56:51.111495 21892 net.cpp:122] Setting up Convolution13
I0305 17:56:51.111534 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:51.111552 21892 net.cpp:137] Memory required for data: 2355462432
I0305 17:56:51.111575 21892 layer_factory.hpp:77] Creating layer BatchNorm13
I0305 17:56:51.111600 21892 net.cpp:84] Creating Layer BatchNorm13
I0305 17:56:51.111616 21892 net.cpp:406] BatchNorm13 <- Convolution13
I0305 17:56:51.111637 21892 net.cpp:367] BatchNorm13 -> Convolution13 (in-place)
I0305 17:56:51.112617 21892 net.cpp:122] Setting up BatchNorm13
I0305 17:56:51.112649 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:51.112664 21892 net.cpp:137] Memory required for data: 2374730016
I0305 17:56:51.112685 21892 layer_factory.hpp:77] Creating layer Scale13
I0305 17:56:51.112720 21892 net.cpp:84] Creating Layer Scale13
I0305 17:56:51.112736 21892 net.cpp:406] Scale13 <- Convolution13
I0305 17:56:51.112752 21892 net.cpp:367] Scale13 -> Convolution13 (in-place)
I0305 17:56:51.112833 21892 layer_factory.hpp:77] Creating layer Scale13
I0305 17:56:51.112994 21892 net.cpp:122] Setting up Scale13
I0305 17:56:51.113008 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:51.113015 21892 net.cpp:137] Memory required for data: 2393997600
I0305 17:56:51.113028 21892 layer_factory.hpp:77] Creating layer ReLU11
I0305 17:56:51.113039 21892 net.cpp:84] Creating Layer ReLU11
I0305 17:56:51.113047 21892 net.cpp:406] ReLU11 <- Convolution13
I0305 17:56:51.113057 21892 net.cpp:367] ReLU11 -> Convolution13 (in-place)
I0305 17:56:51.113099 21892 net.cpp:122] Setting up ReLU11
I0305 17:56:51.113108 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:51.113116 21892 net.cpp:137] Memory required for data: 2413265184
I0305 17:56:51.113123 21892 layer_factory.hpp:77] Creating layer Convolution14
I0305 17:56:51.113140 21892 net.cpp:84] Creating Layer Convolution14
I0305 17:56:51.113147 21892 net.cpp:406] Convolution14 <- Convolution13
I0305 17:56:51.113158 21892 net.cpp:380] Convolution14 -> Convolution14
I0305 17:56:51.113889 21892 net.cpp:122] Setting up Convolution14
I0305 17:56:51.113912 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:51.113920 21892 net.cpp:137] Memory required for data: 2432532768
I0305 17:56:51.113931 21892 layer_factory.hpp:77] Creating layer BatchNorm14
I0305 17:56:51.113945 21892 net.cpp:84] Creating Layer BatchNorm14
I0305 17:56:51.113953 21892 net.cpp:406] BatchNorm14 <- Convolution14
I0305 17:56:51.113963 21892 net.cpp:367] BatchNorm14 -> Convolution14 (in-place)
I0305 17:56:51.114207 21892 net.cpp:122] Setting up BatchNorm14
I0305 17:56:51.114218 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:51.114226 21892 net.cpp:137] Memory required for data: 2451800352
I0305 17:56:51.114238 21892 layer_factory.hpp:77] Creating layer Scale14
I0305 17:56:51.114253 21892 net.cpp:84] Creating Layer Scale14
I0305 17:56:51.114260 21892 net.cpp:406] Scale14 <- Convolution14
I0305 17:56:51.114270 21892 net.cpp:367] Scale14 -> Convolution14 (in-place)
I0305 17:56:51.114331 21892 layer_factory.hpp:77] Creating layer Scale14
I0305 17:56:51.114481 21892 net.cpp:122] Setting up Scale14
I0305 17:56:51.114495 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:51.114501 21892 net.cpp:137] Memory required for data: 2471067936
I0305 17:56:51.114511 21892 layer_factory.hpp:77] Creating layer Eltwise6
I0305 17:56:51.114524 21892 net.cpp:84] Creating Layer Eltwise6
I0305 17:56:51.114531 21892 net.cpp:406] Eltwise6 <- Convolution14
I0305 17:56:51.114542 21892 net.cpp:406] Eltwise6 <- Eltwise5_ReLU10_0_split_1
I0305 17:56:51.114552 21892 net.cpp:380] Eltwise6 -> Eltwise6
I0305 17:56:51.114586 21892 net.cpp:122] Setting up Eltwise6
I0305 17:56:51.114598 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:51.114604 21892 net.cpp:137] Memory required for data: 2490335520
I0305 17:56:51.114612 21892 layer_factory.hpp:77] Creating layer ReLU12
I0305 17:56:51.114629 21892 net.cpp:84] Creating Layer ReLU12
I0305 17:56:51.114639 21892 net.cpp:406] ReLU12 <- Eltwise6
I0305 17:56:51.114655 21892 net.cpp:367] ReLU12 -> Eltwise6 (in-place)
I0305 17:56:51.114670 21892 net.cpp:122] Setting up ReLU12
I0305 17:56:51.114681 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:51.114689 21892 net.cpp:137] Memory required for data: 2509603104
I0305 17:56:51.114697 21892 layer_factory.hpp:77] Creating layer Eltwise6_ReLU12_0_split
I0305 17:56:51.114708 21892 net.cpp:84] Creating Layer Eltwise6_ReLU12_0_split
I0305 17:56:51.114715 21892 net.cpp:406] Eltwise6_ReLU12_0_split <- Eltwise6
I0305 17:56:51.114725 21892 net.cpp:380] Eltwise6_ReLU12_0_split -> Eltwise6_ReLU12_0_split_0
I0305 17:56:51.114735 21892 net.cpp:380] Eltwise6_ReLU12_0_split -> Eltwise6_ReLU12_0_split_1
I0305 17:56:51.114781 21892 net.cpp:122] Setting up Eltwise6_ReLU12_0_split
I0305 17:56:51.114791 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:51.114800 21892 net.cpp:129] Top shape: 24 64 56 56 (4816896)
I0305 17:56:51.114806 21892 net.cpp:137] Memory required for data: 2548138272
I0305 17:56:51.114814 21892 layer_factory.hpp:77] Creating layer Convolution15
I0305 17:56:51.114830 21892 net.cpp:84] Creating Layer Convolution15
I0305 17:56:51.114840 21892 net.cpp:406] Convolution15 <- Eltwise6_ReLU12_0_split_0
I0305 17:56:51.114850 21892 net.cpp:380] Convolution15 -> Convolution15
I0305 17:56:51.115669 21892 net.cpp:122] Setting up Convolution15
I0305 17:56:51.115684 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:51.115694 21892 net.cpp:137] Memory required for data: 2557772064
I0305 17:56:51.115741 21892 layer_factory.hpp:77] Creating layer BatchNorm15
I0305 17:56:51.115754 21892 net.cpp:84] Creating Layer BatchNorm15
I0305 17:56:51.115762 21892 net.cpp:406] BatchNorm15 <- Convolution15
I0305 17:56:51.115774 21892 net.cpp:367] BatchNorm15 -> Convolution15 (in-place)
I0305 17:56:51.116026 21892 net.cpp:122] Setting up BatchNorm15
I0305 17:56:51.116040 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:51.116048 21892 net.cpp:137] Memory required for data: 2567405856
I0305 17:56:51.116061 21892 layer_factory.hpp:77] Creating layer Scale15
I0305 17:56:51.116073 21892 net.cpp:84] Creating Layer Scale15
I0305 17:56:51.116081 21892 net.cpp:406] Scale15 <- Convolution15
I0305 17:56:51.116091 21892 net.cpp:367] Scale15 -> Convolution15 (in-place)
I0305 17:56:51.116145 21892 layer_factory.hpp:77] Creating layer Scale15
I0305 17:56:51.116281 21892 net.cpp:122] Setting up Scale15
I0305 17:56:51.116293 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:51.116300 21892 net.cpp:137] Memory required for data: 2577039648
I0305 17:56:51.116310 21892 layer_factory.hpp:77] Creating layer ReLU13
I0305 17:56:51.116322 21892 net.cpp:84] Creating Layer ReLU13
I0305 17:56:51.116328 21892 net.cpp:406] ReLU13 <- Convolution15
I0305 17:56:51.116338 21892 net.cpp:367] ReLU13 -> Convolution15 (in-place)
I0305 17:56:51.116348 21892 net.cpp:122] Setting up ReLU13
I0305 17:56:51.116356 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:51.116364 21892 net.cpp:137] Memory required for data: 2586673440
I0305 17:56:51.116370 21892 layer_factory.hpp:77] Creating layer Convolution16
I0305 17:56:51.116384 21892 net.cpp:84] Creating Layer Convolution16
I0305 17:56:51.116399 21892 net.cpp:406] Convolution16 <- Convolution15
I0305 17:56:51.116410 21892 net.cpp:380] Convolution16 -> Convolution16
I0305 17:56:51.117863 21892 net.cpp:122] Setting up Convolution16
I0305 17:56:51.117884 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:51.117893 21892 net.cpp:137] Memory required for data: 2596307232
I0305 17:56:51.117903 21892 layer_factory.hpp:77] Creating layer BatchNorm16
I0305 17:56:51.117916 21892 net.cpp:84] Creating Layer BatchNorm16
I0305 17:56:51.117924 21892 net.cpp:406] BatchNorm16 <- Convolution16
I0305 17:56:51.117935 21892 net.cpp:367] BatchNorm16 -> Convolution16 (in-place)
I0305 17:56:51.118198 21892 net.cpp:122] Setting up BatchNorm16
I0305 17:56:51.118224 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:51.118232 21892 net.cpp:137] Memory required for data: 2605941024
I0305 17:56:51.118244 21892 layer_factory.hpp:77] Creating layer Scale16
I0305 17:56:51.118257 21892 net.cpp:84] Creating Layer Scale16
I0305 17:56:51.118268 21892 net.cpp:406] Scale16 <- Convolution16
I0305 17:56:51.118278 21892 net.cpp:367] Scale16 -> Convolution16 (in-place)
I0305 17:56:51.118711 21892 layer_factory.hpp:77] Creating layer Scale16
I0305 17:56:51.118997 21892 net.cpp:122] Setting up Scale16
I0305 17:56:51.119019 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:51.119027 21892 net.cpp:137] Memory required for data: 2615574816
I0305 17:56:51.119038 21892 layer_factory.hpp:77] Creating layer Convolution17
I0305 17:56:51.119053 21892 net.cpp:84] Creating Layer Convolution17
I0305 17:56:51.119063 21892 net.cpp:406] Convolution17 <- Eltwise6_ReLU12_0_split_1
I0305 17:56:51.119079 21892 net.cpp:380] Convolution17 -> Convolution17
I0305 17:56:51.119608 21892 net.cpp:122] Setting up Convolution17
I0305 17:56:51.119633 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:51.119647 21892 net.cpp:137] Memory required for data: 2625208608
I0305 17:56:51.119663 21892 layer_factory.hpp:77] Creating layer BatchNorm17
I0305 17:56:51.119684 21892 net.cpp:84] Creating Layer BatchNorm17
I0305 17:56:51.119696 21892 net.cpp:406] BatchNorm17 <- Convolution17
I0305 17:56:51.119707 21892 net.cpp:367] BatchNorm17 -> Convolution17 (in-place)
I0305 17:56:51.120071 21892 net.cpp:122] Setting up BatchNorm17
I0305 17:56:51.120087 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:51.120128 21892 net.cpp:137] Memory required for data: 2634842400
I0305 17:56:51.120151 21892 layer_factory.hpp:77] Creating layer Scale17
I0305 17:56:51.120168 21892 net.cpp:84] Creating Layer Scale17
I0305 17:56:51.120182 21892 net.cpp:406] Scale17 <- Convolution17
I0305 17:56:51.120198 21892 net.cpp:367] Scale17 -> Convolution17 (in-place)
I0305 17:56:51.120280 21892 layer_factory.hpp:77] Creating layer Scale17
I0305 17:56:51.120462 21892 net.cpp:122] Setting up Scale17
I0305 17:56:51.120486 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:51.120499 21892 net.cpp:137] Memory required for data: 2644476192
I0305 17:56:51.120515 21892 layer_factory.hpp:77] Creating layer Eltwise7
I0305 17:56:51.120535 21892 net.cpp:84] Creating Layer Eltwise7
I0305 17:56:51.120548 21892 net.cpp:406] Eltwise7 <- Convolution16
I0305 17:56:51.120560 21892 net.cpp:406] Eltwise7 <- Convolution17
I0305 17:56:51.120577 21892 net.cpp:380] Eltwise7 -> Eltwise7
I0305 17:56:51.120625 21892 net.cpp:122] Setting up Eltwise7
I0305 17:56:51.120642 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:51.120653 21892 net.cpp:137] Memory required for data: 2654109984
I0305 17:56:51.120666 21892 layer_factory.hpp:77] Creating layer ReLU14
I0305 17:56:51.120682 21892 net.cpp:84] Creating Layer ReLU14
I0305 17:56:51.120693 21892 net.cpp:406] ReLU14 <- Eltwise7
I0305 17:56:51.120703 21892 net.cpp:367] ReLU14 -> Eltwise7 (in-place)
I0305 17:56:51.120712 21892 net.cpp:122] Setting up ReLU14
I0305 17:56:51.120721 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:51.120728 21892 net.cpp:137] Memory required for data: 2663743776
I0305 17:56:51.120735 21892 layer_factory.hpp:77] Creating layer Eltwise7_ReLU14_0_split
I0305 17:56:51.120764 21892 net.cpp:84] Creating Layer Eltwise7_ReLU14_0_split
I0305 17:56:51.120777 21892 net.cpp:406] Eltwise7_ReLU14_0_split <- Eltwise7
I0305 17:56:51.120793 21892 net.cpp:380] Eltwise7_ReLU14_0_split -> Eltwise7_ReLU14_0_split_0
I0305 17:56:51.120810 21892 net.cpp:380] Eltwise7_ReLU14_0_split -> Eltwise7_ReLU14_0_split_1
I0305 17:56:51.120882 21892 net.cpp:122] Setting up Eltwise7_ReLU14_0_split
I0305 17:56:51.120899 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:51.120913 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:51.120925 21892 net.cpp:137] Memory required for data: 2683011360
I0305 17:56:51.120937 21892 layer_factory.hpp:77] Creating layer Convolution18
I0305 17:56:51.120959 21892 net.cpp:84] Creating Layer Convolution18
I0305 17:56:51.120972 21892 net.cpp:406] Convolution18 <- Eltwise7_ReLU14_0_split_0
I0305 17:56:51.120990 21892 net.cpp:380] Convolution18 -> Convolution18
I0305 17:56:51.125695 21892 net.cpp:122] Setting up Convolution18
I0305 17:56:51.125769 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:51.125779 21892 net.cpp:137] Memory required for data: 2692645152
I0305 17:56:51.125795 21892 layer_factory.hpp:77] Creating layer BatchNorm18
I0305 17:56:51.125820 21892 net.cpp:84] Creating Layer BatchNorm18
I0305 17:56:51.125838 21892 net.cpp:406] BatchNorm18 <- Convolution18
I0305 17:56:51.125859 21892 net.cpp:367] BatchNorm18 -> Convolution18 (in-place)
I0305 17:56:51.126237 21892 net.cpp:122] Setting up BatchNorm18
I0305 17:56:51.126266 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:51.126279 21892 net.cpp:137] Memory required for data: 2702278944
I0305 17:56:51.126302 21892 layer_factory.hpp:77] Creating layer Scale18
I0305 17:56:51.126325 21892 net.cpp:84] Creating Layer Scale18
I0305 17:56:51.126339 21892 net.cpp:406] Scale18 <- Convolution18
I0305 17:56:51.126355 21892 net.cpp:367] Scale18 -> Convolution18 (in-place)
I0305 17:56:51.126462 21892 layer_factory.hpp:77] Creating layer Scale18
I0305 17:56:51.126659 21892 net.cpp:122] Setting up Scale18
I0305 17:56:51.126694 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:51.126708 21892 net.cpp:137] Memory required for data: 2711912736
I0305 17:56:51.126724 21892 layer_factory.hpp:77] Creating layer ReLU15
I0305 17:56:51.126783 21892 net.cpp:84] Creating Layer ReLU15
I0305 17:56:51.126797 21892 net.cpp:406] ReLU15 <- Convolution18
I0305 17:56:51.126808 21892 net.cpp:367] ReLU15 -> Convolution18 (in-place)
I0305 17:56:51.126819 21892 net.cpp:122] Setting up ReLU15
I0305 17:56:51.126828 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:51.126835 21892 net.cpp:137] Memory required for data: 2721546528
I0305 17:56:51.126842 21892 layer_factory.hpp:77] Creating layer Convolution19
I0305 17:56:51.126863 21892 net.cpp:84] Creating Layer Convolution19
I0305 17:56:51.126889 21892 net.cpp:406] Convolution19 <- Convolution18
I0305 17:56:51.126909 21892 net.cpp:380] Convolution19 -> Convolution19
I0305 17:56:51.128949 21892 net.cpp:122] Setting up Convolution19
I0305 17:56:51.129006 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:51.129019 21892 net.cpp:137] Memory required for data: 2731180320
I0305 17:56:51.129040 21892 layer_factory.hpp:77] Creating layer BatchNorm19
I0305 17:56:51.129061 21892 net.cpp:84] Creating Layer BatchNorm19
I0305 17:56:51.129079 21892 net.cpp:406] BatchNorm19 <- Convolution19
I0305 17:56:51.129096 21892 net.cpp:367] BatchNorm19 -> Convolution19 (in-place)
I0305 17:56:51.129428 21892 net.cpp:122] Setting up BatchNorm19
I0305 17:56:51.129452 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:51.129462 21892 net.cpp:137] Memory required for data: 2740814112
I0305 17:56:51.129503 21892 layer_factory.hpp:77] Creating layer Scale19
I0305 17:56:51.129524 21892 net.cpp:84] Creating Layer Scale19
I0305 17:56:51.129537 21892 net.cpp:406] Scale19 <- Convolution19
I0305 17:56:51.129552 21892 net.cpp:367] Scale19 -> Convolution19 (in-place)
I0305 17:56:51.129644 21892 layer_factory.hpp:77] Creating layer Scale19
I0305 17:56:51.129850 21892 net.cpp:122] Setting up Scale19
I0305 17:56:51.129873 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:51.129887 21892 net.cpp:137] Memory required for data: 2750447904
I0305 17:56:51.129905 21892 layer_factory.hpp:77] Creating layer Eltwise8
I0305 17:56:51.129926 21892 net.cpp:84] Creating Layer Eltwise8
I0305 17:56:51.129938 21892 net.cpp:406] Eltwise8 <- Convolution19
I0305 17:56:51.129954 21892 net.cpp:406] Eltwise8 <- Eltwise7_ReLU14_0_split_1
I0305 17:56:51.129971 21892 net.cpp:380] Eltwise8 -> Eltwise8
I0305 17:56:51.130023 21892 net.cpp:122] Setting up Eltwise8
I0305 17:56:51.130040 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:51.130053 21892 net.cpp:137] Memory required for data: 2760081696
I0305 17:56:51.130064 21892 layer_factory.hpp:77] Creating layer ReLU16
I0305 17:56:51.130081 21892 net.cpp:84] Creating Layer ReLU16
I0305 17:56:51.130095 21892 net.cpp:406] ReLU16 <- Eltwise8
I0305 17:56:51.130123 21892 net.cpp:367] ReLU16 -> Eltwise8 (in-place)
I0305 17:56:51.130163 21892 net.cpp:122] Setting up ReLU16
I0305 17:56:51.130172 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:51.130180 21892 net.cpp:137] Memory required for data: 2769715488
I0305 17:56:51.130187 21892 layer_factory.hpp:77] Creating layer Eltwise8_ReLU16_0_split
I0305 17:56:51.130203 21892 net.cpp:84] Creating Layer Eltwise8_ReLU16_0_split
I0305 17:56:51.130215 21892 net.cpp:406] Eltwise8_ReLU16_0_split <- Eltwise8
I0305 17:56:51.130231 21892 net.cpp:380] Eltwise8_ReLU16_0_split -> Eltwise8_ReLU16_0_split_0
I0305 17:56:51.130249 21892 net.cpp:380] Eltwise8_ReLU16_0_split -> Eltwise8_ReLU16_0_split_1
I0305 17:56:51.130322 21892 net.cpp:122] Setting up Eltwise8_ReLU16_0_split
I0305 17:56:51.130339 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:51.130354 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:51.130367 21892 net.cpp:137] Memory required for data: 2788983072
I0305 17:56:51.130378 21892 layer_factory.hpp:77] Creating layer Convolution20
I0305 17:56:51.130403 21892 net.cpp:84] Creating Layer Convolution20
I0305 17:56:51.130416 21892 net.cpp:406] Convolution20 <- Eltwise8_ReLU16_0_split_0
I0305 17:56:51.130435 21892 net.cpp:380] Convolution20 -> Convolution20
I0305 17:56:51.132457 21892 net.cpp:122] Setting up Convolution20
I0305 17:56:51.132488 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:51.132501 21892 net.cpp:137] Memory required for data: 2798616864
I0305 17:56:51.132519 21892 layer_factory.hpp:77] Creating layer BatchNorm20
I0305 17:56:51.132552 21892 net.cpp:84] Creating Layer BatchNorm20
I0305 17:56:51.132580 21892 net.cpp:406] BatchNorm20 <- Convolution20
I0305 17:56:51.132607 21892 net.cpp:367] BatchNorm20 -> Convolution20 (in-place)
I0305 17:56:51.132874 21892 net.cpp:122] Setting up BatchNorm20
I0305 17:56:51.132886 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:51.132890 21892 net.cpp:137] Memory required for data: 2808250656
I0305 17:56:51.132916 21892 layer_factory.hpp:77] Creating layer Scale20
I0305 17:56:51.132927 21892 net.cpp:84] Creating Layer Scale20
I0305 17:56:51.132931 21892 net.cpp:406] Scale20 <- Convolution20
I0305 17:56:51.132939 21892 net.cpp:367] Scale20 -> Convolution20 (in-place)
I0305 17:56:51.132992 21892 layer_factory.hpp:77] Creating layer Scale20
I0305 17:56:51.133124 21892 net.cpp:122] Setting up Scale20
I0305 17:56:51.133132 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:51.133136 21892 net.cpp:137] Memory required for data: 2817884448
I0305 17:56:51.133144 21892 layer_factory.hpp:77] Creating layer ReLU17
I0305 17:56:51.133152 21892 net.cpp:84] Creating Layer ReLU17
I0305 17:56:51.133157 21892 net.cpp:406] ReLU17 <- Convolution20
I0305 17:56:51.133163 21892 net.cpp:367] ReLU17 -> Convolution20 (in-place)
I0305 17:56:51.133169 21892 net.cpp:122] Setting up ReLU17
I0305 17:56:51.133175 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:51.133179 21892 net.cpp:137] Memory required for data: 2827518240
I0305 17:56:51.133183 21892 layer_factory.hpp:77] Creating layer Convolution21
I0305 17:56:51.133195 21892 net.cpp:84] Creating Layer Convolution21
I0305 17:56:51.133199 21892 net.cpp:406] Convolution21 <- Convolution20
I0305 17:56:51.133208 21892 net.cpp:380] Convolution21 -> Convolution21
I0305 17:56:51.137925 21892 net.cpp:122] Setting up Convolution21
I0305 17:56:51.138003 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:51.138020 21892 net.cpp:137] Memory required for data: 2837152032
I0305 17:56:51.138042 21892 layer_factory.hpp:77] Creating layer BatchNorm21
I0305 17:56:51.138069 21892 net.cpp:84] Creating Layer BatchNorm21
I0305 17:56:51.138089 21892 net.cpp:406] BatchNorm21 <- Convolution21
I0305 17:56:51.138108 21892 net.cpp:367] BatchNorm21 -> Convolution21 (in-place)
I0305 17:56:51.138468 21892 net.cpp:122] Setting up BatchNorm21
I0305 17:56:51.138491 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:51.138504 21892 net.cpp:137] Memory required for data: 2846785824
I0305 17:56:51.138525 21892 layer_factory.hpp:77] Creating layer Scale21
I0305 17:56:51.138545 21892 net.cpp:84] Creating Layer Scale21
I0305 17:56:51.138557 21892 net.cpp:406] Scale21 <- Convolution21
I0305 17:56:51.138571 21892 net.cpp:367] Scale21 -> Convolution21 (in-place)
I0305 17:56:51.138675 21892 layer_factory.hpp:77] Creating layer Scale21
I0305 17:56:51.138891 21892 net.cpp:122] Setting up Scale21
I0305 17:56:51.138911 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:51.138921 21892 net.cpp:137] Memory required for data: 2856419616
I0305 17:56:51.138936 21892 layer_factory.hpp:77] Creating layer Eltwise9
I0305 17:56:51.138955 21892 net.cpp:84] Creating Layer Eltwise9
I0305 17:56:51.138968 21892 net.cpp:406] Eltwise9 <- Convolution21
I0305 17:56:51.138983 21892 net.cpp:406] Eltwise9 <- Eltwise8_ReLU16_0_split_1
I0305 17:56:51.138998 21892 net.cpp:380] Eltwise9 -> Eltwise9
I0305 17:56:51.139046 21892 net.cpp:122] Setting up Eltwise9
I0305 17:56:51.139062 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:51.139073 21892 net.cpp:137] Memory required for data: 2866053408
I0305 17:56:51.139084 21892 layer_factory.hpp:77] Creating layer ReLU18
I0305 17:56:51.139101 21892 net.cpp:84] Creating Layer ReLU18
I0305 17:56:51.139163 21892 net.cpp:406] ReLU18 <- Eltwise9
I0305 17:56:51.139180 21892 net.cpp:367] ReLU18 -> Eltwise9 (in-place)
I0305 17:56:51.139196 21892 net.cpp:122] Setting up ReLU18
I0305 17:56:51.139209 21892 net.cpp:129] Top shape: 24 128 28 28 (2408448)
I0305 17:56:51.139225 21892 net.cpp:137] Memory required for data: 2875687200
I0305 17:56:51.139235 21892 layer_factory.hpp:77] Creating layer Deconvolution1
I0305 17:56:51.139256 21892 net.cpp:84] Creating Layer Deconvolution1
I0305 17:56:51.139268 21892 net.cpp:406] Deconvolution1 <- Eltwise9
I0305 17:56:51.139284 21892 net.cpp:380] Deconvolution1 -> Deconvolution1
I0305 17:56:51.140508 21892 net.cpp:122] Setting up Deconvolution1
I0305 17:56:51.140548 21892 net.cpp:129] Top shape: 24 128 56 56 (9633792)
I0305 17:56:51.140561 21892 net.cpp:137] Memory required for data: 2914222368
I0305 17:56:51.140578 21892 layer_factory.hpp:77] Creating layer BatchNorm22
I0305 17:56:51.140596 21892 net.cpp:84] Creating Layer BatchNorm22
I0305 17:56:51.140610 21892 net.cpp:406] BatchNorm22 <- Deconvolution1
I0305 17:56:51.140625 21892 net.cpp:367] BatchNorm22 -> Deconvolution1 (in-place)
I0305 17:56:51.140956 21892 net.cpp:122] Setting up BatchNorm22
I0305 17:56:51.140975 21892 net.cpp:129] Top shape: 24 128 56 56 (9633792)
I0305 17:56:51.140986 21892 net.cpp:137] Memory required for data: 2952757536
I0305 17:56:51.141005 21892 layer_factory.hpp:77] Creating layer Scale22
I0305 17:56:51.141024 21892 net.cpp:84] Creating Layer Scale22
I0305 17:56:51.141037 21892 net.cpp:406] Scale22 <- Deconvolution1
I0305 17:56:51.141052 21892 net.cpp:367] Scale22 -> Deconvolution1 (in-place)
I0305 17:56:51.141132 21892 layer_factory.hpp:77] Creating layer Scale22
I0305 17:56:51.141345 21892 net.cpp:122] Setting up Scale22
I0305 17:56:51.141362 21892 net.cpp:129] Top shape: 24 128 56 56 (9633792)
I0305 17:56:51.141373 21892 net.cpp:137] Memory required for data: 2991292704
I0305 17:56:51.141388 21892 layer_factory.hpp:77] Creating layer ReLU19
I0305 17:56:51.141405 21892 net.cpp:84] Creating Layer ReLU19
I0305 17:56:51.141417 21892 net.cpp:406] ReLU19 <- Deconvolution1
I0305 17:56:51.141430 21892 net.cpp:367] ReLU19 -> Deconvolution1 (in-place)
I0305 17:56:51.141444 21892 net.cpp:122] Setting up ReLU19
I0305 17:56:51.141458 21892 net.cpp:129] Top shape: 24 128 56 56 (9633792)
I0305 17:56:51.141469 21892 net.cpp:137] Memory required for data: 3029827872
I0305 17:56:51.141480 21892 layer_factory.hpp:77] Creating layer Deconvolution2
I0305 17:56:51.141499 21892 net.cpp:84] Creating Layer Deconvolution2
I0305 17:56:51.141511 21892 net.cpp:406] Deconvolution2 <- Deconvolution1
I0305 17:56:51.141527 21892 net.cpp:380] Deconvolution2 -> Deconvolution2
I0305 17:56:51.142520 21892 net.cpp:122] Setting up Deconvolution2
I0305 17:56:51.142557 21892 net.cpp:129] Top shape: 24 64 112 112 (19267584)
I0305 17:56:51.142576 21892 net.cpp:137] Memory required for data: 3106898208
I0305 17:56:51.142616 21892 layer_factory.hpp:77] Creating layer BatchNorm23
I0305 17:56:51.142643 21892 net.cpp:84] Creating Layer BatchNorm23
I0305 17:56:51.142666 21892 net.cpp:406] BatchNorm23 <- Deconvolution2
I0305 17:56:51.142690 21892 net.cpp:367] BatchNorm23 -> Deconvolution2 (in-place)
I0305 17:56:51.143265 21892 net.cpp:122] Setting up BatchNorm23
I0305 17:56:51.143304 21892 net.cpp:129] Top shape: 24 64 112 112 (19267584)
I0305 17:56:51.143324 21892 net.cpp:137] Memory required for data: 3183968544
I0305 17:56:51.143354 21892 layer_factory.hpp:77] Creating layer Scale23
I0305 17:56:51.143383 21892 net.cpp:84] Creating Layer Scale23
I0305 17:56:51.143404 21892 net.cpp:406] Scale23 <- Deconvolution2
I0305 17:56:51.143427 21892 net.cpp:367] Scale23 -> Deconvolution2 (in-place)
I0305 17:56:51.143548 21892 layer_factory.hpp:77] Creating layer Scale23
I0305 17:56:51.143839 21892 net.cpp:122] Setting up Scale23
I0305 17:56:51.143893 21892 net.cpp:129] Top shape: 24 64 112 112 (19267584)
I0305 17:56:51.143937 21892 net.cpp:137] Memory required for data: 3261038880
I0305 17:56:51.143986 21892 layer_factory.hpp:77] Creating layer ReLU20
I0305 17:56:51.144078 21892 net.cpp:84] Creating Layer ReLU20
I0305 17:56:51.144124 21892 net.cpp:406] ReLU20 <- Deconvolution2
I0305 17:56:51.144170 21892 net.cpp:367] ReLU20 -> Deconvolution2 (in-place)
I0305 17:56:51.144217 21892 net.cpp:122] Setting up ReLU20
I0305 17:56:51.144264 21892 net.cpp:129] Top shape: 24 64 112 112 (19267584)
I0305 17:56:51.144279 21892 net.cpp:137] Memory required for data: 3338109216
I0305 17:56:51.144318 21892 layer_factory.hpp:77] Creating layer Deconvolution3
I0305 17:56:51.144340 21892 net.cpp:84] Creating Layer Deconvolution3
I0305 17:56:51.144382 21892 net.cpp:406] Deconvolution3 <- Deconvolution2
I0305 17:56:51.144445 21892 net.cpp:380] Deconvolution3 -> Deconvolution3
I0305 17:56:51.145114 21892 net.cpp:122] Setting up Deconvolution3
I0305 17:56:51.145175 21892 net.cpp:129] Top shape: 24 32 224 224 (38535168)
I0305 17:56:51.145218 21892 net.cpp:137] Memory required for data: 3492249888
I0305 17:56:51.145269 21892 layer_factory.hpp:77] Creating layer BatchNorm24
I0305 17:56:51.145320 21892 net.cpp:84] Creating Layer BatchNorm24
I0305 17:56:51.145365 21892 net.cpp:406] BatchNorm24 <- Deconvolution3
I0305 17:56:51.145385 21892 net.cpp:367] BatchNorm24 -> Deconvolution3 (in-place)
I0305 17:56:51.148176 21892 net.cpp:122] Setting up BatchNorm24
I0305 17:56:51.148231 21892 net.cpp:129] Top shape: 24 32 224 224 (38535168)
I0305 17:56:51.148246 21892 net.cpp:137] Memory required for data: 3646390560
I0305 17:56:51.148273 21892 layer_factory.hpp:77] Creating layer Scale24
I0305 17:56:51.148298 21892 net.cpp:84] Creating Layer Scale24
I0305 17:56:51.148313 21892 net.cpp:406] Scale24 <- Deconvolution3
I0305 17:56:51.148332 21892 net.cpp:367] Scale24 -> Deconvolution3 (in-place)
I0305 17:56:51.148437 21892 layer_factory.hpp:77] Creating layer Scale24
I0305 17:56:51.148799 21892 net.cpp:122] Setting up Scale24
I0305 17:56:51.148821 21892 net.cpp:129] Top shape: 24 32 224 224 (38535168)
I0305 17:56:51.148834 21892 net.cpp:137] Memory required for data: 3800531232
I0305 17:56:51.148850 21892 layer_factory.hpp:77] Creating layer ReLU21
I0305 17:56:51.148867 21892 net.cpp:84] Creating Layer ReLU21
I0305 17:56:51.148878 21892 net.cpp:406] ReLU21 <- Deconvolution3
I0305 17:56:51.148905 21892 net.cpp:367] ReLU21 -> Deconvolution3 (in-place)
I0305 17:56:51.148919 21892 net.cpp:122] Setting up ReLU21
I0305 17:56:51.148933 21892 net.cpp:129] Top shape: 24 32 224 224 (38535168)
I0305 17:56:51.148946 21892 net.cpp:137] Memory required for data: 3954671904
I0305 17:56:51.148957 21892 layer_factory.hpp:77] Creating layer Convolution22
I0305 17:56:51.148979 21892 net.cpp:84] Creating Layer Convolution22
I0305 17:56:51.148991 21892 net.cpp:406] Convolution22 <- Deconvolution3
I0305 17:56:51.149009 21892 net.cpp:380] Convolution22 -> Convolution22
I0305 17:56:51.149509 21892 net.cpp:122] Setting up Convolution22
I0305 17:56:51.149531 21892 net.cpp:129] Top shape: 24 1 224 224 (1204224)
I0305 17:56:51.149544 21892 net.cpp:137] Memory required for data: 3959488800
I0305 17:56:51.149559 21892 layer_factory.hpp:77] Creating layer ReLU22
I0305 17:56:51.149574 21892 net.cpp:84] Creating Layer ReLU22
I0305 17:56:51.149585 21892 net.cpp:406] ReLU22 <- Convolution22
I0305 17:56:51.149600 21892 net.cpp:367] ReLU22 -> Convolution22 (in-place)
I0305 17:56:51.149615 21892 net.cpp:122] Setting up ReLU22
I0305 17:56:51.149628 21892 net.cpp:129] Top shape: 24 1 224 224 (1204224)
I0305 17:56:51.149639 21892 net.cpp:137] Memory required for data: 3964305696
I0305 17:56:51.149652 21892 layer_factory.hpp:77] Creating layer Loss
I0305 17:56:51.149669 21892 net.cpp:84] Creating Layer Loss
I0305 17:56:51.149682 21892 net.cpp:406] Loss <- Convolution22
I0305 17:56:51.149694 21892 net.cpp:406] Loss <- Data5
I0305 17:56:51.149709 21892 net.cpp:380] Loss -> Loss
I0305 17:56:51.149806 21892 net.cpp:122] Setting up Loss
I0305 17:56:51.149837 21892 net.cpp:129] Top shape: (1)
I0305 17:56:51.149855 21892 net.cpp:132]     with loss weight 1
I0305 17:56:51.149895 21892 net.cpp:137] Memory required for data: 3964305700
I0305 17:56:51.149914 21892 net.cpp:198] Loss needs backward computation.
I0305 17:56:51.149977 21892 net.cpp:198] ReLU22 needs backward computation.
I0305 17:56:51.149996 21892 net.cpp:198] Convolution22 needs backward computation.
I0305 17:56:51.150015 21892 net.cpp:198] ReLU21 needs backward computation.
I0305 17:56:51.150033 21892 net.cpp:198] Scale24 needs backward computation.
I0305 17:56:51.150049 21892 net.cpp:198] BatchNorm24 needs backward computation.
I0305 17:56:51.150068 21892 net.cpp:198] Deconvolution3 needs backward computation.
I0305 17:56:51.150086 21892 net.cpp:198] ReLU20 needs backward computation.
I0305 17:56:51.150105 21892 net.cpp:198] Scale23 needs backward computation.
I0305 17:56:51.150122 21892 net.cpp:198] BatchNorm23 needs backward computation.
I0305 17:56:51.150140 21892 net.cpp:198] Deconvolution2 needs backward computation.
I0305 17:56:51.150158 21892 net.cpp:198] ReLU19 needs backward computation.
I0305 17:56:51.150177 21892 net.cpp:198] Scale22 needs backward computation.
I0305 17:56:51.150195 21892 net.cpp:198] BatchNorm22 needs backward computation.
I0305 17:56:51.150213 21892 net.cpp:198] Deconvolution1 needs backward computation.
I0305 17:56:51.150231 21892 net.cpp:198] ReLU18 needs backward computation.
I0305 17:56:51.150249 21892 net.cpp:198] Eltwise9 needs backward computation.
I0305 17:56:51.150267 21892 net.cpp:198] Scale21 needs backward computation.
I0305 17:56:51.150285 21892 net.cpp:198] BatchNorm21 needs backward computation.
I0305 17:56:51.150305 21892 net.cpp:198] Convolution21 needs backward computation.
I0305 17:56:51.150321 21892 net.cpp:198] ReLU17 needs backward computation.
I0305 17:56:51.150337 21892 net.cpp:198] Scale20 needs backward computation.
I0305 17:56:51.150355 21892 net.cpp:198] BatchNorm20 needs backward computation.
I0305 17:56:51.150374 21892 net.cpp:198] Convolution20 needs backward computation.
I0305 17:56:51.150393 21892 net.cpp:198] Eltwise8_ReLU16_0_split needs backward computation.
I0305 17:56:51.150411 21892 net.cpp:198] ReLU16 needs backward computation.
I0305 17:56:51.150429 21892 net.cpp:198] Eltwise8 needs backward computation.
I0305 17:56:51.150449 21892 net.cpp:198] Scale19 needs backward computation.
I0305 17:56:51.150467 21892 net.cpp:198] BatchNorm19 needs backward computation.
I0305 17:56:51.150485 21892 net.cpp:198] Convolution19 needs backward computation.
I0305 17:56:51.150501 21892 net.cpp:198] ReLU15 needs backward computation.
I0305 17:56:51.150519 21892 net.cpp:198] Scale18 needs backward computation.
I0305 17:56:51.150537 21892 net.cpp:198] BatchNorm18 needs backward computation.
I0305 17:56:51.150555 21892 net.cpp:198] Convolution18 needs backward computation.
I0305 17:56:51.150573 21892 net.cpp:198] Eltwise7_ReLU14_0_split needs backward computation.
I0305 17:56:51.150593 21892 net.cpp:198] ReLU14 needs backward computation.
I0305 17:56:51.150614 21892 net.cpp:198] Eltwise7 needs backward computation.
I0305 17:56:51.150634 21892 net.cpp:198] Scale17 needs backward computation.
I0305 17:56:51.150651 21892 net.cpp:198] BatchNorm17 needs backward computation.
I0305 17:56:51.150669 21892 net.cpp:198] Convolution17 needs backward computation.
I0305 17:56:51.150688 21892 net.cpp:198] Scale16 needs backward computation.
I0305 17:56:51.150708 21892 net.cpp:198] BatchNorm16 needs backward computation.
I0305 17:56:51.150727 21892 net.cpp:198] Convolution16 needs backward computation.
I0305 17:56:51.150744 21892 net.cpp:198] ReLU13 needs backward computation.
I0305 17:56:51.150763 21892 net.cpp:198] Scale15 needs backward computation.
I0305 17:56:51.150781 21892 net.cpp:198] BatchNorm15 needs backward computation.
I0305 17:56:51.150799 21892 net.cpp:198] Convolution15 needs backward computation.
I0305 17:56:51.150816 21892 net.cpp:198] Eltwise6_ReLU12_0_split needs backward computation.
I0305 17:56:51.150835 21892 net.cpp:198] ReLU12 needs backward computation.
I0305 17:56:51.150854 21892 net.cpp:198] Eltwise6 needs backward computation.
I0305 17:56:51.150887 21892 net.cpp:198] Scale14 needs backward computation.
I0305 17:56:51.150918 21892 net.cpp:198] BatchNorm14 needs backward computation.
I0305 17:56:51.150985 21892 net.cpp:198] Convolution14 needs backward computation.
I0305 17:56:51.151010 21892 net.cpp:198] ReLU11 needs backward computation.
I0305 17:56:51.151028 21892 net.cpp:198] Scale13 needs backward computation.
I0305 17:56:51.151046 21892 net.cpp:198] BatchNorm13 needs backward computation.
I0305 17:56:51.151067 21892 net.cpp:198] Convolution13 needs backward computation.
I0305 17:56:51.151088 21892 net.cpp:198] Eltwise5_ReLU10_0_split needs backward computation.
I0305 17:56:51.151106 21892 net.cpp:198] ReLU10 needs backward computation.
I0305 17:56:51.151124 21892 net.cpp:198] Eltwise5 needs backward computation.
I0305 17:56:51.151142 21892 net.cpp:198] Scale12 needs backward computation.
I0305 17:56:51.151160 21892 net.cpp:198] BatchNorm12 needs backward computation.
I0305 17:56:51.151180 21892 net.cpp:198] Convolution12 needs backward computation.
I0305 17:56:51.151197 21892 net.cpp:198] ReLU9 needs backward computation.
I0305 17:56:51.151216 21892 net.cpp:198] Scale11 needs backward computation.
I0305 17:56:51.151232 21892 net.cpp:198] BatchNorm11 needs backward computation.
I0305 17:56:51.151250 21892 net.cpp:198] Convolution11 needs backward computation.
I0305 17:56:51.151283 21892 net.cpp:198] Eltwise4_ReLU8_0_split needs backward computation.
I0305 17:56:51.151301 21892 net.cpp:198] ReLU8 needs backward computation.
I0305 17:56:51.151319 21892 net.cpp:198] Eltwise4 needs backward computation.
I0305 17:56:51.151340 21892 net.cpp:198] Scale10 needs backward computation.
I0305 17:56:51.151357 21892 net.cpp:198] BatchNorm10 needs backward computation.
I0305 17:56:51.151374 21892 net.cpp:198] Convolution10 needs backward computation.
I0305 17:56:51.151392 21892 net.cpp:198] Scale9 needs backward computation.
I0305 17:56:51.151412 21892 net.cpp:198] BatchNorm9 needs backward computation.
I0305 17:56:51.151429 21892 net.cpp:198] Convolution9 needs backward computation.
I0305 17:56:51.151448 21892 net.cpp:198] ReLU7 needs backward computation.
I0305 17:56:51.151463 21892 net.cpp:198] Scale8 needs backward computation.
I0305 17:56:51.151482 21892 net.cpp:198] BatchNorm8 needs backward computation.
I0305 17:56:51.151499 21892 net.cpp:198] Convolution8 needs backward computation.
I0305 17:56:51.151518 21892 net.cpp:198] Eltwise3_ReLU6_0_split needs backward computation.
I0305 17:56:51.151536 21892 net.cpp:198] ReLU6 needs backward computation.
I0305 17:56:51.151569 21892 net.cpp:198] Eltwise3 needs backward computation.
I0305 17:56:51.151589 21892 net.cpp:198] Scale7 needs backward computation.
I0305 17:56:51.151610 21892 net.cpp:198] BatchNorm7 needs backward computation.
I0305 17:56:51.151651 21892 net.cpp:198] Convolution7 needs backward computation.
I0305 17:56:51.151665 21892 net.cpp:198] ReLU5 needs backward computation.
I0305 17:56:51.151705 21892 net.cpp:198] Scale6 needs backward computation.
I0305 17:56:51.151747 21892 net.cpp:198] BatchNorm6 needs backward computation.
I0305 17:56:51.151762 21892 net.cpp:198] Convolution6 needs backward computation.
I0305 17:56:51.151803 21892 net.cpp:198] Eltwise2_ReLU4_0_split needs backward computation.
I0305 17:56:51.151844 21892 net.cpp:198] ReLU4 needs backward computation.
I0305 17:56:51.151886 21892 net.cpp:198] Eltwise2 needs backward computation.
I0305 17:56:51.151929 21892 net.cpp:198] Scale5 needs backward computation.
I0305 17:56:51.151973 21892 net.cpp:198] BatchNorm5 needs backward computation.
I0305 17:56:51.152014 21892 net.cpp:198] Convolution5 needs backward computation.
I0305 17:56:51.152056 21892 net.cpp:198] ReLU3 needs backward computation.
I0305 17:56:51.152098 21892 net.cpp:198] Scale4 needs backward computation.
I0305 17:56:51.152113 21892 net.cpp:198] BatchNorm4 needs backward computation.
I0305 17:56:51.152153 21892 net.cpp:198] Convolution4 needs backward computation.
I0305 17:56:51.152196 21892 net.cpp:198] Eltwise1_ReLU2_0_split needs backward computation.
I0305 17:56:51.152264 21892 net.cpp:198] ReLU2 needs backward computation.
I0305 17:56:51.152317 21892 net.cpp:198] Eltwise1 needs backward computation.
I0305 17:56:51.152384 21892 net.cpp:198] Scale3 needs backward computation.
I0305 17:56:51.152431 21892 net.cpp:198] BatchNorm3 needs backward computation.
I0305 17:56:51.152472 21892 net.cpp:198] Convolution3 needs backward computation.
I0305 17:56:51.152515 21892 net.cpp:198] ReLU1 needs backward computation.
I0305 17:56:51.152557 21892 net.cpp:198] Scale2 needs backward computation.
I0305 17:56:51.152571 21892 net.cpp:198] BatchNorm2 needs backward computation.
I0305 17:56:51.152587 21892 net.cpp:198] Convolution2 needs backward computation.
I0305 17:56:51.152628 21892 net.cpp:198] Convolution1_Scale1_0_split needs backward computation.
I0305 17:56:51.152643 21892 net.cpp:198] Scale1 needs backward computation.
I0305 17:56:51.152700 21892 net.cpp:198] BatchNorm1 needs backward computation.
I0305 17:56:51.152741 21892 net.cpp:198] Convolution1 needs backward computation.
I0305 17:56:51.152783 21892 net.cpp:200] Concat1 does not need backward computation.
I0305 17:56:51.152827 21892 net.cpp:200] Silence does not need backward computation.
I0305 17:56:51.152842 21892 net.cpp:200] Data5 does not need backward computation.
I0305 17:56:51.152854 21892 net.cpp:200] Data3 does not need backward computation.
I0305 17:56:51.152878 21892 net.cpp:200] Data1 does not need backward computation.
I0305 17:56:51.152918 21892 net.cpp:242] This network produces output Loss
I0305 17:56:51.153069 21892 net.cpp:255] Network initialization done.
I0305 17:56:51.153771 21892 solver.cpp:56] Solver scaffolding done.
I0305 17:56:51.165865 21892 caffe.cpp:248] Starting Optimization
I0305 17:56:51.165922 21892 solver.cpp:272] Solving 
I0305 17:56:51.165951 21892 solver.cpp:273] Learning Rate Policy: inv
I0305 17:56:51.177539 21892 solver.cpp:330] Iteration 0, Testing net (#0)
I0305 17:56:51.183861 21892 blocking_queue.cpp:49] Waiting for data
I0305 17:56:53.041296 21892 solver.cpp:397]     Test net output #0: Loss = 1248.07 (* 1 = 1248.07 loss)
I0305 17:56:53.963140 21892 solver.cpp:218] Iteration 0 (0 iter/s, 2.797s/25 iters), loss = 1300.41
I0305 17:56:53.963234 21892 solver.cpp:237]     Train net output #0: Loss = 1300.41 (* 1 = 1300.41 loss)
I0305 17:56:53.963282 21892 sgd_solver.cpp:105] Iteration 0, lr = 1e-06
I0305 17:57:07.990098 21892 solver.cpp:218] Iteration 25 (1.78235 iter/s, 14.0264s/25 iters), loss = 400.226
I0305 17:57:07.990198 21892 solver.cpp:237]     Train net output #0: Loss = 400.226 (* 1 = 400.226 loss)
I0305 17:57:07.990221 21892 sgd_solver.cpp:105] Iteration 25, lr = 9.98129e-07
I0305 17:57:22.116189 21892 solver.cpp:218] Iteration 50 (1.76985 iter/s, 14.1255s/25 iters), loss = 626.342
I0305 17:57:22.116400 21892 solver.cpp:237]     Train net output #0: Loss = 626.342 (* 1 = 626.342 loss)
I0305 17:57:22.116433 21892 sgd_solver.cpp:105] Iteration 50, lr = 9.96266e-07
I0305 17:57:36.154482 21892 solver.cpp:218] Iteration 75 (1.78093 iter/s, 14.0376s/25 iters), loss = 527.13
I0305 17:57:36.154584 21892 solver.cpp:237]     Train net output #0: Loss = 527.13 (* 1 = 527.13 loss)
I0305 17:57:36.154608 21892 sgd_solver.cpp:105] Iteration 75, lr = 9.94412e-07
I0305 17:57:50.149129 21892 solver.cpp:218] Iteration 100 (1.78647 iter/s, 13.9941s/25 iters), loss = 578.366
I0305 17:57:50.149238 21892 solver.cpp:237]     Train net output #0: Loss = 578.366 (* 1 = 578.366 loss)
I0305 17:57:50.149261 21892 sgd_solver.cpp:105] Iteration 100, lr = 9.92565e-07
I0305 17:58:04.200965 21892 solver.cpp:218] Iteration 125 (1.7792 iter/s, 14.0513s/25 iters), loss = 539.145
I0305 17:58:04.201187 21892 solver.cpp:237]     Train net output #0: Loss = 539.145 (* 1 = 539.145 loss)
I0305 17:58:04.201220 21892 sgd_solver.cpp:105] Iteration 125, lr = 9.90726e-07
I0305 17:58:18.331020 21892 solver.cpp:218] Iteration 150 (1.76936 iter/s, 14.1294s/25 iters), loss = 375.279
I0305 17:58:18.331132 21892 solver.cpp:237]     Train net output #0: Loss = 375.279 (* 1 = 375.279 loss)
I0305 17:58:18.331156 21892 sgd_solver.cpp:105] Iteration 150, lr = 9.88896e-07
I0305 17:58:32.427857 21892 solver.cpp:218] Iteration 175 (1.77352 iter/s, 14.0962s/25 iters), loss = 578.818
I0305 17:58:32.427973 21892 solver.cpp:237]     Train net output #0: Loss = 578.818 (* 1 = 578.818 loss)
I0305 17:58:32.427994 21892 sgd_solver.cpp:105] Iteration 175, lr = 9.87073e-07
I0305 17:58:46.544530 21892 solver.cpp:218] Iteration 200 (1.77103 iter/s, 14.1161s/25 iters), loss = 487.695
I0305 17:58:46.544811 21892 solver.cpp:237]     Train net output #0: Loss = 487.694 (* 1 = 487.694 loss)
I0305 17:58:46.544849 21892 sgd_solver.cpp:105] Iteration 200, lr = 9.85258e-07
I0305 17:59:00.662744 21892 solver.cpp:218] Iteration 225 (1.77086 iter/s, 14.1175s/25 iters), loss = 533.865
I0305 17:59:00.662853 21892 solver.cpp:237]     Train net output #0: Loss = 533.865 (* 1 = 533.865 loss)
I0305 17:59:00.662888 21892 sgd_solver.cpp:105] Iteration 225, lr = 9.8345e-07
I0305 17:59:14.770634 21892 solver.cpp:218] Iteration 250 (1.77213 iter/s, 14.1073s/25 iters), loss = 495.935
I0305 17:59:14.770757 21892 solver.cpp:237]     Train net output #0: Loss = 495.935 (* 1 = 495.935 loss)
I0305 17:59:14.770781 21892 sgd_solver.cpp:105] Iteration 250, lr = 9.81651e-07
I0305 17:59:28.902348 21892 solver.cpp:218] Iteration 275 (1.76915 iter/s, 14.1311s/25 iters), loss = 328.292
I0305 17:59:28.904850 21892 solver.cpp:237]     Train net output #0: Loss = 328.292 (* 1 = 328.292 loss)
I0305 17:59:28.904875 21892 sgd_solver.cpp:105] Iteration 275, lr = 9.79859e-07
I0305 17:59:43.054985 21892 solver.cpp:218] Iteration 300 (1.76683 iter/s, 14.1497s/25 iters), loss = 354.28
I0305 17:59:43.055088 21892 solver.cpp:237]     Train net output #0: Loss = 354.28 (* 1 = 354.28 loss)
I0305 17:59:43.055110 21892 sgd_solver.cpp:105] Iteration 300, lr = 9.78075e-07
I0305 17:59:57.171228 21892 solver.cpp:218] Iteration 325 (1.77109 iter/s, 14.1156s/25 iters), loss = 205.022
I0305 17:59:57.171336 21892 solver.cpp:237]     Train net output #0: Loss = 205.022 (* 1 = 205.022 loss)
I0305 17:59:57.171360 21892 sgd_solver.cpp:105] Iteration 325, lr = 9.76298e-07
I0305 18:00:11.278674 21892 solver.cpp:218] Iteration 350 (1.77219 iter/s, 14.1068s/25 iters), loss = 167.165
I0305 18:00:11.278945 21892 solver.cpp:237]     Train net output #0: Loss = 167.165 (* 1 = 167.165 loss)
I0305 18:00:11.278992 21892 sgd_solver.cpp:105] Iteration 350, lr = 9.74529e-07
I0305 18:00:25.386306 21892 solver.cpp:218] Iteration 375 (1.77218 iter/s, 14.1069s/25 iters), loss = 115.112
I0305 18:00:25.386412 21892 solver.cpp:237]     Train net output #0: Loss = 115.112 (* 1 = 115.112 loss)
I0305 18:00:25.386456 21892 sgd_solver.cpp:105] Iteration 375, lr = 9.72767e-07
I0305 18:00:39.486966 21892 solver.cpp:218] Iteration 400 (1.77304 iter/s, 14.1001s/25 iters), loss = 156.323
I0305 18:00:39.487118 21892 solver.cpp:237]     Train net output #0: Loss = 156.323 (* 1 = 156.323 loss)
I0305 18:00:39.487154 21892 sgd_solver.cpp:105] Iteration 400, lr = 9.71013e-07
I0305 18:00:53.600764 21892 solver.cpp:218] Iteration 425 (1.77139 iter/s, 14.1132s/25 iters), loss = 122.826
I0305 18:00:53.600999 21892 solver.cpp:237]     Train net output #0: Loss = 122.826 (* 1 = 122.826 loss)
I0305 18:00:53.601027 21892 sgd_solver.cpp:105] Iteration 425, lr = 9.69266e-07
I0305 18:01:07.751427 21892 solver.cpp:218] Iteration 450 (1.76679 iter/s, 14.1499s/25 iters), loss = 89.8235
I0305 18:01:07.751538 21892 solver.cpp:237]     Train net output #0: Loss = 89.8234 (* 1 = 89.8234 loss)
I0305 18:01:07.751562 21892 sgd_solver.cpp:105] Iteration 450, lr = 9.67526e-07
I0305 18:01:21.893249 21892 solver.cpp:218] Iteration 475 (1.76788 iter/s, 14.1412s/25 iters), loss = 92.2194
I0305 18:01:21.893386 21892 solver.cpp:237]     Train net output #0: Loss = 92.2194 (* 1 = 92.2194 loss)
I0305 18:01:21.893411 21892 sgd_solver.cpp:105] Iteration 475, lr = 9.65794e-07
I0305 18:01:35.429267 21892 solver.cpp:447] Snapshotting to binary proto file ./snapshots/auto/v3/v3_iter_500.caffemodel
I0305 18:01:35.469964 21892 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/auto/v3/v3_iter_500.solverstate
I0305 18:01:36.044097 21892 solver.cpp:218] Iteration 500 (1.76676 iter/s, 14.1502s/25 iters), loss = 72.4774
I0305 18:01:36.044201 21892 solver.cpp:237]     Train net output #0: Loss = 72.4773 (* 1 = 72.4773 loss)
I0305 18:01:36.044222 21892 sgd_solver.cpp:105] Iteration 500, lr = 9.64069e-07
I0305 18:01:50.165648 21892 solver.cpp:218] Iteration 525 (1.77042 iter/s, 14.1209s/25 iters), loss = 74.4511
I0305 18:01:50.165763 21892 solver.cpp:237]     Train net output #0: Loss = 74.4511 (* 1 = 74.4511 loss)
I0305 18:01:50.165784 21892 sgd_solver.cpp:105] Iteration 525, lr = 9.62351e-07
I0305 18:02:04.297221 21892 solver.cpp:218] Iteration 550 (1.76917 iter/s, 14.131s/25 iters), loss = 59.9361
I0305 18:02:04.297364 21892 solver.cpp:237]     Train net output #0: Loss = 59.936 (* 1 = 59.936 loss)
I0305 18:02:04.297395 21892 sgd_solver.cpp:105] Iteration 550, lr = 9.6064e-07
I0305 18:02:18.592229 21892 solver.cpp:218] Iteration 575 (1.74894 iter/s, 14.2944s/25 iters), loss = 51.3318
I0305 18:02:18.592502 21892 solver.cpp:237]     Train net output #0: Loss = 51.3317 (* 1 = 51.3317 loss)
I0305 18:02:18.592525 21892 sgd_solver.cpp:105] Iteration 575, lr = 9.58936e-07
I0305 18:02:32.719444 21892 solver.cpp:218] Iteration 600 (1.76973 iter/s, 14.1265s/25 iters), loss = 62.8214
I0305 18:02:32.719545 21892 solver.cpp:237]     Train net output #0: Loss = 62.8213 (* 1 = 62.8213 loss)
I0305 18:02:32.719569 21892 sgd_solver.cpp:105] Iteration 600, lr = 9.5724e-07
I0305 18:02:46.843430 21892 solver.cpp:218] Iteration 625 (1.77012 iter/s, 14.1233s/25 iters), loss = 58.9382
I0305 18:02:46.843559 21892 solver.cpp:237]     Train net output #0: Loss = 58.9381 (* 1 = 58.9381 loss)
I0305 18:02:46.843585 21892 sgd_solver.cpp:105] Iteration 625, lr = 9.5555e-07
I0305 18:03:00.980928 21892 solver.cpp:218] Iteration 650 (1.76842 iter/s, 14.1369s/25 iters), loss = 45.3616
I0305 18:03:00.981374 21892 solver.cpp:237]     Train net output #0: Loss = 45.3616 (* 1 = 45.3616 loss)
I0305 18:03:00.981410 21892 sgd_solver.cpp:105] Iteration 650, lr = 9.53867e-07
I0305 18:03:15.101189 21892 solver.cpp:218] Iteration 675 (1.77062 iter/s, 14.1193s/25 iters), loss = 48.8069
I0305 18:03:15.101297 21892 solver.cpp:237]     Train net output #0: Loss = 48.8068 (* 1 = 48.8068 loss)
I0305 18:03:15.101321 21892 sgd_solver.cpp:105] Iteration 675, lr = 9.52191e-07
I0305 18:03:29.194948 21892 solver.cpp:218] Iteration 700 (1.77391 iter/s, 14.0931s/25 iters), loss = 38.9152
I0305 18:03:29.195067 21892 solver.cpp:237]     Train net output #0: Loss = 38.9152 (* 1 = 38.9152 loss)
I0305 18:03:29.195091 21892 sgd_solver.cpp:105] Iteration 700, lr = 9.50522e-07
I0305 18:03:43.354743 21892 solver.cpp:218] Iteration 725 (1.76564 iter/s, 14.1591s/25 iters), loss = 47.9568
I0305 18:03:43.354954 21892 solver.cpp:237]     Train net output #0: Loss = 47.9567 (* 1 = 47.9567 loss)
I0305 18:03:43.354979 21892 sgd_solver.cpp:105] Iteration 725, lr = 9.4886e-07
I0305 18:03:57.481144 21892 solver.cpp:218] Iteration 750 (1.76982 iter/s, 14.1257s/25 iters), loss = 48.6927
I0305 18:03:57.481245 21892 solver.cpp:237]     Train net output #0: Loss = 48.6926 (* 1 = 48.6926 loss)
I0305 18:03:57.481267 21892 sgd_solver.cpp:105] Iteration 750, lr = 9.47204e-07
I0305 18:04:11.602107 21892 solver.cpp:218] Iteration 775 (1.77049 iter/s, 14.1203s/25 iters), loss = 32.2974
I0305 18:04:11.602208 21892 solver.cpp:237]     Train net output #0: Loss = 32.2973 (* 1 = 32.2973 loss)
I0305 18:04:11.602231 21892 sgd_solver.cpp:105] Iteration 775, lr = 9.45556e-07
I0305 18:04:25.777168 21892 solver.cpp:218] Iteration 800 (1.76374 iter/s, 14.1744s/25 iters), loss = 44.5071
I0305 18:04:25.777722 21892 solver.cpp:237]     Train net output #0: Loss = 44.507 (* 1 = 44.507 loss)
I0305 18:04:25.777748 21892 sgd_solver.cpp:105] Iteration 800, lr = 9.43913e-07
I0305 18:04:39.972661 21892 solver.cpp:218] Iteration 825 (1.76125 iter/s, 14.1944s/25 iters), loss = 32.4114
I0305 18:04:39.972776 21892 solver.cpp:237]     Train net output #0: Loss = 32.4114 (* 1 = 32.4114 loss)
I0305 18:04:39.972796 21892 sgd_solver.cpp:105] Iteration 825, lr = 9.42278e-07
I0305 18:04:54.096745 21892 solver.cpp:218] Iteration 850 (1.77011 iter/s, 14.1234s/25 iters), loss = 41.1736
I0305 18:04:54.096881 21892 solver.cpp:237]     Train net output #0: Loss = 41.1735 (* 1 = 41.1735 loss)
I0305 18:04:54.096911 21892 sgd_solver.cpp:105] Iteration 850, lr = 9.40649e-07
I0305 18:05:08.210942 21892 solver.cpp:218] Iteration 875 (1.77134 iter/s, 14.1136s/25 iters), loss = 40.5894
I0305 18:05:08.212934 21892 solver.cpp:237]     Train net output #0: Loss = 40.5894 (* 1 = 40.5894 loss)
I0305 18:05:08.212962 21892 sgd_solver.cpp:105] Iteration 875, lr = 9.39027e-07
I0305 18:05:22.351810 21892 solver.cpp:218] Iteration 900 (1.76824 iter/s, 14.1383s/25 iters), loss = 29.7958
I0305 18:05:22.351924 21892 solver.cpp:237]     Train net output #0: Loss = 29.7957 (* 1 = 29.7957 loss)
I0305 18:05:22.351949 21892 sgd_solver.cpp:105] Iteration 900, lr = 9.37411e-07
I0305 18:05:36.459367 21892 solver.cpp:218] Iteration 925 (1.77218 iter/s, 14.1069s/25 iters), loss = 36.5524
I0305 18:05:36.459471 21892 solver.cpp:237]     Train net output #0: Loss = 36.5523 (* 1 = 36.5523 loss)
I0305 18:05:36.459494 21892 sgd_solver.cpp:105] Iteration 925, lr = 9.35802e-07
I0305 18:05:50.618788 21892 solver.cpp:218] Iteration 950 (1.76568 iter/s, 14.1588s/25 iters), loss = 31.8716
I0305 18:05:50.619046 21892 solver.cpp:237]     Train net output #0: Loss = 31.8715 (* 1 = 31.8715 loss)
I0305 18:05:50.619072 21892 sgd_solver.cpp:105] Iteration 950, lr = 9.34199e-07
I0305 18:06:04.734632 21892 solver.cpp:218] Iteration 975 (1.77116 iter/s, 14.115s/25 iters), loss = 38.3492
I0305 18:06:04.734761 21892 solver.cpp:237]     Train net output #0: Loss = 38.3491 (* 1 = 38.3491 loss)
I0305 18:06:04.734783 21892 sgd_solver.cpp:105] Iteration 975, lr = 9.32603e-07
I0305 18:06:18.287811 21892 solver.cpp:447] Snapshotting to binary proto file ./snapshots/auto/v3/v3_iter_1000.caffemodel
I0305 18:06:18.314082 21892 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/auto/v3/v3_iter_1000.solverstate
I0305 18:06:18.881498 21892 solver.cpp:218] Iteration 1000 (1.76725 iter/s, 14.1462s/25 iters), loss = 28.4233
I0305 18:06:18.881600 21892 solver.cpp:237]     Train net output #0: Loss = 28.4232 (* 1 = 28.4232 loss)
I0305 18:06:18.881625 21892 sgd_solver.cpp:105] Iteration 1000, lr = 9.31012e-07
I0305 18:06:33.104519 21892 solver.cpp:218] Iteration 1025 (1.75779 iter/s, 14.2224s/25 iters), loss = 38.9954
I0305 18:06:33.104820 21892 solver.cpp:237]     Train net output #0: Loss = 38.9953 (* 1 = 38.9953 loss)
I0305 18:06:33.104854 21892 sgd_solver.cpp:105] Iteration 1025, lr = 9.29429e-07
I0305 18:06:47.207466 21892 solver.cpp:218] Iteration 1050 (1.77278 iter/s, 14.1021s/25 iters), loss = 33.0458
I0305 18:06:47.207634 21892 solver.cpp:237]     Train net output #0: Loss = 33.0457 (* 1 = 33.0457 loss)
I0305 18:06:47.207665 21892 sgd_solver.cpp:105] Iteration 1050, lr = 9.27851e-07
I0305 18:07:01.358932 21892 solver.cpp:218] Iteration 1075 (1.76669 iter/s, 14.1508s/25 iters), loss = 40.1139
I0305 18:07:01.359092 21892 solver.cpp:237]     Train net output #0: Loss = 40.1138 (* 1 = 40.1138 loss)
I0305 18:07:01.359117 21892 sgd_solver.cpp:105] Iteration 1075, lr = 9.2628e-07
I0305 18:07:15.539683 21892 solver.cpp:218] Iteration 1100 (1.76303 iter/s, 14.1801s/25 iters), loss = 39.952
I0305 18:07:15.541836 21892 solver.cpp:237]     Train net output #0: Loss = 39.9519 (* 1 = 39.9519 loss)
I0305 18:07:15.541877 21892 sgd_solver.cpp:105] Iteration 1100, lr = 9.24715e-07
I0305 18:07:29.666834 21892 solver.cpp:218] Iteration 1125 (1.76997 iter/s, 14.1245s/25 iters), loss = 30.8755
I0305 18:07:29.666957 21892 solver.cpp:237]     Train net output #0: Loss = 30.8755 (* 1 = 30.8755 loss)
I0305 18:07:29.666978 21892 sgd_solver.cpp:105] Iteration 1125, lr = 9.23156e-07
I0305 18:07:43.793516 21892 solver.cpp:218] Iteration 1150 (1.76978 iter/s, 14.1261s/25 iters), loss = 39.4637
I0305 18:07:43.793620 21892 solver.cpp:237]     Train net output #0: Loss = 39.4636 (* 1 = 39.4636 loss)
I0305 18:07:43.793643 21892 sgd_solver.cpp:105] Iteration 1150, lr = 9.21603e-07
I0305 18:07:57.929241 21892 solver.cpp:218] Iteration 1175 (1.76865 iter/s, 14.1351s/25 iters), loss = 28.4017
I0305 18:07:57.929946 21892 solver.cpp:237]     Train net output #0: Loss = 28.4016 (* 1 = 28.4016 loss)
I0305 18:07:57.929973 21892 sgd_solver.cpp:105] Iteration 1175, lr = 9.20056e-07
I0305 18:08:12.034613 21892 solver.cpp:218] Iteration 1200 (1.77252 iter/s, 14.1042s/25 iters), loss = 25.5648
I0305 18:08:12.034719 21892 solver.cpp:237]     Train net output #0: Loss = 25.5647 (* 1 = 25.5647 loss)
I0305 18:08:12.034744 21892 sgd_solver.cpp:105] Iteration 1200, lr = 9.18515e-07
I0305 18:08:26.153861 21892 solver.cpp:218] Iteration 1225 (1.77072 iter/s, 14.1186s/25 iters), loss = 39.2251
I0305 18:08:26.153975 21892 solver.cpp:237]     Train net output #0: Loss = 39.2251 (* 1 = 39.2251 loss)
I0305 18:08:26.153995 21892 sgd_solver.cpp:105] Iteration 1225, lr = 9.16981e-07
I0305 18:08:40.339843 21892 solver.cpp:218] Iteration 1250 (1.76238 iter/s, 14.1854s/25 iters), loss = 29.549
I0305 18:08:40.340503 21892 solver.cpp:237]     Train net output #0: Loss = 29.549 (* 1 = 29.549 loss)
I0305 18:08:40.340544 21892 sgd_solver.cpp:105] Iteration 1250, lr = 9.15452e-07
I0305 18:08:54.499322 21892 solver.cpp:218] Iteration 1275 (1.76575 iter/s, 14.1583s/25 iters), loss = 30.2184
I0305 18:08:54.499438 21892 solver.cpp:237]     Train net output #0: Loss = 30.2183 (* 1 = 30.2183 loss)
I0305 18:08:54.499459 21892 sgd_solver.cpp:105] Iteration 1275, lr = 9.13929e-07
I0305 18:09:08.555371 21892 solver.cpp:218] Iteration 1300 (1.77868 iter/s, 14.0554s/25 iters), loss = 28.2405
I0305 18:09:08.555491 21892 solver.cpp:237]     Train net output #0: Loss = 28.2404 (* 1 = 28.2404 loss)
I0305 18:09:08.555516 21892 sgd_solver.cpp:105] Iteration 1300, lr = 9.12412e-07
I0305 18:09:22.646270 21892 solver.cpp:218] Iteration 1325 (1.77427 iter/s, 14.0903s/25 iters), loss = 26.7813
I0305 18:09:22.646579 21892 solver.cpp:237]     Train net output #0: Loss = 26.7813 (* 1 = 26.7813 loss)
I0305 18:09:22.646620 21892 sgd_solver.cpp:105] Iteration 1325, lr = 9.10901e-07
I0305 18:09:36.733783 21892 solver.cpp:218] Iteration 1350 (1.77472 iter/s, 14.0867s/25 iters), loss = 32.762
I0305 18:09:36.733886 21892 solver.cpp:237]     Train net output #0: Loss = 32.7619 (* 1 = 32.7619 loss)
I0305 18:09:36.733913 21892 sgd_solver.cpp:105] Iteration 1350, lr = 9.09396e-07
I0305 18:09:50.826517 21892 solver.cpp:218] Iteration 1375 (1.77404 iter/s, 14.0921s/25 iters), loss = 22.5106
I0305 18:09:50.826654 21892 solver.cpp:237]     Train net output #0: Loss = 22.5105 (* 1 = 22.5105 loss)
I0305 18:09:50.826678 21892 sgd_solver.cpp:105] Iteration 1375, lr = 9.07897e-07
I0305 18:10:04.940297 21892 solver.cpp:218] Iteration 1400 (1.7714 iter/s, 14.1131s/25 iters), loss = 21.9705
I0305 18:10:04.942648 21892 solver.cpp:237]     Train net output #0: Loss = 21.9705 (* 1 = 21.9705 loss)
I0305 18:10:04.942672 21892 sgd_solver.cpp:105] Iteration 1400, lr = 9.06403e-07
I0305 18:10:19.005751 21892 solver.cpp:218] Iteration 1425 (1.77776 iter/s, 14.0626s/25 iters), loss = 26.9408
I0305 18:10:19.005865 21892 solver.cpp:237]     Train net output #0: Loss = 26.9407 (* 1 = 26.9407 loss)
I0305 18:10:19.005893 21892 sgd_solver.cpp:105] Iteration 1425, lr = 9.04915e-07
I0305 18:10:33.078219 21892 solver.cpp:218] Iteration 1450 (1.7766 iter/s, 14.0718s/25 iters), loss = 24.547
I0305 18:10:33.078371 21892 solver.cpp:237]     Train net output #0: Loss = 24.5469 (* 1 = 24.5469 loss)
I0305 18:10:33.078403 21892 sgd_solver.cpp:105] Iteration 1450, lr = 9.03433e-07
I0305 18:10:47.145853 21892 solver.cpp:218] Iteration 1475 (1.77721 iter/s, 14.067s/25 iters), loss = 28.0363
I0305 18:10:47.147605 21892 solver.cpp:237]     Train net output #0: Loss = 28.0362 (* 1 = 28.0362 loss)
I0305 18:10:47.147629 21892 sgd_solver.cpp:105] Iteration 1475, lr = 9.01956e-07
I0305 18:11:00.688663 21892 solver.cpp:447] Snapshotting to binary proto file ./snapshots/auto/v3/v3_iter_1500.caffemodel
I0305 18:11:00.711889 21892 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/auto/v3/v3_iter_1500.solverstate
I0305 18:11:01.284415 21892 solver.cpp:218] Iteration 1500 (1.7685 iter/s, 14.1363s/25 iters), loss = 20.6553
I0305 18:11:01.284560 21892 solver.cpp:237]     Train net output #0: Loss = 20.6553 (* 1 = 20.6553 loss)
I0305 18:11:01.284591 21892 sgd_solver.cpp:105] Iteration 1500, lr = 9.00485e-07
I0305 18:11:15.407719 21892 solver.cpp:218] Iteration 1525 (1.7702 iter/s, 14.1227s/25 iters), loss = 20.549
I0305 18:11:15.407816 21892 solver.cpp:237]     Train net output #0: Loss = 20.549 (* 1 = 20.549 loss)
I0305 18:11:15.407837 21892 sgd_solver.cpp:105] Iteration 1525, lr = 8.9902e-07
I0305 18:11:29.524843 21892 solver.cpp:218] Iteration 1550 (1.77097 iter/s, 14.1165s/25 iters), loss = 25.2287
I0305 18:11:29.525226 21892 solver.cpp:237]     Train net output #0: Loss = 25.2286 (* 1 = 25.2286 loss)
I0305 18:11:29.525279 21892 sgd_solver.cpp:105] Iteration 1550, lr = 8.9756e-07
I0305 18:11:43.623364 21892 solver.cpp:218] Iteration 1575 (1.77334 iter/s, 14.0977s/25 iters), loss = 22.9744
I0305 18:11:43.623476 21892 solver.cpp:237]     Train net output #0: Loss = 22.9743 (* 1 = 22.9743 loss)
I0305 18:11:43.623497 21892 sgd_solver.cpp:105] Iteration 1575, lr = 8.96106e-07
I0305 18:11:57.734711 21892 solver.cpp:218] Iteration 1600 (1.7717 iter/s, 14.1107s/25 iters), loss = 25.4501
I0305 18:11:57.734822 21892 solver.cpp:237]     Train net output #0: Loss = 25.45 (* 1 = 25.45 loss)
I0305 18:11:57.734844 21892 sgd_solver.cpp:105] Iteration 1600, lr = 8.94657e-07
I0305 18:12:11.813480 21892 solver.cpp:218] Iteration 1625 (1.7758 iter/s, 14.0781s/25 iters), loss = 19.3488
I0305 18:12:11.816761 21892 solver.cpp:237]     Train net output #0: Loss = 19.3488 (* 1 = 19.3488 loss)
I0305 18:12:11.816792 21892 sgd_solver.cpp:105] Iteration 1625, lr = 8.93213e-07
I0305 18:12:25.933068 21892 solver.cpp:218] Iteration 1650 (1.77106 iter/s, 14.1158s/25 iters), loss = 18.8405
I0305 18:12:25.933171 21892 solver.cpp:237]     Train net output #0: Loss = 18.8404 (* 1 = 18.8404 loss)
I0305 18:12:25.933193 21892 sgd_solver.cpp:105] Iteration 1650, lr = 8.91776e-07
I0305 18:12:40.047963 21892 solver.cpp:218] Iteration 1675 (1.77125 iter/s, 14.1143s/25 iters), loss = 23.8716
I0305 18:12:40.048063 21892 solver.cpp:237]     Train net output #0: Loss = 23.8716 (* 1 = 23.8716 loss)
I0305 18:12:40.048084 21892 sgd_solver.cpp:105] Iteration 1675, lr = 8.90343e-07
I0305 18:12:54.160276 21892 solver.cpp:218] Iteration 1700 (1.77158 iter/s, 14.1117s/25 iters), loss = 21.5255
I0305 18:12:54.160495 21892 solver.cpp:237]     Train net output #0: Loss = 21.5255 (* 1 = 21.5255 loss)
I0305 18:12:54.160517 21892 sgd_solver.cpp:105] Iteration 1700, lr = 8.88916e-07
I0305 18:13:08.255934 21892 solver.cpp:218] Iteration 1725 (1.77369 iter/s, 14.0949s/25 iters), loss = 23.8192
I0305 18:13:08.256063 21892 solver.cpp:237]     Train net output #0: Loss = 23.8191 (* 1 = 23.8191 loss)
I0305 18:13:08.256095 21892 sgd_solver.cpp:105] Iteration 1725, lr = 8.87494e-07
I0305 18:13:22.365262 21892 solver.cpp:218] Iteration 1750 (1.77196 iter/s, 14.1087s/25 iters), loss = 18.2787
I0305 18:13:22.365373 21892 solver.cpp:237]     Train net output #0: Loss = 18.2786 (* 1 = 18.2786 loss)
I0305 18:13:22.365397 21892 sgd_solver.cpp:105] Iteration 1750, lr = 8.86077e-07
I0305 18:13:36.422694 21892 solver.cpp:218] Iteration 1775 (1.7785 iter/s, 14.0568s/25 iters), loss = 17.2308
I0305 18:13:36.425916 21892 solver.cpp:237]     Train net output #0: Loss = 17.2308 (* 1 = 17.2308 loss)
I0305 18:13:36.425973 21892 sgd_solver.cpp:105] Iteration 1775, lr = 8.84666e-07
I0305 18:13:50.502542 21892 solver.cpp:218] Iteration 1800 (1.77605 iter/s, 14.0762s/25 iters), loss = 22.9691
I0305 18:13:50.502645 21892 solver.cpp:237]     Train net output #0: Loss = 22.9691 (* 1 = 22.9691 loss)
I0305 18:13:50.502666 21892 sgd_solver.cpp:105] Iteration 1800, lr = 8.8326e-07
I0305 18:14:04.604904 21892 solver.cpp:218] Iteration 1825 (1.77283 iter/s, 14.1018s/25 iters), loss = 20.7131
I0305 18:14:04.605029 21892 solver.cpp:237]     Train net output #0: Loss = 20.7131 (* 1 = 20.7131 loss)
I0305 18:14:04.605057 21892 sgd_solver.cpp:105] Iteration 1825, lr = 8.81859e-07
I0305 18:14:18.685969 21892 solver.cpp:218] Iteration 1850 (1.77552 iter/s, 14.0804s/25 iters), loss = 22.5602
I0305 18:14:18.686380 21892 solver.cpp:237]     Train net output #0: Loss = 22.5601 (* 1 = 22.5601 loss)
I0305 18:14:18.686415 21892 sgd_solver.cpp:105] Iteration 1850, lr = 8.80463e-07
I0305 18:14:32.856549 21892 solver.cpp:218] Iteration 1875 (1.76433 iter/s, 14.1697s/25 iters), loss = 17.4475
I0305 18:14:32.856644 21892 solver.cpp:237]     Train net output #0: Loss = 17.4474 (* 1 = 17.4474 loss)
I0305 18:14:32.856664 21892 sgd_solver.cpp:105] Iteration 1875, lr = 8.79073e-07
I0305 18:14:46.983700 21892 solver.cpp:218] Iteration 1900 (1.76972 iter/s, 14.1266s/25 iters), loss = 15.5034
I0305 18:14:46.983799 21892 solver.cpp:237]     Train net output #0: Loss = 15.5034 (* 1 = 15.5034 loss)
I0305 18:14:46.983820 21892 sgd_solver.cpp:105] Iteration 1900, lr = 8.77687e-07
I0305 18:15:01.106662 21892 solver.cpp:218] Iteration 1925 (1.77025 iter/s, 14.1223s/25 iters), loss = 22.1851
I0305 18:15:01.107010 21892 solver.cpp:237]     Train net output #0: Loss = 22.1851 (* 1 = 22.1851 loss)
I0305 18:15:01.107082 21892 sgd_solver.cpp:105] Iteration 1925, lr = 8.76307e-07
I0305 18:15:15.241011 21892 solver.cpp:218] Iteration 1950 (1.76884 iter/s, 14.1336s/25 iters), loss = 19.9651
I0305 18:15:15.241119 21892 solver.cpp:237]     Train net output #0: Loss = 19.965 (* 1 = 19.965 loss)
I0305 18:15:15.241142 21892 sgd_solver.cpp:105] Iteration 1950, lr = 8.74932e-07
I0305 18:15:29.352797 21892 solver.cpp:218] Iteration 1975 (1.77164 iter/s, 14.1112s/25 iters), loss = 21.546
I0305 18:15:29.352897 21892 solver.cpp:237]     Train net output #0: Loss = 21.546 (* 1 = 21.546 loss)
I0305 18:15:29.352921 21892 sgd_solver.cpp:105] Iteration 1975, lr = 8.73561e-07
I0305 18:15:42.935781 21892 solver.cpp:447] Snapshotting to binary proto file ./snapshots/auto/v3/v3_iter_2000.caffemodel
I0305 18:15:42.960206 21892 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/auto/v3/v3_iter_2000.solverstate
I0305 18:15:43.529037 21892 solver.cpp:218] Iteration 2000 (1.76359 iter/s, 14.1757s/25 iters), loss = 16.8303
I0305 18:15:43.529183 21892 solver.cpp:237]     Train net output #0: Loss = 16.8302 (* 1 = 16.8302 loss)
I0305 18:15:43.529206 21892 sgd_solver.cpp:105] Iteration 2000, lr = 8.72196e-07
I0305 18:15:57.689695 21892 solver.cpp:218] Iteration 2025 (1.76554 iter/s, 14.1599s/25 iters), loss = 14.4301
I0305 18:15:57.689877 21892 solver.cpp:237]     Train net output #0: Loss = 14.43 (* 1 = 14.43 loss)
I0305 18:15:57.689935 21892 sgd_solver.cpp:105] Iteration 2025, lr = 8.70836e-07
I0305 18:16:11.946692 21892 solver.cpp:218] Iteration 2050 (1.75361 iter/s, 14.2563s/25 iters), loss = 21.2242
I0305 18:16:11.946815 21892 solver.cpp:237]     Train net output #0: Loss = 21.2242 (* 1 = 21.2242 loss)
I0305 18:16:11.946838 21892 sgd_solver.cpp:105] Iteration 2050, lr = 8.6948e-07
I0305 18:16:26.108525 21892 solver.cpp:218] Iteration 2075 (1.76538 iter/s, 14.1612s/25 iters), loss = 19.1305
I0305 18:16:26.109122 21892 solver.cpp:237]     Train net output #0: Loss = 19.1304 (* 1 = 19.1304 loss)
I0305 18:16:26.109150 21892 sgd_solver.cpp:105] Iteration 2075, lr = 8.6813e-07
I0305 18:16:40.271739 21892 solver.cpp:218] Iteration 2100 (1.76527 iter/s, 14.1621s/25 iters), loss = 20.7129
I0305 18:16:40.271862 21892 solver.cpp:237]     Train net output #0: Loss = 20.7128 (* 1 = 20.7128 loss)
I0305 18:16:40.271885 21892 sgd_solver.cpp:105] Iteration 2100, lr = 8.66784e-07
I0305 18:16:54.451164 21892 solver.cpp:218] Iteration 2125 (1.76319 iter/s, 14.1788s/25 iters), loss = 16.3451
I0305 18:16:54.451277 21892 solver.cpp:237]     Train net output #0: Loss = 16.3451 (* 1 = 16.3451 loss)
I0305 18:16:54.451301 21892 sgd_solver.cpp:105] Iteration 2125, lr = 8.65443e-07
I0305 18:17:08.638396 21892 solver.cpp:218] Iteration 2150 (1.76222 iter/s, 14.1866s/25 iters), loss = 13.669
I0305 18:17:08.639938 21892 solver.cpp:237]     Train net output #0: Loss = 13.669 (* 1 = 13.669 loss)
I0305 18:17:08.639961 21892 sgd_solver.cpp:105] Iteration 2150, lr = 8.64108e-07
I0305 18:17:22.802332 21892 solver.cpp:218] Iteration 2175 (1.7653 iter/s, 14.1619s/25 iters), loss = 19.9382
I0305 18:17:22.802431 21892 solver.cpp:237]     Train net output #0: Loss = 19.9381 (* 1 = 19.9381 loss)
I0305 18:17:22.802453 21892 sgd_solver.cpp:105] Iteration 2175, lr = 8.62776e-07
I0305 18:17:37.051275 21892 solver.cpp:218] Iteration 2200 (1.75459 iter/s, 14.2484s/25 iters), loss = 18.2352
I0305 18:17:37.051376 21892 solver.cpp:237]     Train net output #0: Loss = 18.2352 (* 1 = 18.2352 loss)
I0305 18:17:37.051399 21892 sgd_solver.cpp:105] Iteration 2200, lr = 8.6145e-07
I0305 18:17:51.194972 21892 solver.cpp:218] Iteration 2225 (1.76765 iter/s, 14.1431s/25 iters), loss = 20.0012
I0305 18:17:51.195274 21892 solver.cpp:237]     Train net output #0: Loss = 20.0011 (* 1 = 20.0011 loss)
I0305 18:17:51.195318 21892 sgd_solver.cpp:105] Iteration 2225, lr = 8.60129e-07
I0305 18:18:05.343804 21892 solver.cpp:218] Iteration 2250 (1.76702 iter/s, 14.1481s/25 iters), loss = 15.9599
I0305 18:18:05.343901 21892 solver.cpp:237]     Train net output #0: Loss = 15.9599 (* 1 = 15.9599 loss)
I0305 18:18:05.343921 21892 sgd_solver.cpp:105] Iteration 2250, lr = 8.58812e-07
I0305 18:18:19.464429 21892 solver.cpp:218] Iteration 2275 (1.77054 iter/s, 14.12s/25 iters), loss = 13.1671
I0305 18:18:19.464576 21892 solver.cpp:237]     Train net output #0: Loss = 13.1671 (* 1 = 13.1671 loss)
I0305 18:18:19.464612 21892 sgd_solver.cpp:105] Iteration 2275, lr = 8.575e-07
I0305 18:18:33.616766 21892 solver.cpp:218] Iteration 2300 (1.76657 iter/s, 14.1517s/25 iters), loss = 19.0027
I0305 18:18:33.616955 21892 solver.cpp:237]     Train net output #0: Loss = 19.0026 (* 1 = 19.0026 loss)
I0305 18:18:33.616979 21892 sgd_solver.cpp:105] Iteration 2300, lr = 8.56192e-07
I0305 18:18:47.799929 21892 solver.cpp:218] Iteration 2325 (1.76274 iter/s, 14.1825s/25 iters), loss = 17.4666
I0305 18:18:47.800034 21892 solver.cpp:237]     Train net output #0: Loss = 17.4665 (* 1 = 17.4665 loss)
I0305 18:18:47.800058 21892 sgd_solver.cpp:105] Iteration 2325, lr = 8.54889e-07
I0305 18:19:01.951915 21892 solver.cpp:218] Iteration 2350 (1.76662 iter/s, 14.1513s/25 iters), loss = 19.3564
I0305 18:19:01.952056 21892 solver.cpp:237]     Train net output #0: Loss = 19.3563 (* 1 = 19.3563 loss)
I0305 18:19:01.952080 21892 sgd_solver.cpp:105] Iteration 2350, lr = 8.53591e-07
I0305 18:19:16.094223 21892 solver.cpp:218] Iteration 2375 (1.76782 iter/s, 14.1417s/25 iters), loss = 15.561
I0305 18:19:16.094441 21892 solver.cpp:237]     Train net output #0: Loss = 15.5609 (* 1 = 15.5609 loss)
I0305 18:19:16.094463 21892 sgd_solver.cpp:105] Iteration 2375, lr = 8.52297e-07
I0305 18:19:30.276015 21892 solver.cpp:218] Iteration 2400 (1.76291 iter/s, 14.1811s/25 iters), loss = 12.7852
I0305 18:19:30.276137 21892 solver.cpp:237]     Train net output #0: Loss = 12.7851 (* 1 = 12.7851 loss)
I0305 18:19:30.276159 21892 sgd_solver.cpp:105] Iteration 2400, lr = 8.51008e-07
I0305 18:19:44.412119 21892 solver.cpp:218] Iteration 2425 (1.7686 iter/s, 14.1355s/25 iters), loss = 18.2456
I0305 18:19:44.412235 21892 solver.cpp:237]     Train net output #0: Loss = 18.2456 (* 1 = 18.2456 loss)
I0305 18:19:44.412259 21892 sgd_solver.cpp:105] Iteration 2425, lr = 8.49724e-07
I0305 18:19:58.519224 21892 solver.cpp:218] Iteration 2450 (1.77223 iter/s, 14.1065s/25 iters), loss = 16.8427
I0305 18:19:58.523080 21892 solver.cpp:237]     Train net output #0: Loss = 16.8426 (* 1 = 16.8426 loss)
I0305 18:19:58.523115 21892 sgd_solver.cpp:105] Iteration 2450, lr = 8.48444e-07
I0305 18:20:12.662570 21892 solver.cpp:218] Iteration 2475 (1.76816 iter/s, 14.139s/25 iters), loss = 18.7833
I0305 18:20:12.662680 21892 solver.cpp:237]     Train net output #0: Loss = 18.7832 (* 1 = 18.7832 loss)
I0305 18:20:12.662704 21892 sgd_solver.cpp:105] Iteration 2475, lr = 8.47168e-07
I0305 18:20:26.222784 21892 solver.cpp:447] Snapshotting to binary proto file ./snapshots/auto/v3/v3_iter_2500.caffemodel
I0305 18:20:26.245255 21892 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/auto/v3/v3_iter_2500.solverstate
I0305 18:20:26.819205 21892 solver.cpp:218] Iteration 2500 (1.76604 iter/s, 14.156s/25 iters), loss = 15.2142
I0305 18:20:26.819365 21892 solver.cpp:237]     Train net output #0: Loss = 15.2141 (* 1 = 15.2141 loss)
I0305 18:20:26.819392 21892 sgd_solver.cpp:105] Iteration 2500, lr = 8.45897e-07
I0305 18:20:40.965394 21892 solver.cpp:218] Iteration 2525 (1.76734 iter/s, 14.1455s/25 iters), loss = 12.4352
I0305 18:20:40.965721 21892 solver.cpp:237]     Train net output #0: Loss = 12.4351 (* 1 = 12.4351 loss)
I0305 18:20:40.965745 21892 sgd_solver.cpp:105] Iteration 2525, lr = 8.4463e-07
I0305 18:20:55.084182 21892 solver.cpp:218] Iteration 2550 (1.77079 iter/s, 14.118s/25 iters), loss = 17.5485
I0305 18:20:55.084287 21892 solver.cpp:237]     Train net output #0: Loss = 17.5485 (* 1 = 17.5485 loss)
I0305 18:20:55.084311 21892 sgd_solver.cpp:105] Iteration 2550, lr = 8.43368e-07
I0305 18:21:09.183519 21892 solver.cpp:218] Iteration 2575 (1.77321 iter/s, 14.0987s/25 iters), loss = 16.3417
I0305 18:21:09.183652 21892 solver.cpp:237]     Train net output #0: Loss = 16.3416 (* 1 = 16.3416 loss)
I0305 18:21:09.183693 21892 sgd_solver.cpp:105] Iteration 2575, lr = 8.4211e-07
I0305 18:21:23.321499 21892 solver.cpp:218] Iteration 2600 (1.76836 iter/s, 14.1374s/25 iters), loss = 18.2539
I0305 18:21:23.322007 21892 solver.cpp:237]     Train net output #0: Loss = 18.2539 (* 1 = 18.2539 loss)
I0305 18:21:23.322032 21892 sgd_solver.cpp:105] Iteration 2600, lr = 8.40857e-07
I0305 18:21:37.489796 21892 solver.cpp:218] Iteration 2625 (1.76462 iter/s, 14.1673s/25 iters), loss = 14.7245
I0305 18:21:37.489894 21892 solver.cpp:237]     Train net output #0: Loss = 14.7244 (* 1 = 14.7244 loss)
I0305 18:21:37.489914 21892 sgd_solver.cpp:105] Iteration 2625, lr = 8.39608e-07
I0305 18:21:51.644028 21892 solver.cpp:218] Iteration 2650 (1.76633 iter/s, 14.1536s/25 iters), loss = 12.1958
I0305 18:21:51.644181 21892 solver.cpp:237]     Train net output #0: Loss = 12.1957 (* 1 = 12.1957 loss)
I0305 18:21:51.644215 21892 sgd_solver.cpp:105] Iteration 2650, lr = 8.38363e-07
I0305 18:22:05.784307 21892 solver.cpp:218] Iteration 2675 (1.76809 iter/s, 14.1396s/25 iters), loss = 16.9488
I0305 18:22:05.785181 21892 solver.cpp:237]     Train net output #0: Loss = 16.9488 (* 1 = 16.9488 loss)
I0305 18:22:05.785217 21892 sgd_solver.cpp:105] Iteration 2675, lr = 8.37123e-07
I0305 18:22:20.002493 21892 solver.cpp:218] Iteration 2700 (1.75848 iter/s, 14.2168s/25 iters), loss = 15.836
I0305 18:22:20.002606 21892 solver.cpp:237]     Train net output #0: Loss = 15.8359 (* 1 = 15.8359 loss)
I0305 18:22:20.002631 21892 sgd_solver.cpp:105] Iteration 2700, lr = 8.35886e-07
I0305 18:22:34.121690 21892 solver.cpp:218] Iteration 2725 (1.77072 iter/s, 14.1186s/25 iters), loss = 17.7938
I0305 18:22:34.121793 21892 solver.cpp:237]     Train net output #0: Loss = 17.7938 (* 1 = 17.7938 loss)
I0305 18:22:34.121830 21892 sgd_solver.cpp:105] Iteration 2725, lr = 8.34654e-07
I0305 18:22:48.249402 21892 solver.cpp:218] Iteration 2750 (1.76965 iter/s, 14.1271s/25 iters), loss = 14.3365
I0305 18:22:48.249650 21892 solver.cpp:237]     Train net output #0: Loss = 14.3364 (* 1 = 14.3364 loss)
I0305 18:22:48.249677 21892 sgd_solver.cpp:105] Iteration 2750, lr = 8.33427e-07
I0305 18:23:02.404036 21892 solver.cpp:218] Iteration 2775 (1.7663 iter/s, 14.1539s/25 iters), loss = 11.9206
I0305 18:23:02.404161 21892 solver.cpp:237]     Train net output #0: Loss = 11.9206 (* 1 = 11.9206 loss)
I0305 18:23:02.404186 21892 sgd_solver.cpp:105] Iteration 2775, lr = 8.32203e-07
I0305 18:23:16.560873 21892 solver.cpp:218] Iteration 2800 (1.76601 iter/s, 14.1562s/25 iters), loss = 16.571
I0305 18:23:16.560997 21892 solver.cpp:237]     Train net output #0: Loss = 16.571 (* 1 = 16.571 loss)
I0305 18:23:16.561024 21892 sgd_solver.cpp:105] Iteration 2800, lr = 8.30984e-07
I0305 18:23:30.725872 21892 solver.cpp:218] Iteration 2825 (1.765 iter/s, 14.1643s/25 iters), loss = 15.4422
I0305 18:23:30.726126 21892 solver.cpp:237]     Train net output #0: Loss = 15.4421 (* 1 = 15.4421 loss)
I0305 18:23:30.726155 21892 sgd_solver.cpp:105] Iteration 2825, lr = 8.29769e-07
I0305 18:23:44.858444 21892 solver.cpp:218] Iteration 2850 (1.76906 iter/s, 14.1318s/25 iters), loss = 17.3977
I0305 18:23:44.858562 21892 solver.cpp:237]     Train net output #0: Loss = 17.3976 (* 1 = 17.3976 loss)
I0305 18:23:44.858587 21892 sgd_solver.cpp:105] Iteration 2850, lr = 8.28558e-07
I0305 18:23:58.916759 21892 solver.cpp:218] Iteration 2875 (1.77838 iter/s, 14.0577s/25 iters), loss = 13.9793
I0305 18:23:58.916854 21892 solver.cpp:237]     Train net output #0: Loss = 13.9792 (* 1 = 13.9792 loss)
I0305 18:23:58.916877 21892 sgd_solver.cpp:105] Iteration 2875, lr = 8.27351e-07
I0305 18:24:13.050992 21892 solver.cpp:218] Iteration 2900 (1.76883 iter/s, 14.1336s/25 iters), loss = 11.6666
I0305 18:24:13.051272 21892 solver.cpp:237]     Train net output #0: Loss = 11.6666 (* 1 = 11.6666 loss)
I0305 18:24:13.051307 21892 sgd_solver.cpp:105] Iteration 2900, lr = 8.26148e-07
I0305 18:24:27.172178 21892 solver.cpp:218] Iteration 2925 (1.77049 iter/s, 14.1204s/25 iters), loss = 16.2008
I0305 18:24:27.172296 21892 solver.cpp:237]     Train net output #0: Loss = 16.2007 (* 1 = 16.2007 loss)
I0305 18:24:27.172319 21892 sgd_solver.cpp:105] Iteration 2925, lr = 8.24949e-07
I0305 18:24:41.319926 21892 solver.cpp:218] Iteration 2950 (1.76714 iter/s, 14.1471s/25 iters), loss = 15.0766
I0305 18:24:41.320026 21892 solver.cpp:237]     Train net output #0: Loss = 15.0766 (* 1 = 15.0766 loss)
I0305 18:24:41.320046 21892 sgd_solver.cpp:105] Iteration 2950, lr = 8.23754e-07
I0305 18:24:55.470662 21892 solver.cpp:218] Iteration 2975 (1.76677 iter/s, 14.1501s/25 iters), loss = 17.078
I0305 18:24:55.470964 21892 solver.cpp:237]     Train net output #0: Loss = 17.078 (* 1 = 17.078 loss)
I0305 18:24:55.471001 21892 sgd_solver.cpp:105] Iteration 2975, lr = 8.22564e-07
I0305 18:25:09.022431 21892 solver.cpp:447] Snapshotting to binary proto file ./snapshots/auto/v3/v3_iter_3000.caffemodel
I0305 18:25:09.045315 21892 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/auto/v3/v3_iter_3000.solverstate
I0305 18:25:09.625080 21892 solver.cpp:218] Iteration 3000 (1.76633 iter/s, 14.1536s/25 iters), loss = 13.6819
I0305 18:25:09.625207 21892 solver.cpp:237]     Train net output #0: Loss = 13.6819 (* 1 = 13.6819 loss)
I0305 18:25:09.625239 21892 sgd_solver.cpp:105] Iteration 3000, lr = 8.21377e-07
I0305 18:25:23.727793 21892 solver.cpp:218] Iteration 3025 (1.77279 iter/s, 14.1021s/25 iters), loss = 11.4409
I0305 18:25:23.727941 21892 solver.cpp:237]     Train net output #0: Loss = 11.4408 (* 1 = 11.4408 loss)
I0305 18:25:23.727968 21892 sgd_solver.cpp:105] Iteration 3025, lr = 8.20194e-07
I0305 18:25:37.862547 21892 solver.cpp:218] Iteration 3050 (1.76877 iter/s, 14.1341s/25 iters), loss = 15.8962
I0305 18:25:37.862850 21892 solver.cpp:237]     Train net output #0: Loss = 15.8961 (* 1 = 15.8961 loss)
I0305 18:25:37.862918 21892 sgd_solver.cpp:105] Iteration 3050, lr = 8.19015e-07
I0305 18:25:51.981328 21892 solver.cpp:218] Iteration 3075 (1.77079 iter/s, 14.118s/25 iters), loss = 14.7561
I0305 18:25:51.981451 21892 solver.cpp:237]     Train net output #0: Loss = 14.756 (* 1 = 14.756 loss)
I0305 18:25:51.981472 21892 sgd_solver.cpp:105] Iteration 3075, lr = 8.17841e-07
I0305 18:26:06.153475 21892 solver.cpp:218] Iteration 3100 (1.7641 iter/s, 14.1716s/25 iters), loss = 16.7448
I0305 18:26:06.153578 21892 solver.cpp:237]     Train net output #0: Loss = 16.7448 (* 1 = 16.7448 loss)
I0305 18:26:06.153600 21892 sgd_solver.cpp:105] Iteration 3100, lr = 8.1667e-07
I0305 18:26:20.404461 21892 solver.cpp:218] Iteration 3125 (1.75435 iter/s, 14.2503s/25 iters), loss = 13.3828
I0305 18:26:20.404817 21892 solver.cpp:237]     Train net output #0: Loss = 13.3827 (* 1 = 13.3827 loss)
I0305 18:26:20.404844 21892 sgd_solver.cpp:105] Iteration 3125, lr = 8.15503e-07
I0305 18:26:34.610776 21892 solver.cpp:218] Iteration 3150 (1.75989 iter/s, 14.2055s/25 iters), loss = 11.2821
I0305 18:26:34.610885 21892 solver.cpp:237]     Train net output #0: Loss = 11.2821 (* 1 = 11.2821 loss)
I0305 18:26:34.610909 21892 sgd_solver.cpp:105] Iteration 3150, lr = 8.1434e-07
I0305 18:26:48.732831 21892 solver.cpp:218] Iteration 3175 (1.77036 iter/s, 14.1214s/25 iters), loss = 15.5105
I0305 18:26:48.732940 21892 solver.cpp:237]     Train net output #0: Loss = 15.5104 (* 1 = 15.5104 loss)
I0305 18:26:48.732961 21892 sgd_solver.cpp:105] Iteration 3175, lr = 8.13181e-07
I0305 18:27:02.886466 21892 solver.cpp:218] Iteration 3200 (1.76641 iter/s, 14.153s/25 iters), loss = 14.5573
I0305 18:27:02.886840 21892 solver.cpp:237]     Train net output #0: Loss = 14.5572 (* 1 = 14.5572 loss)
I0305 18:27:02.886895 21892 sgd_solver.cpp:105] Iteration 3200, lr = 8.12025e-07
I0305 18:27:17.032198 21892 solver.cpp:218] Iteration 3225 (1.76743 iter/s, 14.1448s/25 iters), loss = 16.5062
I0305 18:27:17.032341 21892 solver.cpp:237]     Train net output #0: Loss = 16.5061 (* 1 = 16.5061 loss)
I0305 18:27:17.032367 21892 sgd_solver.cpp:105] Iteration 3225, lr = 8.10874e-07
I0305 18:27:31.170207 21892 solver.cpp:218] Iteration 3250 (1.76836 iter/s, 14.1374s/25 iters), loss = 13.1248
I0305 18:27:31.170322 21892 solver.cpp:237]     Train net output #0: Loss = 13.1248 (* 1 = 13.1248 loss)
I0305 18:27:31.170348 21892 sgd_solver.cpp:105] Iteration 3250, lr = 8.09726e-07
I0305 18:27:45.304518 21892 solver.cpp:218] Iteration 3275 (1.76883 iter/s, 14.1336s/25 iters), loss = 10.9814
I0305 18:27:45.306483 21892 solver.cpp:237]     Train net output #0: Loss = 10.9813 (* 1 = 10.9813 loss)
I0305 18:27:45.306515 21892 sgd_solver.cpp:105] Iteration 3275, lr = 8.08582e-07
I0305 18:27:59.456558 21892 solver.cpp:218] Iteration 3300 (1.76683 iter/s, 14.1496s/25 iters), loss = 15.2674
I0305 18:27:59.456691 21892 solver.cpp:237]     Train net output #0: Loss = 15.2674 (* 1 = 15.2674 loss)
I0305 18:27:59.456714 21892 sgd_solver.cpp:105] Iteration 3300, lr = 8.07442e-07
I0305 18:28:13.635808 21892 solver.cpp:218] Iteration 3325 (1.76322 iter/s, 14.1786s/25 iters), loss = 14.283
I0305 18:28:13.635929 21892 solver.cpp:237]     Train net output #0: Loss = 14.2829 (* 1 = 14.2829 loss)
I0305 18:28:13.635954 21892 sgd_solver.cpp:105] Iteration 3325, lr = 8.06305e-07
I0305 18:28:27.761003 21892 solver.cpp:218] Iteration 3350 (1.76997 iter/s, 14.1246s/25 iters), loss = 16.2593
I0305 18:28:27.761392 21892 solver.cpp:237]     Train net output #0: Loss = 16.2592 (* 1 = 16.2592 loss)
I0305 18:28:27.761441 21892 sgd_solver.cpp:105] Iteration 3350, lr = 8.05173e-07
I0305 18:28:41.931177 21892 solver.cpp:218] Iteration 3375 (1.76438 iter/s, 14.1693s/25 iters), loss = 12.9463
I0305 18:28:41.931285 21892 solver.cpp:237]     Train net output #0: Loss = 12.9462 (* 1 = 12.9462 loss)
I0305 18:28:41.931306 21892 sgd_solver.cpp:105] Iteration 3375, lr = 8.04044e-07
I0305 18:28:56.061717 21892 solver.cpp:218] Iteration 3400 (1.76929 iter/s, 14.1299s/25 iters), loss = 10.808
I0305 18:28:56.061820 21892 solver.cpp:237]     Train net output #0: Loss = 10.808 (* 1 = 10.808 loss)
I0305 18:28:56.061842 21892 sgd_solver.cpp:105] Iteration 3400, lr = 8.02918e-07
I0305 18:29:10.194159 21892 solver.cpp:218] Iteration 3425 (1.76906 iter/s, 14.1318s/25 iters), loss = 14.953
I0305 18:29:10.195571 21892 solver.cpp:237]     Train net output #0: Loss = 14.953 (* 1 = 14.953 loss)
I0305 18:29:10.195601 21892 sgd_solver.cpp:105] Iteration 3425, lr = 8.01797e-07
I0305 18:29:24.313937 21892 solver.cpp:218] Iteration 3450 (1.7708 iter/s, 14.1179s/25 iters), loss = 14.0407
I0305 18:29:24.314075 21892 solver.cpp:237]     Train net output #0: Loss = 14.0406 (* 1 = 14.0406 loss)
I0305 18:29:24.314110 21892 sgd_solver.cpp:105] Iteration 3450, lr = 8.00679e-07
I0305 18:29:38.446785 21892 solver.cpp:218] Iteration 3475 (1.76901 iter/s, 14.1322s/25 iters), loss = 16.0263
I0305 18:29:38.446919 21892 solver.cpp:237]     Train net output #0: Loss = 16.0263 (* 1 = 16.0263 loss)
I0305 18:29:38.446943 21892 sgd_solver.cpp:105] Iteration 3475, lr = 7.99564e-07
I0305 18:29:51.952716 21892 solver.cpp:447] Snapshotting to binary proto file ./snapshots/auto/v3/v3_iter_3500.caffemodel
I0305 18:29:51.976948 21892 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/auto/v3/v3_iter_3500.solverstate
I0305 18:29:52.554929 21892 solver.cpp:218] Iteration 3500 (1.7721 iter/s, 14.1075s/25 iters), loss = 12.7403
I0305 18:29:52.555052 21892 solver.cpp:237]     Train net output #0: Loss = 12.7403 (* 1 = 12.7403 loss)
I0305 18:29:52.555073 21892 sgd_solver.cpp:105] Iteration 3500, lr = 7.98454e-07
I0305 18:30:06.618115 21892 solver.cpp:218] Iteration 3525 (1.77777 iter/s, 14.0626s/25 iters), loss = 10.6321
I0305 18:30:06.618216 21892 solver.cpp:237]     Train net output #0: Loss = 10.632 (* 1 = 10.632 loss)
I0305 18:30:06.618237 21892 sgd_solver.cpp:105] Iteration 3525, lr = 7.97346e-07
I0305 18:30:20.732417 21892 solver.cpp:218] Iteration 3550 (1.77133 iter/s, 14.1137s/25 iters), loss = 14.6481
I0305 18:30:20.732522 21892 solver.cpp:237]     Train net output #0: Loss = 14.648 (* 1 = 14.648 loss)
I0305 18:30:20.732544 21892 sgd_solver.cpp:105] Iteration 3550, lr = 7.96243e-07
I0305 18:30:34.814623 21892 solver.cpp:218] Iteration 3575 (1.77537 iter/s, 14.0816s/25 iters), loss = 13.8138
I0305 18:30:34.815382 21892 solver.cpp:237]     Train net output #0: Loss = 13.8138 (* 1 = 13.8138 loss)
I0305 18:30:34.815433 21892 sgd_solver.cpp:105] Iteration 3575, lr = 7.95143e-07
I0305 18:30:48.918145 21892 solver.cpp:218] Iteration 3600 (1.77276 iter/s, 14.1023s/25 iters), loss = 15.8741
I0305 18:30:48.918256 21892 solver.cpp:237]     Train net output #0: Loss = 15.8741 (* 1 = 15.8741 loss)
I0305 18:30:48.918277 21892 sgd_solver.cpp:105] Iteration 3600, lr = 7.94046e-07
I0305 18:31:03.039398 21892 solver.cpp:218] Iteration 3625 (1.77046 iter/s, 14.1206s/25 iters), loss = 12.5523
I0305 18:31:03.039508 21892 solver.cpp:237]     Train net output #0: Loss = 12.5523 (* 1 = 12.5523 loss)
I0305 18:31:03.039535 21892 sgd_solver.cpp:105] Iteration 3625, lr = 7.92953e-07
I0305 18:31:17.149498 21892 solver.cpp:218] Iteration 3650 (1.77186 iter/s, 14.1095s/25 iters), loss = 10.465
I0305 18:31:17.149737 21892 solver.cpp:237]     Train net output #0: Loss = 10.465 (* 1 = 10.465 loss)
I0305 18:31:17.149777 21892 sgd_solver.cpp:105] Iteration 3650, lr = 7.91864e-07
I0305 18:31:31.259654 21892 solver.cpp:218] Iteration 3675 (1.77186 iter/s, 14.1095s/25 iters), loss = 14.3788
I0305 18:31:31.259762 21892 solver.cpp:237]     Train net output #0: Loss = 14.3788 (* 1 = 14.3788 loss)
I0305 18:31:31.259784 21892 sgd_solver.cpp:105] Iteration 3675, lr = 7.90778e-07
I0305 18:31:45.349673 21892 solver.cpp:218] Iteration 3700 (1.77438 iter/s, 14.0894s/25 iters), loss = 13.6337
I0305 18:31:45.349781 21892 solver.cpp:237]     Train net output #0: Loss = 13.6337 (* 1 = 13.6337 loss)
I0305 18:31:45.349803 21892 sgd_solver.cpp:105] Iteration 3700, lr = 7.89695e-07
I0305 18:31:59.462761 21892 solver.cpp:218] Iteration 3725 (1.77148 iter/s, 14.1125s/25 iters), loss = 15.6236
I0305 18:31:59.463081 21892 solver.cpp:237]     Train net output #0: Loss = 15.6236 (* 1 = 15.6236 loss)
I0305 18:31:59.463129 21892 sgd_solver.cpp:105] Iteration 3725, lr = 7.88616e-07
I0305 18:32:13.563405 21892 solver.cpp:218] Iteration 3750 (1.77307 iter/s, 14.0998s/25 iters), loss = 12.3465
I0305 18:32:13.563524 21892 solver.cpp:237]     Train net output #0: Loss = 12.3465 (* 1 = 12.3465 loss)
I0305 18:32:13.563547 21892 sgd_solver.cpp:105] Iteration 3750, lr = 7.87541e-07
I0305 18:32:27.663589 21892 solver.cpp:218] Iteration 3775 (1.7731 iter/s, 14.0996s/25 iters), loss = 10.3886
I0305 18:32:27.663695 21892 solver.cpp:237]     Train net output #0: Loss = 10.3885 (* 1 = 10.3885 loss)
I0305 18:32:27.663720 21892 sgd_solver.cpp:105] Iteration 3775, lr = 7.86468e-07
I0305 18:32:41.762518 21892 solver.cpp:218] Iteration 3800 (1.77326 iter/s, 14.0983s/25 iters), loss = 14.1392
I0305 18:32:41.762771 21892 solver.cpp:237]     Train net output #0: Loss = 14.1392 (* 1 = 14.1392 loss)
I0305 18:32:41.762800 21892 sgd_solver.cpp:105] Iteration 3800, lr = 7.854e-07
I0305 18:32:55.867826 21892 solver.cpp:218] Iteration 3825 (1.77248 iter/s, 14.1046s/25 iters), loss = 13.4684
I0305 18:32:55.867924 21892 solver.cpp:237]     Train net output #0: Loss = 13.4684 (* 1 = 13.4684 loss)
I0305 18:32:55.867947 21892 sgd_solver.cpp:105] Iteration 3825, lr = 7.84334e-07
I0305 18:33:10.002609 21892 solver.cpp:218] Iteration 3850 (1.76877 iter/s, 14.1341s/25 iters), loss = 15.4005
I0305 18:33:10.002735 21892 solver.cpp:237]     Train net output #0: Loss = 15.4004 (* 1 = 15.4004 loss)
I0305 18:33:10.002759 21892 sgd_solver.cpp:105] Iteration 3850, lr = 7.83272e-07
I0305 18:33:24.126893 21892 solver.cpp:218] Iteration 3875 (1.77008 iter/s, 14.1236s/25 iters), loss = 12.2328
I0305 18:33:24.127214 21892 solver.cpp:237]     Train net output #0: Loss = 12.2328 (* 1 = 12.2328 loss)
I0305 18:33:24.127255 21892 sgd_solver.cpp:105] Iteration 3875, lr = 7.82213e-07
I0305 18:33:38.277846 21892 solver.cpp:218] Iteration 3900 (1.76677 iter/s, 14.1502s/25 iters), loss = 10.2618
I0305 18:33:38.277956 21892 solver.cpp:237]     Train net output #0: Loss = 10.2617 (* 1 = 10.2617 loss)
I0305 18:33:38.277979 21892 sgd_solver.cpp:105] Iteration 3900, lr = 7.81158e-07
I0305 18:33:52.460268 21892 solver.cpp:218] Iteration 3925 (1.76282 iter/s, 14.1818s/25 iters), loss = 13.9003
I0305 18:33:52.460371 21892 solver.cpp:237]     Train net output #0: Loss = 13.9003 (* 1 = 13.9003 loss)
I0305 18:33:52.460394 21892 sgd_solver.cpp:105] Iteration 3925, lr = 7.80106e-07
I0305 18:34:06.568475 21892 solver.cpp:218] Iteration 3950 (1.7721 iter/s, 14.1076s/25 iters), loss = 13.2055
I0305 18:34:06.570837 21892 solver.cpp:237]     Train net output #0: Loss = 13.2055 (* 1 = 13.2055 loss)
I0305 18:34:06.570897 21892 sgd_solver.cpp:105] Iteration 3950, lr = 7.79057e-07
I0305 18:34:20.709662 21892 solver.cpp:218] Iteration 3975 (1.76824 iter/s, 14.1383s/25 iters), loss = 15.1659
I0305 18:34:20.709810 21892 solver.cpp:237]     Train net output #0: Loss = 15.1659 (* 1 = 15.1659 loss)
I0305 18:34:20.709833 21892 sgd_solver.cpp:105] Iteration 3975, lr = 7.78012e-07
I0305 18:34:34.287093 21892 solver.cpp:447] Snapshotting to binary proto file ./snapshots/auto/v3/v3_iter_4000.caffemodel
I0305 18:34:34.312644 21892 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/auto/v3/v3_iter_4000.solverstate
I0305 18:34:34.900077 21892 solver.cpp:218] Iteration 4000 (1.76183 iter/s, 14.1898s/25 iters), loss = 12.0065
I0305 18:34:34.900210 21892 solver.cpp:237]     Train net output #0: Loss = 12.0064 (* 1 = 12.0064 loss)
I0305 18:34:34.900243 21892 sgd_solver.cpp:105] Iteration 4000, lr = 7.7697e-07
I0305 18:34:49.253135 21892 solver.cpp:218] Iteration 4025 (1.74187 iter/s, 14.3524s/25 iters), loss = 10.1253
I0305 18:34:49.254792 21892 solver.cpp:237]     Train net output #0: Loss = 10.1252 (* 1 = 10.1252 loss)
I0305 18:34:49.254823 21892 sgd_solver.cpp:105] Iteration 4025, lr = 7.75931e-07
I0305 18:35:03.458989 21892 solver.cpp:218] Iteration 4050 (1.76015 iter/s, 14.2033s/25 iters), loss = 13.6817
I0305 18:35:03.459125 21892 solver.cpp:237]     Train net output #0: Loss = 13.6816 (* 1 = 13.6816 loss)
I0305 18:35:03.459148 21892 sgd_solver.cpp:105] Iteration 4050, lr = 7.74895e-07
I0305 18:35:17.622709 21892 solver.cpp:218] Iteration 4075 (1.76516 iter/s, 14.1631s/25 iters), loss = 12.9858
I0305 18:35:17.622830 21892 solver.cpp:237]     Train net output #0: Loss = 12.9858 (* 1 = 12.9858 loss)
I0305 18:35:17.622854 21892 sgd_solver.cpp:105] Iteration 4075, lr = 7.73862e-07
I0305 18:35:31.750717 21892 solver.cpp:218] Iteration 4100 (1.76961 iter/s, 14.1274s/25 iters), loss = 14.9854
I0305 18:35:31.753141 21892 solver.cpp:237]     Train net output #0: Loss = 14.9854 (* 1 = 14.9854 loss)
I0305 18:35:31.753173 21892 sgd_solver.cpp:105] Iteration 4100, lr = 7.72833e-07
I0305 18:35:45.888357 21892 solver.cpp:218] Iteration 4125 (1.76869 iter/s, 14.1347s/25 iters), loss = 11.835
I0305 18:35:45.888490 21892 solver.cpp:237]     Train net output #0: Loss = 11.835 (* 1 = 11.835 loss)
I0305 18:35:45.888515 21892 sgd_solver.cpp:105] Iteration 4125, lr = 7.71807e-07
I0305 18:36:00.031445 21892 solver.cpp:218] Iteration 4150 (1.76773 iter/s, 14.1425s/25 iters), loss = 10.0489
I0305 18:36:00.031545 21892 solver.cpp:237]     Train net output #0: Loss = 10.0488 (* 1 = 10.0488 loss)
I0305 18:36:00.031568 21892 sgd_solver.cpp:105] Iteration 4150, lr = 7.70784e-07
I0305 18:36:14.178791 21892 solver.cpp:218] Iteration 4175 (1.76719 iter/s, 14.1467s/25 iters), loss = 13.4749
I0305 18:36:14.179419 21892 solver.cpp:237]     Train net output #0: Loss = 13.4748 (* 1 = 13.4748 loss)
I0305 18:36:14.179443 21892 sgd_solver.cpp:105] Iteration 4175, lr = 7.69764e-07
I0305 18:36:28.292973 21892 solver.cpp:218] Iteration 4200 (1.77141 iter/s, 14.1131s/25 iters), loss = 12.8498
I0305 18:36:28.293092 21892 solver.cpp:237]     Train net output #0: Loss = 12.8498 (* 1 = 12.8498 loss)
I0305 18:36:28.293114 21892 sgd_solver.cpp:105] Iteration 4200, lr = 7.68748e-07
I0305 18:36:42.412173 21892 solver.cpp:218] Iteration 4225 (1.77072 iter/s, 14.1185s/25 iters), loss = 14.7715
I0305 18:36:42.412293 21892 solver.cpp:237]     Train net output #0: Loss = 14.7714 (* 1 = 14.7714 loss)
I0305 18:36:42.412317 21892 sgd_solver.cpp:105] Iteration 4225, lr = 7.67734e-07
I0305 18:36:56.506350 21892 solver.cpp:218] Iteration 4250 (1.77386 iter/s, 14.0936s/25 iters), loss = 11.6997
I0305 18:36:56.506602 21892 solver.cpp:237]     Train net output #0: Loss = 11.6996 (* 1 = 11.6996 loss)
I0305 18:36:56.506644 21892 sgd_solver.cpp:105] Iteration 4250, lr = 7.66724e-07
I0305 18:37:10.649206 21892 solver.cpp:218] Iteration 4275 (1.76777 iter/s, 14.1421s/25 iters), loss = 9.95897
I0305 18:37:10.649343 21892 solver.cpp:237]     Train net output #0: Loss = 9.95891 (* 1 = 9.95891 loss)
I0305 18:37:10.649370 21892 sgd_solver.cpp:105] Iteration 4275, lr = 7.65716e-07
I0305 18:37:24.781265 21892 solver.cpp:218] Iteration 4300 (1.76911 iter/s, 14.1314s/25 iters), loss = 13.2966
I0305 18:37:24.781381 21892 solver.cpp:237]     Train net output #0: Loss = 13.2966 (* 1 = 13.2966 loss)
I0305 18:37:24.781405 21892 sgd_solver.cpp:105] Iteration 4300, lr = 7.64712e-07
I0305 18:37:38.869431 21892 solver.cpp:218] Iteration 4325 (1.77462 iter/s, 14.0875s/25 iters), loss = 12.6674
I0305 18:37:38.869698 21892 solver.cpp:237]     Train net output #0: Loss = 12.6673 (* 1 = 12.6673 loss)
I0305 18:37:38.869724 21892 sgd_solver.cpp:105] Iteration 4325, lr = 7.63711e-07
I0305 18:37:53.012987 21892 solver.cpp:218] Iteration 4350 (1.76769 iter/s, 14.1428s/25 iters), loss = 14.5884
I0305 18:37:53.013140 21892 solver.cpp:237]     Train net output #0: Loss = 14.5883 (* 1 = 14.5883 loss)
I0305 18:37:53.013180 21892 sgd_solver.cpp:105] Iteration 4350, lr = 7.62713e-07
I0305 18:38:07.173396 21892 solver.cpp:218] Iteration 4375 (1.76557 iter/s, 14.1598s/25 iters), loss = 11.5669
I0305 18:38:07.173526 21892 solver.cpp:237]     Train net output #0: Loss = 11.5668 (* 1 = 11.5668 loss)
I0305 18:38:07.173545 21892 sgd_solver.cpp:105] Iteration 4375, lr = 7.61718e-07
I0305 18:38:21.350780 21892 solver.cpp:218] Iteration 4400 (1.76345 iter/s, 14.1768s/25 iters), loss = 9.89099
I0305 18:38:21.352497 21892 solver.cpp:237]     Train net output #0: Loss = 9.89093 (* 1 = 9.89093 loss)
I0305 18:38:21.352522 21892 sgd_solver.cpp:105] Iteration 4400, lr = 7.60726e-07
I0305 18:38:35.432090 21892 solver.cpp:218] Iteration 4425 (1.77568 iter/s, 14.0791s/25 iters), loss = 13.1302
I0305 18:38:35.432222 21892 solver.cpp:237]     Train net output #0: Loss = 13.1302 (* 1 = 13.1302 loss)
I0305 18:38:35.432253 21892 sgd_solver.cpp:105] Iteration 4425, lr = 7.59737e-07
I0305 18:38:49.589023 21892 solver.cpp:218] Iteration 4450 (1.766 iter/s, 14.1563s/25 iters), loss = 12.4865
I0305 18:38:49.589169 21892 solver.cpp:237]     Train net output #0: Loss = 12.4864 (* 1 = 12.4864 loss)
I0305 18:38:49.589195 21892 sgd_solver.cpp:105] Iteration 4450, lr = 7.58751e-07
I0305 18:39:03.755703 21892 solver.cpp:218] Iteration 4475 (1.76479 iter/s, 14.166s/25 iters), loss = 14.4026
I0305 18:39:03.756024 21892 solver.cpp:237]     Train net output #0: Loss = 14.4025 (* 1 = 14.4025 loss)
I0305 18:39:03.756053 21892 sgd_solver.cpp:105] Iteration 4475, lr = 7.57768e-07
I0305 18:39:17.362025 21892 solver.cpp:447] Snapshotting to binary proto file ./snapshots/auto/v3/v3_iter_4500.caffemodel
I0305 18:39:17.384889 21892 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/auto/v3/v3_iter_4500.solverstate
I0305 18:39:17.952827 21892 solver.cpp:218] Iteration 4500 (1.76102 iter/s, 14.1963s/25 iters), loss = 11.4427
I0305 18:39:17.952937 21892 solver.cpp:237]     Train net output #0: Loss = 11.4426 (* 1 = 11.4426 loss)
I0305 18:39:17.952976 21892 sgd_solver.cpp:105] Iteration 4500, lr = 7.56788e-07
I0305 18:39:32.098772 21892 solver.cpp:218] Iteration 4525 (1.76737 iter/s, 14.1453s/25 iters), loss = 9.76364
I0305 18:39:32.098879 21892 solver.cpp:237]     Train net output #0: Loss = 9.76358 (* 1 = 9.76358 loss)
I0305 18:39:32.098902 21892 sgd_solver.cpp:105] Iteration 4525, lr = 7.5581e-07
I0305 18:39:46.277755 21892 solver.cpp:218] Iteration 4550 (1.76325 iter/s, 14.1784s/25 iters), loss = 12.9384
I0305 18:39:46.278040 21892 solver.cpp:237]     Train net output #0: Loss = 12.9384 (* 1 = 12.9384 loss)
I0305 18:39:46.278089 21892 sgd_solver.cpp:105] Iteration 4550, lr = 7.54836e-07
I0305 18:40:00.460194 21892 solver.cpp:218] Iteration 4575 (1.76284 iter/s, 14.1817s/25 iters), loss = 12.3228
I0305 18:40:00.460302 21892 solver.cpp:237]     Train net output #0: Loss = 12.3227 (* 1 = 12.3227 loss)
I0305 18:40:00.460325 21892 sgd_solver.cpp:105] Iteration 4575, lr = 7.53865e-07
I0305 18:40:14.620553 21892 solver.cpp:218] Iteration 4600 (1.76557 iter/s, 14.1598s/25 iters), loss = 14.218
I0305 18:40:14.620677 21892 solver.cpp:237]     Train net output #0: Loss = 14.2179 (* 1 = 14.2179 loss)
I0305 18:40:14.620709 21892 sgd_solver.cpp:105] Iteration 4600, lr = 7.52897e-07
I0305 18:40:28.771853 21892 solver.cpp:218] Iteration 4625 (1.76671 iter/s, 14.1506s/25 iters), loss = 11.2529
I0305 18:40:28.772941 21892 solver.cpp:237]     Train net output #0: Loss = 11.2528 (* 1 = 11.2528 loss)
I0305 18:40:28.772987 21892 sgd_solver.cpp:105] Iteration 4625, lr = 7.51931e-07
I0305 18:40:42.948904 21892 solver.cpp:218] Iteration 4650 (1.76361 iter/s, 14.1755s/25 iters), loss = 9.72246
I0305 18:40:42.949028 21892 solver.cpp:237]     Train net output #0: Loss = 9.7224 (* 1 = 9.7224 loss)
I0305 18:40:42.949050 21892 sgd_solver.cpp:105] Iteration 4650, lr = 7.50969e-07
I0305 18:40:57.082667 21892 solver.cpp:218] Iteration 4675 (1.7689 iter/s, 14.1331s/25 iters), loss = 12.7889
I0305 18:40:57.082774 21892 solver.cpp:237]     Train net output #0: Loss = 12.7888 (* 1 = 12.7888 loss)
I0305 18:40:57.082794 21892 sgd_solver.cpp:105] Iteration 4675, lr = 7.50009e-07
I0305 18:41:11.218258 21892 solver.cpp:218] Iteration 4700 (1.76866 iter/s, 14.135s/25 iters), loss = 12.1711
I0305 18:41:11.222993 21892 solver.cpp:237]     Train net output #0: Loss = 12.171 (* 1 = 12.171 loss)
I0305 18:41:11.223039 21892 sgd_solver.cpp:105] Iteration 4700, lr = 7.49052e-07
I0305 18:41:25.391396 21892 solver.cpp:218] Iteration 4725 (1.76455 iter/s, 14.1679s/25 iters), loss = 14.0574
I0305 18:41:25.391510 21892 solver.cpp:237]     Train net output #0: Loss = 14.0574 (* 1 = 14.0574 loss)
I0305 18:41:25.391532 21892 sgd_solver.cpp:105] Iteration 4725, lr = 7.48098e-07
I0305 18:41:39.522397 21892 solver.cpp:218] Iteration 4750 (1.76924 iter/s, 14.1304s/25 iters), loss = 11.1374
I0305 18:41:39.522511 21892 solver.cpp:237]     Train net output #0: Loss = 11.1373 (* 1 = 11.1373 loss)
I0305 18:41:39.522534 21892 sgd_solver.cpp:105] Iteration 4750, lr = 7.47147e-07
I0305 18:41:53.661252 21892 solver.cpp:218] Iteration 4775 (1.76825 iter/s, 14.1382s/25 iters), loss = 9.60903
I0305 18:41:53.661463 21892 solver.cpp:237]     Train net output #0: Loss = 9.60897 (* 1 = 9.60897 loss)
I0305 18:41:53.661486 21892 sgd_solver.cpp:105] Iteration 4775, lr = 7.46199e-07
I0305 18:42:07.830405 21892 solver.cpp:218] Iteration 4800 (1.76448 iter/s, 14.1684s/25 iters), loss = 12.6319
I0305 18:42:07.830529 21892 solver.cpp:237]     Train net output #0: Loss = 12.6319 (* 1 = 12.6319 loss)
I0305 18:42:07.830556 21892 sgd_solver.cpp:105] Iteration 4800, lr = 7.45253e-07
I0305 18:42:21.991777 21892 solver.cpp:218] Iteration 4825 (1.76544 iter/s, 14.1608s/25 iters), loss = 12.0909
I0305 18:42:21.991917 21892 solver.cpp:237]     Train net output #0: Loss = 12.0908 (* 1 = 12.0908 loss)
I0305 18:42:21.991961 21892 sgd_solver.cpp:105] Iteration 4825, lr = 7.4431e-07
I0305 18:42:36.136555 21892 solver.cpp:218] Iteration 4850 (1.76752 iter/s, 14.1441s/25 iters), loss = 13.8759
I0305 18:42:36.138075 21892 solver.cpp:237]     Train net output #0: Loss = 13.8759 (* 1 = 13.8759 loss)
I0305 18:42:36.138103 21892 sgd_solver.cpp:105] Iteration 4850, lr = 7.4337e-07
I0305 18:42:50.265465 21892 solver.cpp:218] Iteration 4875 (1.76968 iter/s, 14.1269s/25 iters), loss = 11.0453
I0305 18:42:50.265641 21892 solver.cpp:237]     Train net output #0: Loss = 11.0452 (* 1 = 11.0452 loss)
I0305 18:42:50.265676 21892 sgd_solver.cpp:105] Iteration 4875, lr = 7.42433e-07
I0305 18:43:04.505861 21892 solver.cpp:218] Iteration 4900 (1.75565 iter/s, 14.2397s/25 iters), loss = 9.50517
I0305 18:43:04.506006 21892 solver.cpp:237]     Train net output #0: Loss = 9.50511 (* 1 = 9.50511 loss)
I0305 18:43:04.506036 21892 sgd_solver.cpp:105] Iteration 4900, lr = 7.41499e-07
I0305 18:43:18.701648 21892 solver.cpp:218] Iteration 4925 (1.76117 iter/s, 14.1951s/25 iters), loss = 12.511
I0305 18:43:18.701889 21892 solver.cpp:237]     Train net output #0: Loss = 12.511 (* 1 = 12.511 loss)
I0305 18:43:18.701915 21892 sgd_solver.cpp:105] Iteration 4925, lr = 7.40567e-07
I0305 18:43:32.817375 21892 solver.cpp:218] Iteration 4950 (1.77116 iter/s, 14.115s/25 iters), loss = 11.9594
I0305 18:43:32.817484 21892 solver.cpp:237]     Train net output #0: Loss = 11.9593 (* 1 = 11.9593 loss)
I0305 18:43:32.817507 21892 sgd_solver.cpp:105] Iteration 4950, lr = 7.39638e-07
I0305 18:43:46.977622 21892 solver.cpp:218] Iteration 4975 (1.76558 iter/s, 14.1596s/25 iters), loss = 13.7395
I0305 18:43:46.977742 21892 solver.cpp:237]     Train net output #0: Loss = 13.7395 (* 1 = 13.7395 loss)
I0305 18:43:46.977766 21892 sgd_solver.cpp:105] Iteration 4975, lr = 7.38712e-07
I0305 18:44:00.539738 21892 solver.cpp:447] Snapshotting to binary proto file ./snapshots/auto/v3/v3_iter_5000.caffemodel
I0305 18:44:00.563563 21892 sgd_solver.cpp:273] Snapshotting solver state to binary proto file ./snapshots/auto/v3/v3_iter_5000.solverstate
I0305 18:44:00.698994 21892 solver.cpp:310] Iteration 5000, loss = 10.9238
I0305 18:44:00.699075 21892 solver.cpp:315] Optimization Done.
I0305 18:44:00.699092 21892 caffe.cpp:259] Optimization Done.
